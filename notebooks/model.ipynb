{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hillfort detection with LiDAR data\n",
    "## Data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "[Code](#code)\n",
    "\n",
    "1. [**Initializing and training the model**](#initializing-and-training-the-model)\n",
    "2. [**Evaluating the model**](#evaluating-the-model)\n",
    "3. [**Hyperparameter tuning**](#hyperparameter-tuning)\n",
    "4. [**Results**](#results)\n",
    "\n",
    "[End](#end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn numpy matplotlib laspy tqdm geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have GPU\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "# Otherwise\n",
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Imports\n",
    "import os\n",
    "# import re\n",
    "# import csv\n",
    "# import typing\n",
    "import itertools\n",
    "# import json\n",
    "import logging\n",
    "import zipfile\n",
    "# import warnings\n",
    "# import evaluate\n",
    "# import types\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import torch\n",
    "# import math\n",
    "import shapely\n",
    "import matplotlib.pyplot as plt\n",
    "import laspy # Reading LAS file format\n",
    "from tqdm import tqdm # Loading bars\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model setup has been copied from Nikita Karaev's Google Colab [PointNetClass](https://colab.research.google.com/github/nikitakaraevv/pointnet/blob/master/nbs/PointNetClass.ipynb#scrollTo=ZV20opgrv23I). Criterion has been customized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tnet(nn.Module):\n",
    "   def __init__(self, k=3):\n",
    "      super().__init__()\n",
    "      self.k=k\n",
    "      self.conv1 = nn.Conv1d(k,64,1)\n",
    "      self.conv2 = nn.Conv1d(64,128,1)\n",
    "      self.conv3 = nn.Conv1d(128,1024,1)\n",
    "      self.fc1 = nn.Linear(1024,512)\n",
    "      self.fc2 = nn.Linear(512,256)\n",
    "      self.fc3 = nn.Linear(256,k*k)\n",
    "\n",
    "      self.bn1 = nn.BatchNorm1d(64)\n",
    "      self.bn2 = nn.BatchNorm1d(128)\n",
    "      self.bn3 = nn.BatchNorm1d(1024)\n",
    "      self.bn4 = nn.BatchNorm1d(512)\n",
    "      self.bn5 = nn.BatchNorm1d(256)\n",
    "       \n",
    "\n",
    "   def forward(self, input):\n",
    "      # input.shape == (bs,n,3)\n",
    "      bs = input.size(0)\n",
    "      xb = F.relu(self.bn1(self.conv1(input)))\n",
    "      xb = F.relu(self.bn2(self.conv2(xb)))\n",
    "      xb = F.relu(self.bn3(self.conv3(xb)))\n",
    "      pool = nn.MaxPool1d(xb.size(-1))(xb)\n",
    "      flat = nn.Flatten(1)(pool)\n",
    "      xb = F.relu(self.bn4(self.fc1(flat)))\n",
    "      xb = F.relu(self.bn5(self.fc2(xb)))\n",
    "      \n",
    "      #initialize as identity\n",
    "      init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n",
    "      if xb.is_cuda:\n",
    "        init=init.cuda()\n",
    "      matrix = self.fc3(xb).view(-1,self.k,self.k) + init\n",
    "      return matrix\n",
    "\n",
    "\n",
    "class Transform(nn.Module):\n",
    "   def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_transform = Tnet(k=3)\n",
    "        self.feature_transform = Tnet(k=64)\n",
    "        self.conv1 = nn.Conv1d(3,64,1)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64,128,1)\n",
    "        self.conv3 = nn.Conv1d(128,1024,1)\n",
    "       \n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "       \n",
    "   def forward(self, input):\n",
    "        matrix3x3 = self.input_transform(input)\n",
    "        # batch matrix multiplication\n",
    "        xb = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n",
    "\n",
    "        xb = F.relu(self.bn1(self.conv1(xb)))\n",
    "\n",
    "        matrix64x64 = self.feature_transform(xb)\n",
    "        xb = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)\n",
    "\n",
    "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
    "        xb = self.bn3(self.conv3(xb))\n",
    "        xb = nn.MaxPool1d(xb.size(-1))(xb)\n",
    "        output = nn.Flatten(1)(xb)\n",
    "        return output, matrix3x3, matrix64x64\n",
    "\n",
    "class PointNet(nn.Module):\n",
    "    def __init__(self, classes = 10):\n",
    "        super().__init__()\n",
    "        self.transform = Transform()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, classes)\n",
    "        \n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        xb, matrix3x3, matrix64x64 = self.transform(input)\n",
    "        xb = F.relu(self.bn1(self.fc1(xb)))\n",
    "        xb = F.relu(self.bn2(self.dropout(self.fc2(xb))))\n",
    "        output = self.fc3(xb)\n",
    "        return self.logsoftmax(output), matrix3x3, matrix64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, xyz, labels, num_points=1024):\n",
    "        self.xyz = xyz\n",
    "        self.labels = labels\n",
    "        self.num_points = num_points\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xyz)  # Number of point clouds in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        points = self.xyz[idx]  # Points for the idx-th sample\n",
    "        labels = self.labels[idx]  # Labels for the same sample\n",
    "        \n",
    "        # Ensure points is a 2D tensor with shape (N, 3)\n",
    "        if points.ndimension() == 1:\n",
    "            points = points.view(-1, 3)\n",
    "\n",
    "        # Padding to ensure every point cloud has num_points\n",
    "        if points.shape[0] < self.num_points:\n",
    "            padding = torch.zeros(self.num_points - points.shape[0], 3)  # Padding with zeros\n",
    "            points = torch.cat([points, padding], dim=0)  # Concatenate the points with the padding\n",
    "        else:\n",
    "            points = points[:self.num_points]  # Truncate if there are more than num_points\n",
    "\n",
    "        return {'pointcloud': points, 'category': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointnetloss(outputs, labels, m3x3, m64x64, criterion, m3x3_weight=0.0001, m64x64_weight=0.0001):\n",
    "    if not criterion:\n",
    "        criterion = torch.nn.NLLLoss()\n",
    "    bs = outputs.size(0)\n",
    "\n",
    "    # Identity matrices for regularization (to penalize transformation deviations)\n",
    "    id3x3 = torch.eye(3, requires_grad=True).repeat(bs, 1, 1).to(outputs.device)\n",
    "    id64x64 = torch.eye(64, requires_grad=True).repeat(bs, 1, 1).to(outputs.device)\n",
    "\n",
    "    # Regularization terms\n",
    "    diff3x3 = id3x3 - torch.bmm(m3x3, m3x3.transpose(1, 2))\n",
    "    diff64x64 = id64x64 - torch.bmm(m64x64, m64x64.transpose(1, 2))\n",
    "\n",
    "    # Compute the base loss (negative log likelihood)\n",
    "    base_loss = criterion(outputs, labels)\n",
    "\n",
    "    # Add regularization (transformation matrices)\n",
    "    reg_loss = m3x3_weight * (torch.norm(diff3x3) + torch.norm(diff64x64)) / float(bs)\n",
    "\n",
    "    return base_loss + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(pointnet, criterion, train_loader, device, val_loader=None, epochs=15, save=True):\n",
    "    optimizer = torch.optim.Adam(pointnet.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs): \n",
    "        pointnet.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, m3x3, m64x64 = pointnet(inputs.transpose(1,2))\n",
    "\n",
    "            loss = pointnetloss(outputs, labels, m3x3, m64x64, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:    # print every 10 mini-batches\n",
    "                    print('[Epoch: %d, Batch: %4d / %4d], loss: %.3f' %\n",
    "                        (epoch + 1, i + 1, len(train_loader), running_loss / 10))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "        pointnet.eval()\n",
    "        correct = total = 0\n",
    "\n",
    "        # validation\n",
    "        if val_loader:\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n",
    "                    outputs, __, __ = pointnet(inputs.transpose(1,2))\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "            val_acc = 100. * correct / total\n",
    "            print('Valid accuracy: %d %%' % val_acc)\n",
    "\n",
    "        # save the model\n",
    "        if save:\n",
    "            print(\"Saving model\")\n",
    "            torch.save(pointnet.state_dict(), f=f\"../model_weights/save_epoch_{str(epoch)}.pth\") # Save the model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "laz_file_dir = '../data/downsampled_class_lazFiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files - Train: 93, Validation: 11, Test: 27\n",
      "Train data shape: (7141378, 3) (7141378,)\n",
      "Validation data shape: (731422, 3) (731422,)\n",
      "Test data shape: (1946003, 3) (1946003,)\n"
     ]
    }
   ],
   "source": [
    "laz_file_dir = '../data/downsampled_class_lazFiles/'\n",
    "\n",
    "# Step 1: Get the list of LiDAR files\n",
    "all_files = [os.path.join(laz_file_dir, f) for f in os.listdir(laz_file_dir) if f.endswith('.laz')]\n",
    "\n",
    "# Step 2: Split the files into train, validation, and test sets\n",
    "train_files, test_files = sk.model_selection.train_test_split(all_files, test_size=0.2, random_state=42)  # 20% for testing\n",
    "train_files, val_files = sk.model_selection.train_test_split(train_files, test_size=0.1, random_state=42)  # 10% of train for validation\n",
    "\n",
    "print(f\"Number of files - Train: {len(train_files)}, Validation: {len(val_files)}, Test: {len(test_files)}\")\n",
    "\n",
    "# Step 3: Load and group data based on the splits\n",
    "def load_grouped_data(file_list):\n",
    "    X, y = [], []\n",
    "    for file in file_list:\n",
    "        las = laspy.read(file)\n",
    "        xyz = las.xyz\n",
    "        labels = (las.points.array['classification'] == 12).astype(int)  # Hillfort class\n",
    "        X.append(xyz)\n",
    "        y.append(labels)\n",
    "    return X, y\n",
    "\n",
    "# Load data for each split\n",
    "X_train, y_train = load_grouped_data(train_files)\n",
    "X_val, y_val = load_grouped_data(val_files)\n",
    "X_test, y_test = load_grouped_data(test_files)\n",
    "\n",
    "# Optionally, combine all points into single arrays for each split\n",
    "X_train_combined = np.vstack(X_train)\n",
    "y_train_combined = np.concatenate(y_train)\n",
    "\n",
    "X_val_combined = np.vstack(X_val)\n",
    "y_val_combined = np.concatenate(y_val)\n",
    "\n",
    "X_test_combined = np.vstack(X_test)\n",
    "y_test_combined = np.concatenate(y_test)\n",
    "\n",
    "# Print final shapes\n",
    "print(\"Train data shape:\", X_train_combined.shape, y_train_combined.shape)\n",
    "print(\"Validation data shape:\", X_val_combined.shape, y_val_combined.shape)\n",
    "print(\"Test data shape:\", X_test_combined.shape, y_test_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train_combined))\n",
    "print(np.unique(y_val_combined))\n",
    "print(np.unique(y_test_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5103787  24.58779661]\n"
     ]
    }
   ],
   "source": [
    "class_weights = sk.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train_combined), y=y_train_combined)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).cuda() if device.type == \"cuda\" else torch.tensor(class_weights, dtype=torch.float32)\n",
    "criterion = torch.nn.NLLLoss(weight=class_weights_tensor)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.from_numpy(X_train_combined)\n",
    "train_y = torch.from_numpy(y_train_combined).long()\n",
    "val_X = torch.from_numpy(X_val_combined)\n",
    "val_y = torch.from_numpy(y_val_combined).long()\n",
    "test_X = torch.from_numpy(X_test_combined)\n",
    "test_y = torch.from_numpy(y_test_combined).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PointCloudDataset(train_X, train_y)\n",
    "val_dataset = PointCloudDataset(val_X, val_y)\n",
    "test_dataset = PointCloudDataset(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointnet = PointNet(classes=2)\n",
    "pointnet.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, Batch:   10 / 55793], loss: 0.700\n",
      "[Epoch: 1, Batch:   20 / 55793], loss: 0.676\n",
      "[Epoch: 1, Batch:   30 / 55793], loss: 0.770\n",
      "[Epoch: 1, Batch:   40 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch:   50 / 55793], loss: 0.706\n",
      "[Epoch: 1, Batch:   60 / 55793], loss: 0.664\n",
      "[Epoch: 1, Batch:   70 / 55793], loss: 0.700\n",
      "[Epoch: 1, Batch:   80 / 55793], loss: 0.724\n",
      "[Epoch: 1, Batch:   90 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch:  100 / 55793], loss: 0.698\n",
      "[Epoch: 1, Batch:  110 / 55793], loss: 0.652\n",
      "[Epoch: 1, Batch:  120 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch:  130 / 55793], loss: 0.713\n",
      "[Epoch: 1, Batch:  140 / 55793], loss: 0.723\n",
      "[Epoch: 1, Batch:  150 / 55793], loss: 0.703\n",
      "[Epoch: 1, Batch:  160 / 55793], loss: 0.690\n",
      "[Epoch: 1, Batch:  170 / 55793], loss: 0.737\n",
      "[Epoch: 1, Batch:  180 / 55793], loss: 0.652\n",
      "[Epoch: 1, Batch:  190 / 55793], loss: 0.691\n",
      "[Epoch: 1, Batch:  200 / 55793], loss: 0.668\n",
      "[Epoch: 1, Batch:  210 / 55793], loss: 0.677\n",
      "[Epoch: 1, Batch:  220 / 55793], loss: 0.714\n",
      "[Epoch: 1, Batch:  230 / 55793], loss: 0.721\n",
      "[Epoch: 1, Batch:  240 / 55793], loss: 0.703\n",
      "[Epoch: 1, Batch:  250 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch:  260 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch:  270 / 55793], loss: 0.671\n",
      "[Epoch: 1, Batch:  280 / 55793], loss: 0.665\n",
      "[Epoch: 1, Batch:  290 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch:  300 / 55793], loss: 0.699\n",
      "[Epoch: 1, Batch:  310 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch:  320 / 55793], loss: 0.706\n",
      "[Epoch: 1, Batch:  330 / 55793], loss: 0.711\n",
      "[Epoch: 1, Batch:  340 / 55793], loss: 0.713\n",
      "[Epoch: 1, Batch:  350 / 55793], loss: 0.698\n",
      "[Epoch: 1, Batch:  360 / 55793], loss: 0.721\n",
      "[Epoch: 1, Batch:  370 / 55793], loss: 0.709\n",
      "[Epoch: 1, Batch:  380 / 55793], loss: 0.699\n",
      "[Epoch: 1, Batch:  390 / 55793], loss: 0.661\n",
      "[Epoch: 1, Batch:  400 / 55793], loss: 0.687\n",
      "[Epoch: 1, Batch:  410 / 55793], loss: 0.675\n",
      "[Epoch: 1, Batch:  420 / 55793], loss: 0.683\n",
      "[Epoch: 1, Batch:  430 / 55793], loss: 0.617\n",
      "[Epoch: 1, Batch:  440 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch:  450 / 55793], loss: 0.634\n",
      "[Epoch: 1, Batch:  460 / 55793], loss: 0.694\n",
      "[Epoch: 1, Batch:  470 / 55793], loss: 0.724\n",
      "[Epoch: 1, Batch:  480 / 55793], loss: 0.696\n",
      "[Epoch: 1, Batch:  490 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch:  500 / 55793], loss: 0.675\n",
      "[Epoch: 1, Batch:  510 / 55793], loss: 0.661\n",
      "[Epoch: 1, Batch:  520 / 55793], loss: 0.706\n",
      "[Epoch: 1, Batch:  530 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch:  540 / 55793], loss: 0.633\n",
      "[Epoch: 1, Batch:  550 / 55793], loss: 0.659\n",
      "[Epoch: 1, Batch:  560 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch:  570 / 55793], loss: 0.676\n",
      "[Epoch: 1, Batch:  580 / 55793], loss: 0.739\n",
      "[Epoch: 1, Batch:  590 / 55793], loss: 0.675\n",
      "[Epoch: 1, Batch:  600 / 55793], loss: 0.679\n",
      "[Epoch: 1, Batch:  610 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch:  620 / 55793], loss: 0.858\n",
      "[Epoch: 1, Batch:  630 / 55793], loss: 0.743\n",
      "[Epoch: 1, Batch:  640 / 55793], loss: 0.730\n",
      "[Epoch: 1, Batch:  650 / 55793], loss: 0.717\n",
      "[Epoch: 1, Batch:  660 / 55793], loss: 0.677\n",
      "[Epoch: 1, Batch:  670 / 55793], loss: 0.668\n",
      "[Epoch: 1, Batch:  680 / 55793], loss: 0.798\n",
      "[Epoch: 1, Batch:  690 / 55793], loss: 0.685\n",
      "[Epoch: 1, Batch:  700 / 55793], loss: 0.722\n",
      "[Epoch: 1, Batch:  710 / 55793], loss: 0.685\n",
      "[Epoch: 1, Batch:  720 / 55793], loss: 0.681\n",
      "[Epoch: 1, Batch:  730 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch:  740 / 55793], loss: 0.537\n",
      "[Epoch: 1, Batch:  750 / 55793], loss: 0.809\n",
      "[Epoch: 1, Batch:  760 / 55793], loss: 0.654\n",
      "[Epoch: 1, Batch:  770 / 55793], loss: 0.634\n",
      "[Epoch: 1, Batch:  780 / 55793], loss: 0.680\n",
      "[Epoch: 1, Batch:  790 / 55793], loss: 0.704\n",
      "[Epoch: 1, Batch:  800 / 55793], loss: 0.685\n",
      "[Epoch: 1, Batch:  810 / 55793], loss: 0.732\n",
      "[Epoch: 1, Batch:  820 / 55793], loss: 0.720\n",
      "[Epoch: 1, Batch:  830 / 55793], loss: 0.656\n",
      "[Epoch: 1, Batch:  840 / 55793], loss: 0.633\n",
      "[Epoch: 1, Batch:  850 / 55793], loss: 0.711\n",
      "[Epoch: 1, Batch:  860 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch:  870 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch:  880 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch:  890 / 55793], loss: 0.651\n",
      "[Epoch: 1, Batch:  900 / 55793], loss: 0.709\n",
      "[Epoch: 1, Batch:  910 / 55793], loss: 0.706\n",
      "[Epoch: 1, Batch:  920 / 55793], loss: 0.647\n",
      "[Epoch: 1, Batch:  930 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch:  940 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch:  950 / 55793], loss: 0.693\n",
      "[Epoch: 1, Batch:  960 / 55793], loss: 0.670\n",
      "[Epoch: 1, Batch:  970 / 55793], loss: 0.654\n",
      "[Epoch: 1, Batch:  980 / 55793], loss: 0.631\n",
      "[Epoch: 1, Batch:  990 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 1000 / 55793], loss: 0.650\n",
      "[Epoch: 1, Batch: 1010 / 55793], loss: 0.681\n",
      "[Epoch: 1, Batch: 1020 / 55793], loss: 0.650\n",
      "[Epoch: 1, Batch: 1030 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 1040 / 55793], loss: 0.645\n",
      "[Epoch: 1, Batch: 1050 / 55793], loss: 0.669\n",
      "[Epoch: 1, Batch: 1060 / 55793], loss: 0.676\n",
      "[Epoch: 1, Batch: 1070 / 55793], loss: 0.651\n",
      "[Epoch: 1, Batch: 1080 / 55793], loss: 0.628\n",
      "[Epoch: 1, Batch: 1090 / 55793], loss: 0.692\n",
      "[Epoch: 1, Batch: 1100 / 55793], loss: 0.694\n",
      "[Epoch: 1, Batch: 1110 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 1120 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 1130 / 55793], loss: 0.665\n",
      "[Epoch: 1, Batch: 1140 / 55793], loss: 0.631\n",
      "[Epoch: 1, Batch: 1150 / 55793], loss: 0.731\n",
      "[Epoch: 1, Batch: 1160 / 55793], loss: 0.744\n",
      "[Epoch: 1, Batch: 1170 / 55793], loss: 0.690\n",
      "[Epoch: 1, Batch: 1180 / 55793], loss: 0.683\n",
      "[Epoch: 1, Batch: 1190 / 55793], loss: 0.700\n",
      "[Epoch: 1, Batch: 1200 / 55793], loss: 0.671\n",
      "[Epoch: 1, Batch: 1210 / 55793], loss: 0.662\n",
      "[Epoch: 1, Batch: 1220 / 55793], loss: 0.547\n",
      "[Epoch: 1, Batch: 1230 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 1240 / 55793], loss: 0.618\n",
      "[Epoch: 1, Batch: 1250 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 1260 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 1270 / 55793], loss: 0.662\n",
      "[Epoch: 1, Batch: 1280 / 55793], loss: 0.699\n",
      "[Epoch: 1, Batch: 1290 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 1300 / 55793], loss: 0.663\n",
      "[Epoch: 1, Batch: 1310 / 55793], loss: 0.666\n",
      "[Epoch: 1, Batch: 1320 / 55793], loss: 0.720\n",
      "[Epoch: 1, Batch: 1330 / 55793], loss: 0.705\n",
      "[Epoch: 1, Batch: 1340 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 1350 / 55793], loss: 0.635\n",
      "[Epoch: 1, Batch: 1360 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 1370 / 55793], loss: 0.661\n",
      "[Epoch: 1, Batch: 1380 / 55793], loss: 0.759\n",
      "[Epoch: 1, Batch: 1390 / 55793], loss: 0.639\n",
      "[Epoch: 1, Batch: 1400 / 55793], loss: 0.654\n",
      "[Epoch: 1, Batch: 1410 / 55793], loss: 0.670\n",
      "[Epoch: 1, Batch: 1420 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 1430 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 1440 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 1450 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 1460 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 1470 / 55793], loss: 0.651\n",
      "[Epoch: 1, Batch: 1480 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 1490 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 1500 / 55793], loss: 0.674\n",
      "[Epoch: 1, Batch: 1510 / 55793], loss: 0.573\n",
      "[Epoch: 1, Batch: 1520 / 55793], loss: 0.618\n",
      "[Epoch: 1, Batch: 1530 / 55793], loss: 0.686\n",
      "[Epoch: 1, Batch: 1540 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch: 1550 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 1560 / 55793], loss: 0.666\n",
      "[Epoch: 1, Batch: 1570 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 1580 / 55793], loss: 0.618\n",
      "[Epoch: 1, Batch: 1590 / 55793], loss: 0.549\n",
      "[Epoch: 1, Batch: 1600 / 55793], loss: 0.600\n",
      "[Epoch: 1, Batch: 1610 / 55793], loss: 0.697\n",
      "[Epoch: 1, Batch: 1620 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 1630 / 55793], loss: 0.646\n",
      "[Epoch: 1, Batch: 1640 / 55793], loss: 0.534\n",
      "[Epoch: 1, Batch: 1650 / 55793], loss: 0.700\n",
      "[Epoch: 1, Batch: 1660 / 55793], loss: 0.693\n",
      "[Epoch: 1, Batch: 1670 / 55793], loss: 0.647\n",
      "[Epoch: 1, Batch: 1680 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 1690 / 55793], loss: 0.599\n",
      "[Epoch: 1, Batch: 1700 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 1710 / 55793], loss: 0.679\n",
      "[Epoch: 1, Batch: 1720 / 55793], loss: 0.656\n",
      "[Epoch: 1, Batch: 1730 / 55793], loss: 0.536\n",
      "[Epoch: 1, Batch: 1740 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 1750 / 55793], loss: 0.598\n",
      "[Epoch: 1, Batch: 1760 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 1770 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 1780 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 1790 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch: 1800 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch: 1810 / 55793], loss: 0.713\n",
      "[Epoch: 1, Batch: 1820 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch: 1830 / 55793], loss: 0.547\n",
      "[Epoch: 1, Batch: 1840 / 55793], loss: 0.667\n",
      "[Epoch: 1, Batch: 1850 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 1860 / 55793], loss: 0.650\n",
      "[Epoch: 1, Batch: 1870 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 1880 / 55793], loss: 0.673\n",
      "[Epoch: 1, Batch: 1890 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 1900 / 55793], loss: 0.646\n",
      "[Epoch: 1, Batch: 1910 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 1920 / 55793], loss: 0.639\n",
      "[Epoch: 1, Batch: 1930 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 1940 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch: 1950 / 55793], loss: 0.660\n",
      "[Epoch: 1, Batch: 1960 / 55793], loss: 0.562\n",
      "[Epoch: 1, Batch: 1970 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 1980 / 55793], loss: 0.648\n",
      "[Epoch: 1, Batch: 1990 / 55793], loss: 0.615\n",
      "[Epoch: 1, Batch: 2000 / 55793], loss: 0.642\n",
      "[Epoch: 1, Batch: 2010 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 2020 / 55793], loss: 0.674\n",
      "[Epoch: 1, Batch: 2030 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch: 2040 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 2050 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 2060 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 2070 / 55793], loss: 0.631\n",
      "[Epoch: 1, Batch: 2080 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 2090 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 2100 / 55793], loss: 0.673\n",
      "[Epoch: 1, Batch: 2110 / 55793], loss: 0.627\n",
      "[Epoch: 1, Batch: 2120 / 55793], loss: 0.631\n",
      "[Epoch: 1, Batch: 2130 / 55793], loss: 0.623\n",
      "[Epoch: 1, Batch: 2140 / 55793], loss: 0.680\n",
      "[Epoch: 1, Batch: 2150 / 55793], loss: 0.684\n",
      "[Epoch: 1, Batch: 2160 / 55793], loss: 0.623\n",
      "[Epoch: 1, Batch: 2170 / 55793], loss: 0.704\n",
      "[Epoch: 1, Batch: 2180 / 55793], loss: 0.635\n",
      "[Epoch: 1, Batch: 2190 / 55793], loss: 0.663\n",
      "[Epoch: 1, Batch: 2200 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 2210 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 2220 / 55793], loss: 0.634\n",
      "[Epoch: 1, Batch: 2230 / 55793], loss: 0.646\n",
      "[Epoch: 1, Batch: 2240 / 55793], loss: 0.694\n",
      "[Epoch: 1, Batch: 2250 / 55793], loss: 0.688\n",
      "[Epoch: 1, Batch: 2260 / 55793], loss: 0.656\n",
      "[Epoch: 1, Batch: 2270 / 55793], loss: 0.675\n",
      "[Epoch: 1, Batch: 2280 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 2290 / 55793], loss: 0.680\n",
      "[Epoch: 1, Batch: 2300 / 55793], loss: 0.681\n",
      "[Epoch: 1, Batch: 2310 / 55793], loss: 0.693\n",
      "[Epoch: 1, Batch: 2320 / 55793], loss: 0.675\n",
      "[Epoch: 1, Batch: 2330 / 55793], loss: 0.672\n",
      "[Epoch: 1, Batch: 2340 / 55793], loss: 0.674\n",
      "[Epoch: 1, Batch: 2350 / 55793], loss: 0.695\n",
      "[Epoch: 1, Batch: 2360 / 55793], loss: 0.674\n",
      "[Epoch: 1, Batch: 2370 / 55793], loss: 0.714\n",
      "[Epoch: 1, Batch: 2380 / 55793], loss: 0.627\n",
      "[Epoch: 1, Batch: 2390 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 2400 / 55793], loss: 0.645\n",
      "[Epoch: 1, Batch: 2410 / 55793], loss: 0.618\n",
      "[Epoch: 1, Batch: 2420 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 2430 / 55793], loss: 0.688\n",
      "[Epoch: 1, Batch: 2440 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch: 2450 / 55793], loss: 0.665\n",
      "[Epoch: 1, Batch: 2460 / 55793], loss: 0.656\n",
      "[Epoch: 1, Batch: 2470 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch: 2480 / 55793], loss: 0.689\n",
      "[Epoch: 1, Batch: 2490 / 55793], loss: 0.672\n",
      "[Epoch: 1, Batch: 2500 / 55793], loss: 0.697\n",
      "[Epoch: 1, Batch: 2510 / 55793], loss: 0.700\n",
      "[Epoch: 1, Batch: 2520 / 55793], loss: 0.683\n",
      "[Epoch: 1, Batch: 2530 / 55793], loss: 0.688\n",
      "[Epoch: 1, Batch: 2540 / 55793], loss: 0.704\n",
      "[Epoch: 1, Batch: 2550 / 55793], loss: 0.648\n",
      "[Epoch: 1, Batch: 2560 / 55793], loss: 0.628\n",
      "[Epoch: 1, Batch: 2570 / 55793], loss: 0.603\n",
      "[Epoch: 1, Batch: 2580 / 55793], loss: 0.700\n",
      "[Epoch: 1, Batch: 2590 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 2600 / 55793], loss: 0.788\n",
      "[Epoch: 1, Batch: 2610 / 55793], loss: 0.631\n",
      "[Epoch: 1, Batch: 2620 / 55793], loss: 0.642\n",
      "[Epoch: 1, Batch: 2630 / 55793], loss: 0.714\n",
      "[Epoch: 1, Batch: 2640 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 2650 / 55793], loss: 0.660\n",
      "[Epoch: 1, Batch: 2660 / 55793], loss: 0.650\n",
      "[Epoch: 1, Batch: 2670 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 2680 / 55793], loss: 0.655\n",
      "[Epoch: 1, Batch: 2690 / 55793], loss: 0.699\n",
      "[Epoch: 1, Batch: 2700 / 55793], loss: 0.673\n",
      "[Epoch: 1, Batch: 2710 / 55793], loss: 0.628\n",
      "[Epoch: 1, Batch: 2720 / 55793], loss: 0.688\n",
      "[Epoch: 1, Batch: 2730 / 55793], loss: 0.673\n",
      "[Epoch: 1, Batch: 2740 / 55793], loss: 0.682\n",
      "[Epoch: 1, Batch: 2750 / 55793], loss: 0.613\n",
      "[Epoch: 1, Batch: 2760 / 55793], loss: 0.645\n",
      "[Epoch: 1, Batch: 2770 / 55793], loss: 0.651\n",
      "[Epoch: 1, Batch: 2780 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 2790 / 55793], loss: 0.633\n",
      "[Epoch: 1, Batch: 2800 / 55793], loss: 0.617\n",
      "[Epoch: 1, Batch: 2810 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 2820 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 2830 / 55793], loss: 0.681\n",
      "[Epoch: 1, Batch: 2840 / 55793], loss: 0.623\n",
      "[Epoch: 1, Batch: 2850 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 2860 / 55793], loss: 0.625\n",
      "[Epoch: 1, Batch: 2870 / 55793], loss: 0.654\n",
      "[Epoch: 1, Batch: 2880 / 55793], loss: 0.681\n",
      "[Epoch: 1, Batch: 2890 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch: 2900 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 2910 / 55793], loss: 0.638\n",
      "[Epoch: 1, Batch: 2920 / 55793], loss: 0.639\n",
      "[Epoch: 1, Batch: 2930 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 2940 / 55793], loss: 0.644\n",
      "[Epoch: 1, Batch: 2950 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 2960 / 55793], loss: 0.659\n",
      "[Epoch: 1, Batch: 2970 / 55793], loss: 0.661\n",
      "[Epoch: 1, Batch: 2980 / 55793], loss: 0.664\n",
      "[Epoch: 1, Batch: 2990 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 3000 / 55793], loss: 0.555\n",
      "[Epoch: 1, Batch: 3010 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 3020 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 3030 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 3040 / 55793], loss: 0.680\n",
      "[Epoch: 1, Batch: 3050 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 3060 / 55793], loss: 0.659\n",
      "[Epoch: 1, Batch: 3070 / 55793], loss: 0.562\n",
      "[Epoch: 1, Batch: 3080 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 3090 / 55793], loss: 0.648\n",
      "[Epoch: 1, Batch: 3100 / 55793], loss: 0.684\n",
      "[Epoch: 1, Batch: 3110 / 55793], loss: 0.703\n",
      "[Epoch: 1, Batch: 3120 / 55793], loss: 0.662\n",
      "[Epoch: 1, Batch: 3130 / 55793], loss: 0.727\n",
      "[Epoch: 1, Batch: 3140 / 55793], loss: 0.711\n",
      "[Epoch: 1, Batch: 3150 / 55793], loss: 0.761\n",
      "[Epoch: 1, Batch: 3160 / 55793], loss: 0.712\n",
      "[Epoch: 1, Batch: 3170 / 55793], loss: 0.679\n",
      "[Epoch: 1, Batch: 3180 / 55793], loss: 0.692\n",
      "[Epoch: 1, Batch: 3190 / 55793], loss: 0.679\n",
      "[Epoch: 1, Batch: 3200 / 55793], loss: 0.750\n",
      "[Epoch: 1, Batch: 3210 / 55793], loss: 0.704\n",
      "[Epoch: 1, Batch: 3220 / 55793], loss: 0.676\n",
      "[Epoch: 1, Batch: 3230 / 55793], loss: 0.677\n",
      "[Epoch: 1, Batch: 3240 / 55793], loss: 0.659\n",
      "[Epoch: 1, Batch: 3250 / 55793], loss: 0.621\n",
      "[Epoch: 1, Batch: 3260 / 55793], loss: 0.718\n",
      "[Epoch: 1, Batch: 3270 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 3280 / 55793], loss: 0.657\n",
      "[Epoch: 1, Batch: 3290 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 3300 / 55793], loss: 0.624\n",
      "[Epoch: 1, Batch: 3310 / 55793], loss: 0.657\n",
      "[Epoch: 1, Batch: 3320 / 55793], loss: 0.741\n",
      "[Epoch: 1, Batch: 3330 / 55793], loss: 0.652\n",
      "[Epoch: 1, Batch: 3340 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 3350 / 55793], loss: 0.612\n",
      "[Epoch: 1, Batch: 3360 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 3370 / 55793], loss: 0.774\n",
      "[Epoch: 1, Batch: 3380 / 55793], loss: 0.671\n",
      "[Epoch: 1, Batch: 3390 / 55793], loss: 0.680\n",
      "[Epoch: 1, Batch: 3400 / 55793], loss: 0.657\n",
      "[Epoch: 1, Batch: 3410 / 55793], loss: 0.677\n",
      "[Epoch: 1, Batch: 3420 / 55793], loss: 0.653\n",
      "[Epoch: 1, Batch: 3430 / 55793], loss: 0.650\n",
      "[Epoch: 1, Batch: 3440 / 55793], loss: 0.652\n",
      "[Epoch: 1, Batch: 3450 / 55793], loss: 0.638\n",
      "[Epoch: 1, Batch: 3460 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 3470 / 55793], loss: 0.655\n",
      "[Epoch: 1, Batch: 3480 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 3490 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 3500 / 55793], loss: 0.624\n",
      "[Epoch: 1, Batch: 3510 / 55793], loss: 0.705\n",
      "[Epoch: 1, Batch: 3520 / 55793], loss: 0.664\n",
      "[Epoch: 1, Batch: 3530 / 55793], loss: 0.677\n",
      "[Epoch: 1, Batch: 3540 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 3550 / 55793], loss: 0.671\n",
      "[Epoch: 1, Batch: 3560 / 55793], loss: 0.673\n",
      "[Epoch: 1, Batch: 3570 / 55793], loss: 0.709\n",
      "[Epoch: 1, Batch: 3580 / 55793], loss: 0.613\n",
      "[Epoch: 1, Batch: 3590 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 3600 / 55793], loss: 0.671\n",
      "[Epoch: 1, Batch: 3610 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 3620 / 55793], loss: 0.650\n",
      "[Epoch: 1, Batch: 3630 / 55793], loss: 0.642\n",
      "[Epoch: 1, Batch: 3640 / 55793], loss: 0.667\n",
      "[Epoch: 1, Batch: 3650 / 55793], loss: 0.642\n",
      "[Epoch: 1, Batch: 3660 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 3670 / 55793], loss: 0.660\n",
      "[Epoch: 1, Batch: 3680 / 55793], loss: 0.668\n",
      "[Epoch: 1, Batch: 3690 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch: 3700 / 55793], loss: 0.701\n",
      "[Epoch: 1, Batch: 3710 / 55793], loss: 0.692\n",
      "[Epoch: 1, Batch: 3720 / 55793], loss: 0.639\n",
      "[Epoch: 1, Batch: 3730 / 55793], loss: 0.625\n",
      "[Epoch: 1, Batch: 3740 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 3750 / 55793], loss: 0.737\n",
      "[Epoch: 1, Batch: 3760 / 55793], loss: 0.583\n",
      "[Epoch: 1, Batch: 3770 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 3780 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 3790 / 55793], loss: 0.698\n",
      "[Epoch: 1, Batch: 3800 / 55793], loss: 0.669\n",
      "[Epoch: 1, Batch: 3810 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 3820 / 55793], loss: 0.647\n",
      "[Epoch: 1, Batch: 3830 / 55793], loss: 0.670\n",
      "[Epoch: 1, Batch: 3840 / 55793], loss: 0.709\n",
      "[Epoch: 1, Batch: 3850 / 55793], loss: 0.683\n",
      "[Epoch: 1, Batch: 3860 / 55793], loss: 0.694\n",
      "[Epoch: 1, Batch: 3870 / 55793], loss: 0.638\n",
      "[Epoch: 1, Batch: 3880 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 3890 / 55793], loss: 0.628\n",
      "[Epoch: 1, Batch: 3900 / 55793], loss: 0.735\n",
      "[Epoch: 1, Batch: 3910 / 55793], loss: 0.598\n",
      "[Epoch: 1, Batch: 3920 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 3930 / 55793], loss: 0.679\n",
      "[Epoch: 1, Batch: 3940 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 3950 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 3960 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 3970 / 55793], loss: 0.666\n",
      "[Epoch: 1, Batch: 3980 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 3990 / 55793], loss: 0.672\n",
      "[Epoch: 1, Batch: 4000 / 55793], loss: 0.675\n",
      "[Epoch: 1, Batch: 4010 / 55793], loss: 0.657\n",
      "[Epoch: 1, Batch: 4020 / 55793], loss: 0.670\n",
      "[Epoch: 1, Batch: 4030 / 55793], loss: 0.669\n",
      "[Epoch: 1, Batch: 4040 / 55793], loss: 0.613\n",
      "[Epoch: 1, Batch: 4050 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 4060 / 55793], loss: 0.662\n",
      "[Epoch: 1, Batch: 4070 / 55793], loss: 0.633\n",
      "[Epoch: 1, Batch: 4080 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 4090 / 55793], loss: 0.655\n",
      "[Epoch: 1, Batch: 4100 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 4110 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 4120 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 4130 / 55793], loss: 0.690\n",
      "[Epoch: 1, Batch: 4140 / 55793], loss: 0.670\n",
      "[Epoch: 1, Batch: 4150 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 4160 / 55793], loss: 0.705\n",
      "[Epoch: 1, Batch: 4170 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 4180 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 4190 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 4200 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 4210 / 55793], loss: 0.679\n",
      "[Epoch: 1, Batch: 4220 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 4230 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch: 4240 / 55793], loss: 0.603\n",
      "[Epoch: 1, Batch: 4250 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 4260 / 55793], loss: 0.678\n",
      "[Epoch: 1, Batch: 4270 / 55793], loss: 0.676\n",
      "[Epoch: 1, Batch: 4280 / 55793], loss: 0.696\n",
      "[Epoch: 1, Batch: 4290 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch: 4300 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 4310 / 55793], loss: 0.656\n",
      "[Epoch: 1, Batch: 4320 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 4330 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 4340 / 55793], loss: 0.621\n",
      "[Epoch: 1, Batch: 4350 / 55793], loss: 0.668\n",
      "[Epoch: 1, Batch: 4360 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch: 4370 / 55793], loss: 0.596\n",
      "[Epoch: 1, Batch: 4380 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 4390 / 55793], loss: 0.660\n",
      "[Epoch: 1, Batch: 4400 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 4410 / 55793], loss: 0.707\n",
      "[Epoch: 1, Batch: 4420 / 55793], loss: 0.661\n",
      "[Epoch: 1, Batch: 4430 / 55793], loss: 0.628\n",
      "[Epoch: 1, Batch: 4440 / 55793], loss: 0.628\n",
      "[Epoch: 1, Batch: 4450 / 55793], loss: 0.684\n",
      "[Epoch: 1, Batch: 4460 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 4470 / 55793], loss: 0.667\n",
      "[Epoch: 1, Batch: 4480 / 55793], loss: 0.624\n",
      "[Epoch: 1, Batch: 4490 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 4500 / 55793], loss: 0.655\n",
      "[Epoch: 1, Batch: 4510 / 55793], loss: 0.612\n",
      "[Epoch: 1, Batch: 4520 / 55793], loss: 0.630\n",
      "[Epoch: 1, Batch: 4530 / 55793], loss: 0.625\n",
      "[Epoch: 1, Batch: 4540 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 4550 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 4560 / 55793], loss: 0.654\n",
      "[Epoch: 1, Batch: 4570 / 55793], loss: 0.497\n",
      "[Epoch: 1, Batch: 4580 / 55793], loss: 0.676\n",
      "[Epoch: 1, Batch: 4590 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 4600 / 55793], loss: 0.703\n",
      "[Epoch: 1, Batch: 4610 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 4620 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch: 4630 / 55793], loss: 0.642\n",
      "[Epoch: 1, Batch: 4640 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 4650 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 4660 / 55793], loss: 0.668\n",
      "[Epoch: 1, Batch: 4670 / 55793], loss: 0.673\n",
      "[Epoch: 1, Batch: 4680 / 55793], loss: 0.768\n",
      "[Epoch: 1, Batch: 4690 / 55793], loss: 0.662\n",
      "[Epoch: 1, Batch: 4700 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 4710 / 55793], loss: 0.669\n",
      "[Epoch: 1, Batch: 4720 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 4730 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 4740 / 55793], loss: 0.648\n",
      "[Epoch: 1, Batch: 4750 / 55793], loss: 0.683\n",
      "[Epoch: 1, Batch: 4760 / 55793], loss: 0.578\n",
      "[Epoch: 1, Batch: 4770 / 55793], loss: 0.656\n",
      "[Epoch: 1, Batch: 4780 / 55793], loss: 0.612\n",
      "[Epoch: 1, Batch: 4790 / 55793], loss: 0.635\n",
      "[Epoch: 1, Batch: 4800 / 55793], loss: 0.680\n",
      "[Epoch: 1, Batch: 4810 / 55793], loss: 0.688\n",
      "[Epoch: 1, Batch: 4820 / 55793], loss: 0.704\n",
      "[Epoch: 1, Batch: 4830 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 4840 / 55793], loss: 0.570\n",
      "[Epoch: 1, Batch: 4850 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 4860 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 4870 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 4880 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 4890 / 55793], loss: 0.724\n",
      "[Epoch: 1, Batch: 4900 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 4910 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 4920 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 4930 / 55793], loss: 0.695\n",
      "[Epoch: 1, Batch: 4940 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 4950 / 55793], loss: 0.673\n",
      "[Epoch: 1, Batch: 4960 / 55793], loss: 0.646\n",
      "[Epoch: 1, Batch: 4970 / 55793], loss: 0.674\n",
      "[Epoch: 1, Batch: 4980 / 55793], loss: 0.670\n",
      "[Epoch: 1, Batch: 4990 / 55793], loss: 0.633\n",
      "[Epoch: 1, Batch: 5000 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 5010 / 55793], loss: 0.677\n",
      "[Epoch: 1, Batch: 5020 / 55793], loss: 0.691\n",
      "[Epoch: 1, Batch: 5030 / 55793], loss: 0.639\n",
      "[Epoch: 1, Batch: 5040 / 55793], loss: 0.708\n",
      "[Epoch: 1, Batch: 5050 / 55793], loss: 0.665\n",
      "[Epoch: 1, Batch: 5060 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 5070 / 55793], loss: 0.646\n",
      "[Epoch: 1, Batch: 5080 / 55793], loss: 0.617\n",
      "[Epoch: 1, Batch: 5090 / 55793], loss: 0.674\n",
      "[Epoch: 1, Batch: 5100 / 55793], loss: 0.634\n",
      "[Epoch: 1, Batch: 5110 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 5120 / 55793], loss: 0.542\n",
      "[Epoch: 1, Batch: 5130 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 5140 / 55793], loss: 0.598\n",
      "[Epoch: 1, Batch: 5150 / 55793], loss: 0.694\n",
      "[Epoch: 1, Batch: 5160 / 55793], loss: 0.667\n",
      "[Epoch: 1, Batch: 5170 / 55793], loss: 0.591\n",
      "[Epoch: 1, Batch: 5180 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch: 5190 / 55793], loss: 0.680\n",
      "[Epoch: 1, Batch: 5200 / 55793], loss: 0.656\n",
      "[Epoch: 1, Batch: 5210 / 55793], loss: 0.612\n",
      "[Epoch: 1, Batch: 5220 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch: 5230 / 55793], loss: 0.624\n",
      "[Epoch: 1, Batch: 5240 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 5250 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 5260 / 55793], loss: 0.621\n",
      "[Epoch: 1, Batch: 5270 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch: 5280 / 55793], loss: 0.572\n",
      "[Epoch: 1, Batch: 5290 / 55793], loss: 0.674\n",
      "[Epoch: 1, Batch: 5300 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 5310 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 5320 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 5330 / 55793], loss: 0.555\n",
      "[Epoch: 1, Batch: 5340 / 55793], loss: 0.698\n",
      "[Epoch: 1, Batch: 5350 / 55793], loss: 0.680\n",
      "[Epoch: 1, Batch: 5360 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 5370 / 55793], loss: 0.683\n",
      "[Epoch: 1, Batch: 5380 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 5390 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 5400 / 55793], loss: 0.678\n",
      "[Epoch: 1, Batch: 5410 / 55793], loss: 0.617\n",
      "[Epoch: 1, Batch: 5420 / 55793], loss: 0.621\n",
      "[Epoch: 1, Batch: 5430 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 5440 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 5450 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 5460 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 5470 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 5480 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 5490 / 55793], loss: 0.662\n",
      "[Epoch: 1, Batch: 5500 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch: 5510 / 55793], loss: 0.690\n",
      "[Epoch: 1, Batch: 5520 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 5530 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 5540 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 5550 / 55793], loss: 0.599\n",
      "[Epoch: 1, Batch: 5560 / 55793], loss: 0.678\n",
      "[Epoch: 1, Batch: 5570 / 55793], loss: 0.655\n",
      "[Epoch: 1, Batch: 5580 / 55793], loss: 0.628\n",
      "[Epoch: 1, Batch: 5590 / 55793], loss: 0.650\n",
      "[Epoch: 1, Batch: 5600 / 55793], loss: 0.663\n",
      "[Epoch: 1, Batch: 5610 / 55793], loss: 0.690\n",
      "[Epoch: 1, Batch: 5620 / 55793], loss: 0.625\n",
      "[Epoch: 1, Batch: 5630 / 55793], loss: 0.648\n",
      "[Epoch: 1, Batch: 5640 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 5650 / 55793], loss: 0.630\n",
      "[Epoch: 1, Batch: 5660 / 55793], loss: 0.655\n",
      "[Epoch: 1, Batch: 5670 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 5680 / 55793], loss: 0.705\n",
      "[Epoch: 1, Batch: 5690 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 5700 / 55793], loss: 0.596\n",
      "[Epoch: 1, Batch: 5710 / 55793], loss: 0.671\n",
      "[Epoch: 1, Batch: 5720 / 55793], loss: 0.651\n",
      "[Epoch: 1, Batch: 5730 / 55793], loss: 0.647\n",
      "[Epoch: 1, Batch: 5740 / 55793], loss: 0.653\n",
      "[Epoch: 1, Batch: 5750 / 55793], loss: 0.578\n",
      "[Epoch: 1, Batch: 5760 / 55793], loss: 0.657\n",
      "[Epoch: 1, Batch: 5770 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 5780 / 55793], loss: 0.583\n",
      "[Epoch: 1, Batch: 5790 / 55793], loss: 0.553\n",
      "[Epoch: 1, Batch: 5800 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 5810 / 55793], loss: 0.683\n",
      "[Epoch: 1, Batch: 5820 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 5830 / 55793], loss: 0.687\n",
      "[Epoch: 1, Batch: 5840 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 5850 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 5860 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 5870 / 55793], loss: 0.653\n",
      "[Epoch: 1, Batch: 5880 / 55793], loss: 0.623\n",
      "[Epoch: 1, Batch: 5890 / 55793], loss: 0.690\n",
      "[Epoch: 1, Batch: 5900 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 5910 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 5920 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 5930 / 55793], loss: 0.670\n",
      "[Epoch: 1, Batch: 5940 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 5950 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch: 5960 / 55793], loss: 0.598\n",
      "[Epoch: 1, Batch: 5970 / 55793], loss: 0.680\n",
      "[Epoch: 1, Batch: 5980 / 55793], loss: 0.679\n",
      "[Epoch: 1, Batch: 5990 / 55793], loss: 0.639\n",
      "[Epoch: 1, Batch: 6000 / 55793], loss: 0.688\n",
      "[Epoch: 1, Batch: 6010 / 55793], loss: 0.672\n",
      "[Epoch: 1, Batch: 6020 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 6030 / 55793], loss: 0.618\n",
      "[Epoch: 1, Batch: 6040 / 55793], loss: 0.656\n",
      "[Epoch: 1, Batch: 6050 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 6060 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 6070 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 6080 / 55793], loss: 0.682\n",
      "[Epoch: 1, Batch: 6090 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 6100 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 6110 / 55793], loss: 0.654\n",
      "[Epoch: 1, Batch: 6120 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 6130 / 55793], loss: 0.570\n",
      "[Epoch: 1, Batch: 6140 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch: 6150 / 55793], loss: 0.659\n",
      "[Epoch: 1, Batch: 6160 / 55793], loss: 0.671\n",
      "[Epoch: 1, Batch: 6170 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 6180 / 55793], loss: 0.621\n",
      "[Epoch: 1, Batch: 6190 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 6200 / 55793], loss: 0.615\n",
      "[Epoch: 1, Batch: 6210 / 55793], loss: 0.570\n",
      "[Epoch: 1, Batch: 6220 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 6230 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 6240 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 6250 / 55793], loss: 0.557\n",
      "[Epoch: 1, Batch: 6260 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 6270 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 6280 / 55793], loss: 0.581\n",
      "[Epoch: 1, Batch: 6290 / 55793], loss: 0.565\n",
      "[Epoch: 1, Batch: 6300 / 55793], loss: 0.646\n",
      "[Epoch: 1, Batch: 6310 / 55793], loss: 0.772\n",
      "[Epoch: 1, Batch: 6320 / 55793], loss: 0.698\n",
      "[Epoch: 1, Batch: 6330 / 55793], loss: 0.693\n",
      "[Epoch: 1, Batch: 6340 / 55793], loss: 0.675\n",
      "[Epoch: 1, Batch: 6350 / 55793], loss: 0.683\n",
      "[Epoch: 1, Batch: 6360 / 55793], loss: 0.679\n",
      "[Epoch: 1, Batch: 6370 / 55793], loss: 0.668\n",
      "[Epoch: 1, Batch: 6380 / 55793], loss: 0.612\n",
      "[Epoch: 1, Batch: 6390 / 55793], loss: 0.628\n",
      "[Epoch: 1, Batch: 6400 / 55793], loss: 0.663\n",
      "[Epoch: 1, Batch: 6410 / 55793], loss: 0.660\n",
      "[Epoch: 1, Batch: 6420 / 55793], loss: 0.745\n",
      "[Epoch: 1, Batch: 6430 / 55793], loss: 0.662\n",
      "[Epoch: 1, Batch: 6440 / 55793], loss: 0.696\n",
      "[Epoch: 1, Batch: 6450 / 55793], loss: 0.660\n",
      "[Epoch: 1, Batch: 6460 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 6470 / 55793], loss: 0.699\n",
      "[Epoch: 1, Batch: 6480 / 55793], loss: 0.690\n",
      "[Epoch: 1, Batch: 6490 / 55793], loss: 0.664\n",
      "[Epoch: 1, Batch: 6500 / 55793], loss: 0.665\n",
      "[Epoch: 1, Batch: 6510 / 55793], loss: 0.665\n",
      "[Epoch: 1, Batch: 6520 / 55793], loss: 0.623\n",
      "[Epoch: 1, Batch: 6530 / 55793], loss: 0.639\n",
      "[Epoch: 1, Batch: 6540 / 55793], loss: 0.623\n",
      "[Epoch: 1, Batch: 6550 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 6560 / 55793], loss: 0.630\n",
      "[Epoch: 1, Batch: 6570 / 55793], loss: 0.654\n",
      "[Epoch: 1, Batch: 6580 / 55793], loss: 0.517\n",
      "[Epoch: 1, Batch: 6590 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch: 6600 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 6610 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 6620 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch: 6630 / 55793], loss: 0.648\n",
      "[Epoch: 1, Batch: 6640 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch: 6650 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 6660 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 6670 / 55793], loss: 0.659\n",
      "[Epoch: 1, Batch: 6680 / 55793], loss: 0.684\n",
      "[Epoch: 1, Batch: 6690 / 55793], loss: 0.583\n",
      "[Epoch: 1, Batch: 6700 / 55793], loss: 0.671\n",
      "[Epoch: 1, Batch: 6710 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 6720 / 55793], loss: 0.612\n",
      "[Epoch: 1, Batch: 6730 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 6740 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 6750 / 55793], loss: 0.645\n",
      "[Epoch: 1, Batch: 6760 / 55793], loss: 0.762\n",
      "[Epoch: 1, Batch: 6770 / 55793], loss: 0.691\n",
      "[Epoch: 1, Batch: 6780 / 55793], loss: 0.639\n",
      "[Epoch: 1, Batch: 6790 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 6800 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 6810 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 6820 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 6830 / 55793], loss: 0.534\n",
      "[Epoch: 1, Batch: 6840 / 55793], loss: 0.560\n",
      "[Epoch: 1, Batch: 6850 / 55793], loss: 0.638\n",
      "[Epoch: 1, Batch: 6860 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 6870 / 55793], loss: 0.532\n",
      "[Epoch: 1, Batch: 6880 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 6890 / 55793], loss: 0.701\n",
      "[Epoch: 1, Batch: 6900 / 55793], loss: 0.677\n",
      "[Epoch: 1, Batch: 6910 / 55793], loss: 0.683\n",
      "[Epoch: 1, Batch: 6920 / 55793], loss: 0.671\n",
      "[Epoch: 1, Batch: 6930 / 55793], loss: 0.633\n",
      "[Epoch: 1, Batch: 6940 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 6950 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 6960 / 55793], loss: 0.529\n",
      "[Epoch: 1, Batch: 6970 / 55793], loss: 0.550\n",
      "[Epoch: 1, Batch: 6980 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 6990 / 55793], loss: 0.693\n",
      "[Epoch: 1, Batch: 7000 / 55793], loss: 0.628\n",
      "[Epoch: 1, Batch: 7010 / 55793], loss: 0.689\n",
      "[Epoch: 1, Batch: 7020 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 7030 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 7040 / 55793], loss: 0.524\n",
      "[Epoch: 1, Batch: 7050 / 55793], loss: 0.719\n",
      "[Epoch: 1, Batch: 7060 / 55793], loss: 0.591\n",
      "[Epoch: 1, Batch: 7070 / 55793], loss: 0.557\n",
      "[Epoch: 1, Batch: 7080 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 7090 / 55793], loss: 0.635\n",
      "[Epoch: 1, Batch: 7100 / 55793], loss: 0.675\n",
      "[Epoch: 1, Batch: 7110 / 55793], loss: 0.651\n",
      "[Epoch: 1, Batch: 7120 / 55793], loss: 0.656\n",
      "[Epoch: 1, Batch: 7130 / 55793], loss: 0.634\n",
      "[Epoch: 1, Batch: 7140 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 7150 / 55793], loss: 0.600\n",
      "[Epoch: 1, Batch: 7160 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 7170 / 55793], loss: 0.617\n",
      "[Epoch: 1, Batch: 7180 / 55793], loss: 0.682\n",
      "[Epoch: 1, Batch: 7190 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch: 7200 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 7210 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch: 7220 / 55793], loss: 0.633\n",
      "[Epoch: 1, Batch: 7230 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 7240 / 55793], loss: 0.557\n",
      "[Epoch: 1, Batch: 7250 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch: 7260 / 55793], loss: 0.660\n",
      "[Epoch: 1, Batch: 7270 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 7280 / 55793], loss: 0.567\n",
      "[Epoch: 1, Batch: 7290 / 55793], loss: 0.712\n",
      "[Epoch: 1, Batch: 7300 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 7310 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 7320 / 55793], loss: 0.526\n",
      "[Epoch: 1, Batch: 7330 / 55793], loss: 0.555\n",
      "[Epoch: 1, Batch: 7340 / 55793], loss: 0.591\n",
      "[Epoch: 1, Batch: 7350 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 7360 / 55793], loss: 0.687\n",
      "[Epoch: 1, Batch: 7370 / 55793], loss: 0.690\n",
      "[Epoch: 1, Batch: 7380 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 7390 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 7400 / 55793], loss: 0.638\n",
      "[Epoch: 1, Batch: 7410 / 55793], loss: 0.633\n",
      "[Epoch: 1, Batch: 7420 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 7430 / 55793], loss: 0.615\n",
      "[Epoch: 1, Batch: 7440 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 7450 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 7460 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 7470 / 55793], loss: 0.598\n",
      "[Epoch: 1, Batch: 7480 / 55793], loss: 0.670\n",
      "[Epoch: 1, Batch: 7490 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch: 7500 / 55793], loss: 0.646\n",
      "[Epoch: 1, Batch: 7510 / 55793], loss: 0.598\n",
      "[Epoch: 1, Batch: 7520 / 55793], loss: 0.647\n",
      "[Epoch: 1, Batch: 7530 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 7540 / 55793], loss: 0.661\n",
      "[Epoch: 1, Batch: 7550 / 55793], loss: 0.642\n",
      "[Epoch: 1, Batch: 7560 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 7570 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 7580 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 7590 / 55793], loss: 0.653\n",
      "[Epoch: 1, Batch: 7600 / 55793], loss: 0.650\n",
      "[Epoch: 1, Batch: 7610 / 55793], loss: 0.612\n",
      "[Epoch: 1, Batch: 7620 / 55793], loss: 0.545\n",
      "[Epoch: 1, Batch: 7630 / 55793], loss: 0.591\n",
      "[Epoch: 1, Batch: 7640 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 7650 / 55793], loss: 0.656\n",
      "[Epoch: 1, Batch: 7660 / 55793], loss: 0.697\n",
      "[Epoch: 1, Batch: 7670 / 55793], loss: 0.612\n",
      "[Epoch: 1, Batch: 7680 / 55793], loss: 0.652\n",
      "[Epoch: 1, Batch: 7690 / 55793], loss: 0.578\n",
      "[Epoch: 1, Batch: 7700 / 55793], loss: 0.572\n",
      "[Epoch: 1, Batch: 7710 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 7720 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 7730 / 55793], loss: 0.600\n",
      "[Epoch: 1, Batch: 7740 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch: 7750 / 55793], loss: 0.528\n",
      "[Epoch: 1, Batch: 7760 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 7770 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 7780 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 7790 / 55793], loss: 0.665\n",
      "[Epoch: 1, Batch: 7800 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 7810 / 55793], loss: 0.630\n",
      "[Epoch: 1, Batch: 7820 / 55793], loss: 0.545\n",
      "[Epoch: 1, Batch: 7830 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 7840 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 7850 / 55793], loss: 0.547\n",
      "[Epoch: 1, Batch: 7860 / 55793], loss: 0.654\n",
      "[Epoch: 1, Batch: 7870 / 55793], loss: 0.746\n",
      "[Epoch: 1, Batch: 7880 / 55793], loss: 0.621\n",
      "[Epoch: 1, Batch: 7890 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 7900 / 55793], loss: 0.653\n",
      "[Epoch: 1, Batch: 7910 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch: 7920 / 55793], loss: 0.541\n",
      "[Epoch: 1, Batch: 7930 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 7940 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 7950 / 55793], loss: 0.656\n",
      "[Epoch: 1, Batch: 7960 / 55793], loss: 0.654\n",
      "[Epoch: 1, Batch: 7970 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 7980 / 55793], loss: 0.522\n",
      "[Epoch: 1, Batch: 7990 / 55793], loss: 0.667\n",
      "[Epoch: 1, Batch: 8000 / 55793], loss: 0.545\n",
      "[Epoch: 1, Batch: 8010 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 8020 / 55793], loss: 0.631\n",
      "[Epoch: 1, Batch: 8030 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 8040 / 55793], loss: 0.670\n",
      "[Epoch: 1, Batch: 8050 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 8060 / 55793], loss: 0.666\n",
      "[Epoch: 1, Batch: 8070 / 55793], loss: 0.621\n",
      "[Epoch: 1, Batch: 8080 / 55793], loss: 0.673\n",
      "[Epoch: 1, Batch: 8090 / 55793], loss: 0.683\n",
      "[Epoch: 1, Batch: 8100 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 8110 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 8120 / 55793], loss: 0.514\n",
      "[Epoch: 1, Batch: 8130 / 55793], loss: 0.677\n",
      "[Epoch: 1, Batch: 8140 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 8150 / 55793], loss: 0.520\n",
      "[Epoch: 1, Batch: 8160 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 8170 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 8180 / 55793], loss: 0.661\n",
      "[Epoch: 1, Batch: 8190 / 55793], loss: 0.676\n",
      "[Epoch: 1, Batch: 8200 / 55793], loss: 0.626\n",
      "[Epoch: 1, Batch: 8210 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 8220 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 8230 / 55793], loss: 0.596\n",
      "[Epoch: 1, Batch: 8240 / 55793], loss: 0.630\n",
      "[Epoch: 1, Batch: 8250 / 55793], loss: 0.603\n",
      "[Epoch: 1, Batch: 8260 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 8270 / 55793], loss: 0.712\n",
      "[Epoch: 1, Batch: 8280 / 55793], loss: 0.555\n",
      "[Epoch: 1, Batch: 8290 / 55793], loss: 0.550\n",
      "[Epoch: 1, Batch: 8300 / 55793], loss: 0.634\n",
      "[Epoch: 1, Batch: 8310 / 55793], loss: 0.644\n",
      "[Epoch: 1, Batch: 8320 / 55793], loss: 0.542\n",
      "[Epoch: 1, Batch: 8330 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 8340 / 55793], loss: 0.591\n",
      "[Epoch: 1, Batch: 8350 / 55793], loss: 0.519\n",
      "[Epoch: 1, Batch: 8360 / 55793], loss: 0.600\n",
      "[Epoch: 1, Batch: 8370 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 8380 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 8390 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 8400 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 8410 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 8420 / 55793], loss: 0.663\n",
      "[Epoch: 1, Batch: 8430 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 8440 / 55793], loss: 0.615\n",
      "[Epoch: 1, Batch: 8450 / 55793], loss: 0.562\n",
      "[Epoch: 1, Batch: 8460 / 55793], loss: 0.723\n",
      "[Epoch: 1, Batch: 8470 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 8480 / 55793], loss: 0.627\n",
      "[Epoch: 1, Batch: 8490 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 8500 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 8510 / 55793], loss: 0.565\n",
      "[Epoch: 1, Batch: 8520 / 55793], loss: 0.648\n",
      "[Epoch: 1, Batch: 8530 / 55793], loss: 0.653\n",
      "[Epoch: 1, Batch: 8540 / 55793], loss: 0.624\n",
      "[Epoch: 1, Batch: 8550 / 55793], loss: 0.512\n",
      "[Epoch: 1, Batch: 8560 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 8570 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 8580 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 8590 / 55793], loss: 0.628\n",
      "[Epoch: 1, Batch: 8600 / 55793], loss: 0.471\n",
      "[Epoch: 1, Batch: 8610 / 55793], loss: 0.675\n",
      "[Epoch: 1, Batch: 8620 / 55793], loss: 0.654\n",
      "[Epoch: 1, Batch: 8630 / 55793], loss: 0.522\n",
      "[Epoch: 1, Batch: 8640 / 55793], loss: 0.665\n",
      "[Epoch: 1, Batch: 8650 / 55793], loss: 0.596\n",
      "[Epoch: 1, Batch: 8660 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 8670 / 55793], loss: 0.553\n",
      "[Epoch: 1, Batch: 8680 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 8690 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 8700 / 55793], loss: 0.627\n",
      "[Epoch: 1, Batch: 8710 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 8720 / 55793], loss: 0.618\n",
      "[Epoch: 1, Batch: 8730 / 55793], loss: 0.553\n",
      "[Epoch: 1, Batch: 8740 / 55793], loss: 0.668\n",
      "[Epoch: 1, Batch: 8750 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 8760 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 8770 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 8780 / 55793], loss: 0.598\n",
      "[Epoch: 1, Batch: 8790 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 8800 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 8810 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 8820 / 55793], loss: 0.599\n",
      "[Epoch: 1, Batch: 8830 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch: 8840 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 8850 / 55793], loss: 0.536\n",
      "[Epoch: 1, Batch: 8860 / 55793], loss: 0.644\n",
      "[Epoch: 1, Batch: 8870 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 8880 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 8890 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 8900 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 8910 / 55793], loss: 0.553\n",
      "[Epoch: 1, Batch: 8920 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 8930 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 8940 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 8950 / 55793], loss: 0.638\n",
      "[Epoch: 1, Batch: 8960 / 55793], loss: 0.565\n",
      "[Epoch: 1, Batch: 8970 / 55793], loss: 0.567\n",
      "[Epoch: 1, Batch: 8980 / 55793], loss: 0.715\n",
      "[Epoch: 1, Batch: 8990 / 55793], loss: 0.623\n",
      "[Epoch: 1, Batch: 9000 / 55793], loss: 0.656\n",
      "[Epoch: 1, Batch: 9010 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch: 9020 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 9030 / 55793], loss: 0.646\n",
      "[Epoch: 1, Batch: 9040 / 55793], loss: 0.567\n",
      "[Epoch: 1, Batch: 9050 / 55793], loss: 0.621\n",
      "[Epoch: 1, Batch: 9060 / 55793], loss: 0.573\n",
      "[Epoch: 1, Batch: 9070 / 55793], loss: 0.630\n",
      "[Epoch: 1, Batch: 9080 / 55793], loss: 0.618\n",
      "[Epoch: 1, Batch: 9090 / 55793], loss: 0.645\n",
      "[Epoch: 1, Batch: 9100 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 9110 / 55793], loss: 0.539\n",
      "[Epoch: 1, Batch: 9120 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch: 9130 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 9140 / 55793], loss: 0.492\n",
      "[Epoch: 1, Batch: 9150 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch: 9160 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 9170 / 55793], loss: 0.638\n",
      "[Epoch: 1, Batch: 9180 / 55793], loss: 0.548\n",
      "[Epoch: 1, Batch: 9190 / 55793], loss: 0.528\n",
      "[Epoch: 1, Batch: 9200 / 55793], loss: 0.694\n",
      "[Epoch: 1, Batch: 9210 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 9220 / 55793], loss: 0.538\n",
      "[Epoch: 1, Batch: 9230 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 9240 / 55793], loss: 0.644\n",
      "[Epoch: 1, Batch: 9250 / 55793], loss: 0.551\n",
      "[Epoch: 1, Batch: 9260 / 55793], loss: 0.623\n",
      "[Epoch: 1, Batch: 9270 / 55793], loss: 0.682\n",
      "[Epoch: 1, Batch: 9280 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 9290 / 55793], loss: 0.573\n",
      "[Epoch: 1, Batch: 9300 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 9310 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 9320 / 55793], loss: 0.567\n",
      "[Epoch: 1, Batch: 9330 / 55793], loss: 0.655\n",
      "[Epoch: 1, Batch: 9340 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 9350 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 9360 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 9370 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 9380 / 55793], loss: 0.662\n",
      "[Epoch: 1, Batch: 9390 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 9400 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 9410 / 55793], loss: 0.673\n",
      "[Epoch: 1, Batch: 9420 / 55793], loss: 0.631\n",
      "[Epoch: 1, Batch: 9430 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 9440 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 9450 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 9460 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 9470 / 55793], loss: 0.536\n",
      "[Epoch: 1, Batch: 9480 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 9490 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 9500 / 55793], loss: 0.523\n",
      "[Epoch: 1, Batch: 9510 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 9520 / 55793], loss: 0.688\n",
      "[Epoch: 1, Batch: 9530 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 9540 / 55793], loss: 0.687\n",
      "[Epoch: 1, Batch: 9550 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 9560 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 9570 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 9580 / 55793], loss: 0.525\n",
      "[Epoch: 1, Batch: 9590 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 9600 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 9610 / 55793], loss: 0.663\n",
      "[Epoch: 1, Batch: 9620 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 9630 / 55793], loss: 0.591\n",
      "[Epoch: 1, Batch: 9640 / 55793], loss: 0.652\n",
      "[Epoch: 1, Batch: 9650 / 55793], loss: 0.615\n",
      "[Epoch: 1, Batch: 9660 / 55793], loss: 0.528\n",
      "[Epoch: 1, Batch: 9670 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 9680 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 9690 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 9700 / 55793], loss: 0.555\n",
      "[Epoch: 1, Batch: 9710 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 9720 / 55793], loss: 0.697\n",
      "[Epoch: 1, Batch: 9730 / 55793], loss: 0.572\n",
      "[Epoch: 1, Batch: 9740 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 9750 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 9760 / 55793], loss: 0.652\n",
      "[Epoch: 1, Batch: 9770 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 9780 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 9790 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 9800 / 55793], loss: 0.687\n",
      "[Epoch: 1, Batch: 9810 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 9820 / 55793], loss: 0.625\n",
      "[Epoch: 1, Batch: 9830 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 9840 / 55793], loss: 0.650\n",
      "[Epoch: 1, Batch: 9850 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 9860 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 9870 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 9880 / 55793], loss: 0.553\n",
      "[Epoch: 1, Batch: 9890 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 9900 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 9910 / 55793], loss: 0.638\n",
      "[Epoch: 1, Batch: 9920 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 9930 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 9940 / 55793], loss: 0.598\n",
      "[Epoch: 1, Batch: 9950 / 55793], loss: 0.556\n",
      "[Epoch: 1, Batch: 9960 / 55793], loss: 0.539\n",
      "[Epoch: 1, Batch: 9970 / 55793], loss: 0.664\n",
      "[Epoch: 1, Batch: 9980 / 55793], loss: 0.618\n",
      "[Epoch: 1, Batch: 9990 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 10000 / 55793], loss: 0.644\n",
      "[Epoch: 1, Batch: 10010 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 10020 / 55793], loss: 0.615\n",
      "[Epoch: 1, Batch: 10030 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 10040 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 10050 / 55793], loss: 0.666\n",
      "[Epoch: 1, Batch: 10060 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 10070 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 10080 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 10090 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 10100 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 10110 / 55793], loss: 0.635\n",
      "[Epoch: 1, Batch: 10120 / 55793], loss: 0.536\n",
      "[Epoch: 1, Batch: 10130 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 10140 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 10150 / 55793], loss: 0.599\n",
      "[Epoch: 1, Batch: 10160 / 55793], loss: 0.556\n",
      "[Epoch: 1, Batch: 10170 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 10180 / 55793], loss: 0.615\n",
      "[Epoch: 1, Batch: 10190 / 55793], loss: 0.542\n",
      "[Epoch: 1, Batch: 10200 / 55793], loss: 0.538\n",
      "[Epoch: 1, Batch: 10210 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 10220 / 55793], loss: 0.644\n",
      "[Epoch: 1, Batch: 10230 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 10240 / 55793], loss: 0.563\n",
      "[Epoch: 1, Batch: 10250 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 10260 / 55793], loss: 0.545\n",
      "[Epoch: 1, Batch: 10270 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 10280 / 55793], loss: 0.612\n",
      "[Epoch: 1, Batch: 10290 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch: 10300 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 10310 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch: 10320 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 10330 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 10340 / 55793], loss: 0.702\n",
      "[Epoch: 1, Batch: 10350 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 10360 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch: 10370 / 55793], loss: 0.556\n",
      "[Epoch: 1, Batch: 10380 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 10390 / 55793], loss: 0.644\n",
      "[Epoch: 1, Batch: 10400 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 10410 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 10420 / 55793], loss: 0.603\n",
      "[Epoch: 1, Batch: 10430 / 55793], loss: 0.639\n",
      "[Epoch: 1, Batch: 10440 / 55793], loss: 0.596\n",
      "[Epoch: 1, Batch: 10450 / 55793], loss: 0.560\n",
      "[Epoch: 1, Batch: 10460 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 10470 / 55793], loss: 0.529\n",
      "[Epoch: 1, Batch: 10480 / 55793], loss: 0.535\n",
      "[Epoch: 1, Batch: 10490 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 10500 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 10510 / 55793], loss: 0.624\n",
      "[Epoch: 1, Batch: 10520 / 55793], loss: 0.651\n",
      "[Epoch: 1, Batch: 10530 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch: 10540 / 55793], loss: 0.563\n",
      "[Epoch: 1, Batch: 10550 / 55793], loss: 0.593\n",
      "[Epoch: 1, Batch: 10560 / 55793], loss: 0.600\n",
      "[Epoch: 1, Batch: 10570 / 55793], loss: 0.549\n",
      "[Epoch: 1, Batch: 10580 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 10590 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 10600 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 10610 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 10620 / 55793], loss: 0.534\n",
      "[Epoch: 1, Batch: 10630 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 10640 / 55793], loss: 0.613\n",
      "[Epoch: 1, Batch: 10650 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 10660 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 10670 / 55793], loss: 0.654\n",
      "[Epoch: 1, Batch: 10680 / 55793], loss: 0.627\n",
      "[Epoch: 1, Batch: 10690 / 55793], loss: 0.615\n",
      "[Epoch: 1, Batch: 10700 / 55793], loss: 0.562\n",
      "[Epoch: 1, Batch: 10710 / 55793], loss: 0.617\n",
      "[Epoch: 1, Batch: 10720 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch: 10730 / 55793], loss: 0.583\n",
      "[Epoch: 1, Batch: 10740 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 10750 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 10760 / 55793], loss: 0.550\n",
      "[Epoch: 1, Batch: 10770 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 10780 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 10790 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 10800 / 55793], loss: 0.578\n",
      "[Epoch: 1, Batch: 10810 / 55793], loss: 0.615\n",
      "[Epoch: 1, Batch: 10820 / 55793], loss: 0.512\n",
      "[Epoch: 1, Batch: 10830 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 10840 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 10850 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 10860 / 55793], loss: 0.635\n",
      "[Epoch: 1, Batch: 10870 / 55793], loss: 0.599\n",
      "[Epoch: 1, Batch: 10880 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 10890 / 55793], loss: 0.556\n",
      "[Epoch: 1, Batch: 10900 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 10910 / 55793], loss: 0.672\n",
      "[Epoch: 1, Batch: 10920 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 10930 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch: 10940 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 10950 / 55793], loss: 0.635\n",
      "[Epoch: 1, Batch: 10960 / 55793], loss: 0.549\n",
      "[Epoch: 1, Batch: 10970 / 55793], loss: 0.681\n",
      "[Epoch: 1, Batch: 10980 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 10990 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 11000 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 11010 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 11020 / 55793], loss: 0.613\n",
      "[Epoch: 1, Batch: 11030 / 55793], loss: 0.699\n",
      "[Epoch: 1, Batch: 11040 / 55793], loss: 0.563\n",
      "[Epoch: 1, Batch: 11050 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 11060 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 11070 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 11080 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 11090 / 55793], loss: 0.648\n",
      "[Epoch: 1, Batch: 11100 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 11110 / 55793], loss: 0.674\n",
      "[Epoch: 1, Batch: 11120 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 11130 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 11140 / 55793], loss: 0.539\n",
      "[Epoch: 1, Batch: 11150 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch: 11160 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 11170 / 55793], loss: 0.630\n",
      "[Epoch: 1, Batch: 11180 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 11190 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 11200 / 55793], loss: 0.562\n",
      "[Epoch: 1, Batch: 11210 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 11220 / 55793], loss: 0.578\n",
      "[Epoch: 1, Batch: 11230 / 55793], loss: 0.685\n",
      "[Epoch: 1, Batch: 11240 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 11250 / 55793], loss: 0.531\n",
      "[Epoch: 1, Batch: 11260 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 11270 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 11280 / 55793], loss: 0.573\n",
      "[Epoch: 1, Batch: 11290 / 55793], loss: 0.498\n",
      "[Epoch: 1, Batch: 11300 / 55793], loss: 0.645\n",
      "[Epoch: 1, Batch: 11310 / 55793], loss: 0.523\n",
      "[Epoch: 1, Batch: 11320 / 55793], loss: 0.517\n",
      "[Epoch: 1, Batch: 11330 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 11340 / 55793], loss: 0.613\n",
      "[Epoch: 1, Batch: 11350 / 55793], loss: 0.626\n",
      "[Epoch: 1, Batch: 11360 / 55793], loss: 0.661\n",
      "[Epoch: 1, Batch: 11370 / 55793], loss: 0.572\n",
      "[Epoch: 1, Batch: 11380 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 11390 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 11400 / 55793], loss: 0.491\n",
      "[Epoch: 1, Batch: 11410 / 55793], loss: 0.560\n",
      "[Epoch: 1, Batch: 11420 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 11430 / 55793], loss: 0.652\n",
      "[Epoch: 1, Batch: 11440 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 11450 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 11460 / 55793], loss: 0.556\n",
      "[Epoch: 1, Batch: 11470 / 55793], loss: 0.532\n",
      "[Epoch: 1, Batch: 11480 / 55793], loss: 0.642\n",
      "[Epoch: 1, Batch: 11490 / 55793], loss: 0.617\n",
      "[Epoch: 1, Batch: 11500 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 11510 / 55793], loss: 0.549\n",
      "[Epoch: 1, Batch: 11520 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 11530 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 11540 / 55793], loss: 0.534\n",
      "[Epoch: 1, Batch: 11550 / 55793], loss: 0.600\n",
      "[Epoch: 1, Batch: 11560 / 55793], loss: 0.562\n",
      "[Epoch: 1, Batch: 11570 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 11580 / 55793], loss: 0.642\n",
      "[Epoch: 1, Batch: 11590 / 55793], loss: 0.630\n",
      "[Epoch: 1, Batch: 11600 / 55793], loss: 0.626\n",
      "[Epoch: 1, Batch: 11610 / 55793], loss: 0.513\n",
      "[Epoch: 1, Batch: 11620 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 11630 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 11640 / 55793], loss: 0.527\n",
      "[Epoch: 1, Batch: 11650 / 55793], loss: 0.600\n",
      "[Epoch: 1, Batch: 11660 / 55793], loss: 0.511\n",
      "[Epoch: 1, Batch: 11670 / 55793], loss: 0.511\n",
      "[Epoch: 1, Batch: 11680 / 55793], loss: 0.565\n",
      "[Epoch: 1, Batch: 11690 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 11700 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 11710 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 11720 / 55793], loss: 0.555\n",
      "[Epoch: 1, Batch: 11730 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 11740 / 55793], loss: 0.634\n",
      "[Epoch: 1, Batch: 11750 / 55793], loss: 0.654\n",
      "[Epoch: 1, Batch: 11760 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 11770 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 11780 / 55793], loss: 0.667\n",
      "[Epoch: 1, Batch: 11790 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 11800 / 55793], loss: 0.624\n",
      "[Epoch: 1, Batch: 11810 / 55793], loss: 0.560\n",
      "[Epoch: 1, Batch: 11820 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 11830 / 55793], loss: 0.673\n",
      "[Epoch: 1, Batch: 11840 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 11850 / 55793], loss: 0.565\n",
      "[Epoch: 1, Batch: 11860 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 11870 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 11880 / 55793], loss: 0.634\n",
      "[Epoch: 1, Batch: 11890 / 55793], loss: 0.541\n",
      "[Epoch: 1, Batch: 11900 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 11910 / 55793], loss: 0.515\n",
      "[Epoch: 1, Batch: 11920 / 55793], loss: 0.651\n",
      "[Epoch: 1, Batch: 11930 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 11940 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 11950 / 55793], loss: 0.567\n",
      "[Epoch: 1, Batch: 11960 / 55793], loss: 0.625\n",
      "[Epoch: 1, Batch: 11970 / 55793], loss: 0.638\n",
      "[Epoch: 1, Batch: 11980 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 11990 / 55793], loss: 0.618\n",
      "[Epoch: 1, Batch: 12000 / 55793], loss: 0.545\n",
      "[Epoch: 1, Batch: 12010 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 12020 / 55793], loss: 0.527\n",
      "[Epoch: 1, Batch: 12030 / 55793], loss: 0.536\n",
      "[Epoch: 1, Batch: 12040 / 55793], loss: 0.613\n",
      "[Epoch: 1, Batch: 12050 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 12060 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 12070 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 12080 / 55793], loss: 0.553\n",
      "[Epoch: 1, Batch: 12090 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 12100 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 12110 / 55793], loss: 0.514\n",
      "[Epoch: 1, Batch: 12120 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 12130 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 12140 / 55793], loss: 0.603\n",
      "[Epoch: 1, Batch: 12150 / 55793], loss: 0.546\n",
      "[Epoch: 1, Batch: 12160 / 55793], loss: 0.520\n",
      "[Epoch: 1, Batch: 12170 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 12180 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 12190 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 12200 / 55793], loss: 0.642\n",
      "[Epoch: 1, Batch: 12210 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 12220 / 55793], loss: 0.532\n",
      "[Epoch: 1, Batch: 12230 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 12240 / 55793], loss: 0.682\n",
      "[Epoch: 1, Batch: 12250 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 12260 / 55793], loss: 0.633\n",
      "[Epoch: 1, Batch: 12270 / 55793], loss: 0.507\n",
      "[Epoch: 1, Batch: 12280 / 55793], loss: 0.650\n",
      "[Epoch: 1, Batch: 12290 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 12300 / 55793], loss: 0.562\n",
      "[Epoch: 1, Batch: 12310 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 12320 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 12330 / 55793], loss: 0.526\n",
      "[Epoch: 1, Batch: 12340 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 12350 / 55793], loss: 0.660\n",
      "[Epoch: 1, Batch: 12360 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 12370 / 55793], loss: 0.539\n",
      "[Epoch: 1, Batch: 12380 / 55793], loss: 0.627\n",
      "[Epoch: 1, Batch: 12390 / 55793], loss: 0.553\n",
      "[Epoch: 1, Batch: 12400 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 12410 / 55793], loss: 0.536\n",
      "[Epoch: 1, Batch: 12420 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 12430 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 12440 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 12450 / 55793], loss: 0.593\n",
      "[Epoch: 1, Batch: 12460 / 55793], loss: 0.544\n",
      "[Epoch: 1, Batch: 12470 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 12480 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 12490 / 55793], loss: 0.686\n",
      "[Epoch: 1, Batch: 12500 / 55793], loss: 0.544\n",
      "[Epoch: 1, Batch: 12510 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 12520 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 12530 / 55793], loss: 0.691\n",
      "[Epoch: 1, Batch: 12540 / 55793], loss: 0.631\n",
      "[Epoch: 1, Batch: 12550 / 55793], loss: 0.562\n",
      "[Epoch: 1, Batch: 12560 / 55793], loss: 0.541\n",
      "[Epoch: 1, Batch: 12570 / 55793], loss: 0.727\n",
      "[Epoch: 1, Batch: 12580 / 55793], loss: 0.537\n",
      "[Epoch: 1, Batch: 12590 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 12600 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 12610 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 12620 / 55793], loss: 0.653\n",
      "[Epoch: 1, Batch: 12630 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 12640 / 55793], loss: 0.551\n",
      "[Epoch: 1, Batch: 12650 / 55793], loss: 0.653\n",
      "[Epoch: 1, Batch: 12660 / 55793], loss: 0.565\n",
      "[Epoch: 1, Batch: 12670 / 55793], loss: 0.593\n",
      "[Epoch: 1, Batch: 12680 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 12690 / 55793], loss: 0.546\n",
      "[Epoch: 1, Batch: 12700 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 12710 / 55793], loss: 0.659\n",
      "[Epoch: 1, Batch: 12720 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 12730 / 55793], loss: 0.546\n",
      "[Epoch: 1, Batch: 12740 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 12750 / 55793], loss: 0.615\n",
      "[Epoch: 1, Batch: 12760 / 55793], loss: 0.654\n",
      "[Epoch: 1, Batch: 12770 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 12780 / 55793], loss: 0.542\n",
      "[Epoch: 1, Batch: 12790 / 55793], loss: 0.581\n",
      "[Epoch: 1, Batch: 12800 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 12810 / 55793], loss: 0.631\n",
      "[Epoch: 1, Batch: 12820 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 12830 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 12840 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 12850 / 55793], loss: 0.517\n",
      "[Epoch: 1, Batch: 12860 / 55793], loss: 0.655\n",
      "[Epoch: 1, Batch: 12870 / 55793], loss: 0.484\n",
      "[Epoch: 1, Batch: 12880 / 55793], loss: 0.627\n",
      "[Epoch: 1, Batch: 12890 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 12900 / 55793], loss: 0.534\n",
      "[Epoch: 1, Batch: 12910 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 12920 / 55793], loss: 0.544\n",
      "[Epoch: 1, Batch: 12930 / 55793], loss: 0.651\n",
      "[Epoch: 1, Batch: 12940 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 12950 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 12960 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 12970 / 55793], loss: 0.572\n",
      "[Epoch: 1, Batch: 12980 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 12990 / 55793], loss: 0.563\n",
      "[Epoch: 1, Batch: 13000 / 55793], loss: 0.630\n",
      "[Epoch: 1, Batch: 13010 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 13020 / 55793], loss: 0.539\n",
      "[Epoch: 1, Batch: 13030 / 55793], loss: 0.567\n",
      "[Epoch: 1, Batch: 13040 / 55793], loss: 0.532\n",
      "[Epoch: 1, Batch: 13050 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch: 13060 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 13070 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch: 13080 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 13090 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 13100 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 13110 / 55793], loss: 0.599\n",
      "[Epoch: 1, Batch: 13120 / 55793], loss: 0.531\n",
      "[Epoch: 1, Batch: 13130 / 55793], loss: 0.596\n",
      "[Epoch: 1, Batch: 13140 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch: 13150 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 13160 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 13170 / 55793], loss: 0.603\n",
      "[Epoch: 1, Batch: 13180 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 13190 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 13200 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 13210 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch: 13220 / 55793], loss: 0.560\n",
      "[Epoch: 1, Batch: 13230 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 13240 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 13250 / 55793], loss: 0.536\n",
      "[Epoch: 1, Batch: 13260 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 13270 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 13280 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 13290 / 55793], loss: 0.596\n",
      "[Epoch: 1, Batch: 13300 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 13310 / 55793], loss: 0.557\n",
      "[Epoch: 1, Batch: 13320 / 55793], loss: 0.583\n",
      "[Epoch: 1, Batch: 13330 / 55793], loss: 0.617\n",
      "[Epoch: 1, Batch: 13340 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 13350 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 13360 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 13370 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 13380 / 55793], loss: 0.563\n",
      "[Epoch: 1, Batch: 13390 / 55793], loss: 0.617\n",
      "[Epoch: 1, Batch: 13400 / 55793], loss: 0.659\n",
      "[Epoch: 1, Batch: 13410 / 55793], loss: 0.548\n",
      "[Epoch: 1, Batch: 13420 / 55793], loss: 0.630\n",
      "[Epoch: 1, Batch: 13430 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 13440 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 13450 / 55793], loss: 0.626\n",
      "[Epoch: 1, Batch: 13460 / 55793], loss: 0.667\n",
      "[Epoch: 1, Batch: 13470 / 55793], loss: 0.631\n",
      "[Epoch: 1, Batch: 13480 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 13490 / 55793], loss: 0.507\n",
      "[Epoch: 1, Batch: 13500 / 55793], loss: 0.625\n",
      "[Epoch: 1, Batch: 13510 / 55793], loss: 0.551\n",
      "[Epoch: 1, Batch: 13520 / 55793], loss: 0.593\n",
      "[Epoch: 1, Batch: 13530 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch: 13540 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 13550 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 13560 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 13570 / 55793], loss: 0.532\n",
      "[Epoch: 1, Batch: 13580 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 13590 / 55793], loss: 0.480\n",
      "[Epoch: 1, Batch: 13600 / 55793], loss: 0.621\n",
      "[Epoch: 1, Batch: 13610 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 13620 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 13630 / 55793], loss: 0.680\n",
      "[Epoch: 1, Batch: 13640 / 55793], loss: 0.613\n",
      "[Epoch: 1, Batch: 13650 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 13660 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 13670 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 13680 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 13690 / 55793], loss: 0.618\n",
      "[Epoch: 1, Batch: 13700 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 13710 / 55793], loss: 0.528\n",
      "[Epoch: 1, Batch: 13720 / 55793], loss: 0.573\n",
      "[Epoch: 1, Batch: 13730 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 13740 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 13750 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 13760 / 55793], loss: 0.677\n",
      "[Epoch: 1, Batch: 13770 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 13780 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 13790 / 55793], loss: 0.488\n",
      "[Epoch: 1, Batch: 13800 / 55793], loss: 0.652\n",
      "[Epoch: 1, Batch: 13810 / 55793], loss: 0.660\n",
      "[Epoch: 1, Batch: 13820 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 13830 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 13840 / 55793], loss: 0.618\n",
      "[Epoch: 1, Batch: 13850 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 13860 / 55793], loss: 0.548\n",
      "[Epoch: 1, Batch: 13870 / 55793], loss: 0.496\n",
      "[Epoch: 1, Batch: 13880 / 55793], loss: 0.532\n",
      "[Epoch: 1, Batch: 13890 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 13900 / 55793], loss: 0.526\n",
      "[Epoch: 1, Batch: 13910 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 13920 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 13930 / 55793], loss: 0.527\n",
      "[Epoch: 1, Batch: 13940 / 55793], loss: 0.642\n",
      "[Epoch: 1, Batch: 13950 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 13960 / 55793], loss: 0.553\n",
      "[Epoch: 1, Batch: 13970 / 55793], loss: 0.639\n",
      "[Epoch: 1, Batch: 13980 / 55793], loss: 0.465\n",
      "[Epoch: 1, Batch: 13990 / 55793], loss: 0.541\n",
      "[Epoch: 1, Batch: 14000 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 14010 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 14020 / 55793], loss: 0.647\n",
      "[Epoch: 1, Batch: 14030 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch: 14040 / 55793], loss: 0.629\n",
      "[Epoch: 1, Batch: 14050 / 55793], loss: 0.647\n",
      "[Epoch: 1, Batch: 14060 / 55793], loss: 0.599\n",
      "[Epoch: 1, Batch: 14070 / 55793], loss: 0.681\n",
      "[Epoch: 1, Batch: 14080 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 14090 / 55793], loss: 0.600\n",
      "[Epoch: 1, Batch: 14100 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 14110 / 55793], loss: 0.535\n",
      "[Epoch: 1, Batch: 14120 / 55793], loss: 0.621\n",
      "[Epoch: 1, Batch: 14130 / 55793], loss: 0.573\n",
      "[Epoch: 1, Batch: 14140 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 14150 / 55793], loss: 0.525\n",
      "[Epoch: 1, Batch: 14160 / 55793], loss: 0.646\n",
      "[Epoch: 1, Batch: 14170 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 14180 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 14190 / 55793], loss: 0.537\n",
      "[Epoch: 1, Batch: 14200 / 55793], loss: 0.517\n",
      "[Epoch: 1, Batch: 14210 / 55793], loss: 0.524\n",
      "[Epoch: 1, Batch: 14220 / 55793], loss: 0.473\n",
      "[Epoch: 1, Batch: 14230 / 55793], loss: 0.547\n",
      "[Epoch: 1, Batch: 14240 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 14250 / 55793], loss: 0.551\n",
      "[Epoch: 1, Batch: 14260 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 14270 / 55793], loss: 0.562\n",
      "[Epoch: 1, Batch: 14280 / 55793], loss: 0.626\n",
      "[Epoch: 1, Batch: 14290 / 55793], loss: 0.511\n",
      "[Epoch: 1, Batch: 14300 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 14310 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 14320 / 55793], loss: 0.625\n",
      "[Epoch: 1, Batch: 14330 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 14340 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 14350 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 14360 / 55793], loss: 0.527\n",
      "[Epoch: 1, Batch: 14370 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 14380 / 55793], loss: 0.647\n",
      "[Epoch: 1, Batch: 14390 / 55793], loss: 0.548\n",
      "[Epoch: 1, Batch: 14400 / 55793], loss: 0.557\n",
      "[Epoch: 1, Batch: 14410 / 55793], loss: 0.556\n",
      "[Epoch: 1, Batch: 14420 / 55793], loss: 0.712\n",
      "[Epoch: 1, Batch: 14430 / 55793], loss: 0.691\n",
      "[Epoch: 1, Batch: 14440 / 55793], loss: 0.678\n",
      "[Epoch: 1, Batch: 14450 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 14460 / 55793], loss: 0.652\n",
      "[Epoch: 1, Batch: 14470 / 55793], loss: 0.635\n",
      "[Epoch: 1, Batch: 14480 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 14490 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 14500 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 14510 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 14520 / 55793], loss: 0.613\n",
      "[Epoch: 1, Batch: 14530 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 14540 / 55793], loss: 0.526\n",
      "[Epoch: 1, Batch: 14550 / 55793], loss: 0.570\n",
      "[Epoch: 1, Batch: 14560 / 55793], loss: 0.563\n",
      "[Epoch: 1, Batch: 14570 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 14580 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 14590 / 55793], loss: 0.529\n",
      "[Epoch: 1, Batch: 14600 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 14610 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 14620 / 55793], loss: 0.599\n",
      "[Epoch: 1, Batch: 14630 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 14640 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 14650 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 14660 / 55793], loss: 0.560\n",
      "[Epoch: 1, Batch: 14670 / 55793], loss: 0.637\n",
      "[Epoch: 1, Batch: 14680 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 14690 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 14700 / 55793], loss: 0.593\n",
      "[Epoch: 1, Batch: 14710 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 14720 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 14730 / 55793], loss: 0.509\n",
      "[Epoch: 1, Batch: 14740 / 55793], loss: 0.645\n",
      "[Epoch: 1, Batch: 14750 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 14760 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 14770 / 55793], loss: 0.625\n",
      "[Epoch: 1, Batch: 14780 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 14790 / 55793], loss: 0.534\n",
      "[Epoch: 1, Batch: 14800 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 14810 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 14820 / 55793], loss: 0.583\n",
      "[Epoch: 1, Batch: 14830 / 55793], loss: 0.680\n",
      "[Epoch: 1, Batch: 14840 / 55793], loss: 0.505\n",
      "[Epoch: 1, Batch: 14850 / 55793], loss: 0.624\n",
      "[Epoch: 1, Batch: 14860 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 14870 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 14880 / 55793], loss: 0.647\n",
      "[Epoch: 1, Batch: 14890 / 55793], loss: 0.516\n",
      "[Epoch: 1, Batch: 14900 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 14910 / 55793], loss: 0.536\n",
      "[Epoch: 1, Batch: 14920 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 14930 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 14940 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 14950 / 55793], loss: 0.631\n",
      "[Epoch: 1, Batch: 14960 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 14970 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 14980 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 14990 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 15000 / 55793], loss: 0.556\n",
      "[Epoch: 1, Batch: 15010 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 15020 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 15030 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 15040 / 55793], loss: 0.656\n",
      "[Epoch: 1, Batch: 15050 / 55793], loss: 0.652\n",
      "[Epoch: 1, Batch: 15060 / 55793], loss: 0.659\n",
      "[Epoch: 1, Batch: 15070 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 15080 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 15090 / 55793], loss: 0.578\n",
      "[Epoch: 1, Batch: 15100 / 55793], loss: 0.628\n",
      "[Epoch: 1, Batch: 15110 / 55793], loss: 0.603\n",
      "[Epoch: 1, Batch: 15120 / 55793], loss: 0.613\n",
      "[Epoch: 1, Batch: 15130 / 55793], loss: 0.537\n",
      "[Epoch: 1, Batch: 15140 / 55793], loss: 0.593\n",
      "[Epoch: 1, Batch: 15150 / 55793], loss: 0.562\n",
      "[Epoch: 1, Batch: 15160 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 15170 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 15180 / 55793], loss: 0.536\n",
      "[Epoch: 1, Batch: 15190 / 55793], loss: 0.598\n",
      "[Epoch: 1, Batch: 15200 / 55793], loss: 0.497\n",
      "[Epoch: 1, Batch: 15210 / 55793], loss: 0.591\n",
      "[Epoch: 1, Batch: 15220 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 15230 / 55793], loss: 0.510\n",
      "[Epoch: 1, Batch: 15240 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 15250 / 55793], loss: 0.556\n",
      "[Epoch: 1, Batch: 15260 / 55793], loss: 0.549\n",
      "[Epoch: 1, Batch: 15270 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 15280 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 15290 / 55793], loss: 0.524\n",
      "[Epoch: 1, Batch: 15300 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 15310 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 15320 / 55793], loss: 0.497\n",
      "[Epoch: 1, Batch: 15330 / 55793], loss: 0.628\n",
      "[Epoch: 1, Batch: 15340 / 55793], loss: 0.573\n",
      "[Epoch: 1, Batch: 15350 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 15360 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 15370 / 55793], loss: 0.623\n",
      "[Epoch: 1, Batch: 15380 / 55793], loss: 0.634\n",
      "[Epoch: 1, Batch: 15390 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 15400 / 55793], loss: 0.644\n",
      "[Epoch: 1, Batch: 15410 / 55793], loss: 0.520\n",
      "[Epoch: 1, Batch: 15420 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 15430 / 55793], loss: 0.460\n",
      "[Epoch: 1, Batch: 15440 / 55793], loss: 0.648\n",
      "[Epoch: 1, Batch: 15450 / 55793], loss: 0.515\n",
      "[Epoch: 1, Batch: 15460 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 15470 / 55793], loss: 0.506\n",
      "[Epoch: 1, Batch: 15480 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch: 15490 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 15500 / 55793], loss: 0.624\n",
      "[Epoch: 1, Batch: 15510 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 15520 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 15530 / 55793], loss: 0.516\n",
      "[Epoch: 1, Batch: 15540 / 55793], loss: 0.457\n",
      "[Epoch: 1, Batch: 15550 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 15560 / 55793], loss: 0.663\n",
      "[Epoch: 1, Batch: 15570 / 55793], loss: 0.627\n",
      "[Epoch: 1, Batch: 15580 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 15590 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 15600 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 15610 / 55793], loss: 0.603\n",
      "[Epoch: 1, Batch: 15620 / 55793], loss: 0.627\n",
      "[Epoch: 1, Batch: 15630 / 55793], loss: 0.535\n",
      "[Epoch: 1, Batch: 15640 / 55793], loss: 0.591\n",
      "[Epoch: 1, Batch: 15650 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 15660 / 55793], loss: 0.591\n",
      "[Epoch: 1, Batch: 15670 / 55793], loss: 0.519\n",
      "[Epoch: 1, Batch: 15680 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 15690 / 55793], loss: 0.528\n",
      "[Epoch: 1, Batch: 15700 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 15710 / 55793], loss: 0.581\n",
      "[Epoch: 1, Batch: 15720 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 15730 / 55793], loss: 0.628\n",
      "[Epoch: 1, Batch: 15740 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 15750 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 15760 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 15770 / 55793], loss: 0.524\n",
      "[Epoch: 1, Batch: 15780 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 15790 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 15800 / 55793], loss: 0.485\n",
      "[Epoch: 1, Batch: 15810 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 15820 / 55793], loss: 0.545\n",
      "[Epoch: 1, Batch: 15830 / 55793], loss: 0.680\n",
      "[Epoch: 1, Batch: 15840 / 55793], loss: 0.473\n",
      "[Epoch: 1, Batch: 15850 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 15860 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 15870 / 55793], loss: 0.555\n",
      "[Epoch: 1, Batch: 15880 / 55793], loss: 0.694\n",
      "[Epoch: 1, Batch: 15890 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 15900 / 55793], loss: 0.599\n",
      "[Epoch: 1, Batch: 15910 / 55793], loss: 0.525\n",
      "[Epoch: 1, Batch: 15920 / 55793], loss: 0.544\n",
      "[Epoch: 1, Batch: 15930 / 55793], loss: 0.518\n",
      "[Epoch: 1, Batch: 15940 / 55793], loss: 0.612\n",
      "[Epoch: 1, Batch: 15950 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 15960 / 55793], loss: 0.470\n",
      "[Epoch: 1, Batch: 15970 / 55793], loss: 0.600\n",
      "[Epoch: 1, Batch: 15980 / 55793], loss: 0.555\n",
      "[Epoch: 1, Batch: 15990 / 55793], loss: 0.593\n",
      "[Epoch: 1, Batch: 16000 / 55793], loss: 0.535\n",
      "[Epoch: 1, Batch: 16010 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 16020 / 55793], loss: 0.537\n",
      "[Epoch: 1, Batch: 16030 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 16040 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 16050 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 16060 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 16070 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 16080 / 55793], loss: 0.583\n",
      "[Epoch: 1, Batch: 16090 / 55793], loss: 0.555\n",
      "[Epoch: 1, Batch: 16100 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 16110 / 55793], loss: 0.547\n",
      "[Epoch: 1, Batch: 16120 / 55793], loss: 0.628\n",
      "[Epoch: 1, Batch: 16130 / 55793], loss: 0.541\n",
      "[Epoch: 1, Batch: 16140 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 16150 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 16160 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 16170 / 55793], loss: 0.487\n",
      "[Epoch: 1, Batch: 16180 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 16190 / 55793], loss: 0.510\n",
      "[Epoch: 1, Batch: 16200 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 16210 / 55793], loss: 0.524\n",
      "[Epoch: 1, Batch: 16220 / 55793], loss: 0.551\n",
      "[Epoch: 1, Batch: 16230 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 16240 / 55793], loss: 0.512\n",
      "[Epoch: 1, Batch: 16250 / 55793], loss: 0.531\n",
      "[Epoch: 1, Batch: 16260 / 55793], loss: 0.499\n",
      "[Epoch: 1, Batch: 16270 / 55793], loss: 0.530\n",
      "[Epoch: 1, Batch: 16280 / 55793], loss: 0.527\n",
      "[Epoch: 1, Batch: 16290 / 55793], loss: 0.573\n",
      "[Epoch: 1, Batch: 16300 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 16310 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 16320 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 16330 / 55793], loss: 0.557\n",
      "[Epoch: 1, Batch: 16340 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 16350 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 16360 / 55793], loss: 0.529\n",
      "[Epoch: 1, Batch: 16370 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 16380 / 55793], loss: 0.651\n",
      "[Epoch: 1, Batch: 16390 / 55793], loss: 0.600\n",
      "[Epoch: 1, Batch: 16400 / 55793], loss: 0.538\n",
      "[Epoch: 1, Batch: 16410 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 16420 / 55793], loss: 0.530\n",
      "[Epoch: 1, Batch: 16430 / 55793], loss: 0.591\n",
      "[Epoch: 1, Batch: 16440 / 55793], loss: 0.562\n",
      "[Epoch: 1, Batch: 16450 / 55793], loss: 0.615\n",
      "[Epoch: 1, Batch: 16460 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 16470 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 16480 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 16490 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 16500 / 55793], loss: 0.525\n",
      "[Epoch: 1, Batch: 16510 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 16520 / 55793], loss: 0.499\n",
      "[Epoch: 1, Batch: 16530 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 16540 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 16550 / 55793], loss: 0.522\n",
      "[Epoch: 1, Batch: 16560 / 55793], loss: 0.645\n",
      "[Epoch: 1, Batch: 16570 / 55793], loss: 0.600\n",
      "[Epoch: 1, Batch: 16580 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 16590 / 55793], loss: 0.520\n",
      "[Epoch: 1, Batch: 16600 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch: 16610 / 55793], loss: 0.510\n",
      "[Epoch: 1, Batch: 16620 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 16630 / 55793], loss: 0.630\n",
      "[Epoch: 1, Batch: 16640 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 16650 / 55793], loss: 0.555\n",
      "[Epoch: 1, Batch: 16660 / 55793], loss: 0.557\n",
      "[Epoch: 1, Batch: 16670 / 55793], loss: 0.484\n",
      "[Epoch: 1, Batch: 16680 / 55793], loss: 0.651\n",
      "[Epoch: 1, Batch: 16690 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 16700 / 55793], loss: 0.653\n",
      "[Epoch: 1, Batch: 16710 / 55793], loss: 0.578\n",
      "[Epoch: 1, Batch: 16720 / 55793], loss: 0.565\n",
      "[Epoch: 1, Batch: 16730 / 55793], loss: 0.550\n",
      "[Epoch: 1, Batch: 16740 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 16750 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 16760 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 16770 / 55793], loss: 0.547\n",
      "[Epoch: 1, Batch: 16780 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 16790 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 16800 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 16810 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 16820 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 16830 / 55793], loss: 0.660\n",
      "[Epoch: 1, Batch: 16840 / 55793], loss: 0.539\n",
      "[Epoch: 1, Batch: 16850 / 55793], loss: 0.551\n",
      "[Epoch: 1, Batch: 16860 / 55793], loss: 0.581\n",
      "[Epoch: 1, Batch: 16870 / 55793], loss: 0.516\n",
      "[Epoch: 1, Batch: 16880 / 55793], loss: 0.541\n",
      "[Epoch: 1, Batch: 16890 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 16900 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 16910 / 55793], loss: 0.524\n",
      "[Epoch: 1, Batch: 16920 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 16930 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 16940 / 55793], loss: 0.600\n",
      "[Epoch: 1, Batch: 16950 / 55793], loss: 0.495\n",
      "[Epoch: 1, Batch: 16960 / 55793], loss: 0.572\n",
      "[Epoch: 1, Batch: 16970 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 16980 / 55793], loss: 0.567\n",
      "[Epoch: 1, Batch: 16990 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 17000 / 55793], loss: 0.528\n",
      "[Epoch: 1, Batch: 17010 / 55793], loss: 0.653\n",
      "[Epoch: 1, Batch: 17020 / 55793], loss: 0.537\n",
      "[Epoch: 1, Batch: 17030 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 17040 / 55793], loss: 0.651\n",
      "[Epoch: 1, Batch: 17050 / 55793], loss: 0.501\n",
      "[Epoch: 1, Batch: 17060 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 17070 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 17080 / 55793], loss: 0.661\n",
      "[Epoch: 1, Batch: 17090 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 17100 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 17110 / 55793], loss: 0.617\n",
      "[Epoch: 1, Batch: 17120 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 17130 / 55793], loss: 0.583\n",
      "[Epoch: 1, Batch: 17140 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch: 17150 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 17160 / 55793], loss: 0.544\n",
      "[Epoch: 1, Batch: 17170 / 55793], loss: 0.524\n",
      "[Epoch: 1, Batch: 17180 / 55793], loss: 0.633\n",
      "[Epoch: 1, Batch: 17190 / 55793], loss: 0.468\n",
      "[Epoch: 1, Batch: 17200 / 55793], loss: 0.489\n",
      "[Epoch: 1, Batch: 17210 / 55793], loss: 0.672\n",
      "[Epoch: 1, Batch: 17220 / 55793], loss: 0.544\n",
      "[Epoch: 1, Batch: 17230 / 55793], loss: 0.500\n",
      "[Epoch: 1, Batch: 17240 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 17250 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 17260 / 55793], loss: 0.581\n",
      "[Epoch: 1, Batch: 17270 / 55793], loss: 0.638\n",
      "[Epoch: 1, Batch: 17280 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 17290 / 55793], loss: 0.503\n",
      "[Epoch: 1, Batch: 17300 / 55793], loss: 0.596\n",
      "[Epoch: 1, Batch: 17310 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 17320 / 55793], loss: 0.547\n",
      "[Epoch: 1, Batch: 17330 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 17340 / 55793], loss: 0.625\n",
      "[Epoch: 1, Batch: 17350 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 17360 / 55793], loss: 0.567\n",
      "[Epoch: 1, Batch: 17370 / 55793], loss: 0.527\n",
      "[Epoch: 1, Batch: 17380 / 55793], loss: 0.631\n",
      "[Epoch: 1, Batch: 17390 / 55793], loss: 0.606\n",
      "[Epoch: 1, Batch: 17400 / 55793], loss: 0.612\n",
      "[Epoch: 1, Batch: 17410 / 55793], loss: 0.542\n",
      "[Epoch: 1, Batch: 17420 / 55793], loss: 0.621\n",
      "[Epoch: 1, Batch: 17430 / 55793], loss: 0.516\n",
      "[Epoch: 1, Batch: 17440 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 17450 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 17460 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 17470 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 17480 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 17490 / 55793], loss: 0.512\n",
      "[Epoch: 1, Batch: 17500 / 55793], loss: 0.567\n",
      "[Epoch: 1, Batch: 17510 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 17520 / 55793], loss: 0.704\n",
      "[Epoch: 1, Batch: 17530 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 17540 / 55793], loss: 0.598\n",
      "[Epoch: 1, Batch: 17550 / 55793], loss: 0.478\n",
      "[Epoch: 1, Batch: 17560 / 55793], loss: 0.507\n",
      "[Epoch: 1, Batch: 17570 / 55793], loss: 0.490\n",
      "[Epoch: 1, Batch: 17580 / 55793], loss: 0.508\n",
      "[Epoch: 1, Batch: 17590 / 55793], loss: 0.677\n",
      "[Epoch: 1, Batch: 17600 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 17610 / 55793], loss: 0.603\n",
      "[Epoch: 1, Batch: 17620 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 17630 / 55793], loss: 0.572\n",
      "[Epoch: 1, Batch: 17640 / 55793], loss: 0.658\n",
      "[Epoch: 1, Batch: 17650 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 17660 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 17670 / 55793], loss: 0.572\n",
      "[Epoch: 1, Batch: 17680 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 17690 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 17700 / 55793], loss: 0.494\n",
      "[Epoch: 1, Batch: 17710 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 17720 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 17730 / 55793], loss: 0.687\n",
      "[Epoch: 1, Batch: 17740 / 55793], loss: 0.461\n",
      "[Epoch: 1, Batch: 17750 / 55793], loss: 0.557\n",
      "[Epoch: 1, Batch: 17760 / 55793], loss: 0.548\n",
      "[Epoch: 1, Batch: 17770 / 55793], loss: 0.667\n",
      "[Epoch: 1, Batch: 17780 / 55793], loss: 0.560\n",
      "[Epoch: 1, Batch: 17790 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 17800 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 17810 / 55793], loss: 0.638\n",
      "[Epoch: 1, Batch: 17820 / 55793], loss: 0.581\n",
      "[Epoch: 1, Batch: 17830 / 55793], loss: 0.642\n",
      "[Epoch: 1, Batch: 17840 / 55793], loss: 0.560\n",
      "[Epoch: 1, Batch: 17850 / 55793], loss: 0.523\n",
      "[Epoch: 1, Batch: 17860 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 17870 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 17880 / 55793], loss: 0.680\n",
      "[Epoch: 1, Batch: 17890 / 55793], loss: 0.526\n",
      "[Epoch: 1, Batch: 17900 / 55793], loss: 0.650\n",
      "[Epoch: 1, Batch: 17910 / 55793], loss: 0.511\n",
      "[Epoch: 1, Batch: 17920 / 55793], loss: 0.510\n",
      "[Epoch: 1, Batch: 17930 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 17940 / 55793], loss: 0.492\n",
      "[Epoch: 1, Batch: 17950 / 55793], loss: 0.664\n",
      "[Epoch: 1, Batch: 17960 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 17970 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 17980 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 17990 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 18000 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 18010 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 18020 / 55793], loss: 0.502\n",
      "[Epoch: 1, Batch: 18030 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 18040 / 55793], loss: 0.518\n",
      "[Epoch: 1, Batch: 18050 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 18060 / 55793], loss: 0.527\n",
      "[Epoch: 1, Batch: 18070 / 55793], loss: 0.504\n",
      "[Epoch: 1, Batch: 18080 / 55793], loss: 0.555\n",
      "[Epoch: 1, Batch: 18090 / 55793], loss: 0.650\n",
      "[Epoch: 1, Batch: 18100 / 55793], loss: 0.666\n",
      "[Epoch: 1, Batch: 18110 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 18120 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 18130 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 18140 / 55793], loss: 0.545\n",
      "[Epoch: 1, Batch: 18150 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 18160 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 18170 / 55793], loss: 0.535\n",
      "[Epoch: 1, Batch: 18180 / 55793], loss: 0.538\n",
      "[Epoch: 1, Batch: 18190 / 55793], loss: 0.625\n",
      "[Epoch: 1, Batch: 18200 / 55793], loss: 0.538\n",
      "[Epoch: 1, Batch: 18210 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 18220 / 55793], loss: 0.700\n",
      "[Epoch: 1, Batch: 18230 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 18240 / 55793], loss: 0.540\n",
      "[Epoch: 1, Batch: 18250 / 55793], loss: 0.549\n",
      "[Epoch: 1, Batch: 18260 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 18270 / 55793], loss: 0.653\n",
      "[Epoch: 1, Batch: 18280 / 55793], loss: 0.634\n",
      "[Epoch: 1, Batch: 18290 / 55793], loss: 0.565\n",
      "[Epoch: 1, Batch: 18300 / 55793], loss: 0.624\n",
      "[Epoch: 1, Batch: 18310 / 55793], loss: 0.596\n",
      "[Epoch: 1, Batch: 18320 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 18330 / 55793], loss: 0.520\n",
      "[Epoch: 1, Batch: 18340 / 55793], loss: 0.544\n",
      "[Epoch: 1, Batch: 18350 / 55793], loss: 0.565\n",
      "[Epoch: 1, Batch: 18360 / 55793], loss: 0.692\n",
      "[Epoch: 1, Batch: 18370 / 55793], loss: 0.562\n",
      "[Epoch: 1, Batch: 18380 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 18390 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 18400 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 18410 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 18420 / 55793], loss: 0.593\n",
      "[Epoch: 1, Batch: 18430 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 18440 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 18450 / 55793], loss: 0.645\n",
      "[Epoch: 1, Batch: 18460 / 55793], loss: 0.648\n",
      "[Epoch: 1, Batch: 18470 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 18480 / 55793], loss: 0.522\n",
      "[Epoch: 1, Batch: 18490 / 55793], loss: 0.573\n",
      "[Epoch: 1, Batch: 18500 / 55793], loss: 0.553\n",
      "[Epoch: 1, Batch: 18510 / 55793], loss: 0.548\n",
      "[Epoch: 1, Batch: 18520 / 55793], loss: 0.625\n",
      "[Epoch: 1, Batch: 18530 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 18540 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 18550 / 55793], loss: 0.605\n",
      "[Epoch: 1, Batch: 18560 / 55793], loss: 0.535\n",
      "[Epoch: 1, Batch: 18570 / 55793], loss: 0.626\n",
      "[Epoch: 1, Batch: 18580 / 55793], loss: 0.682\n",
      "[Epoch: 1, Batch: 18590 / 55793], loss: 0.538\n",
      "[Epoch: 1, Batch: 18600 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 18610 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 18620 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 18630 / 55793], loss: 0.684\n",
      "[Epoch: 1, Batch: 18640 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 18650 / 55793], loss: 0.652\n",
      "[Epoch: 1, Batch: 18660 / 55793], loss: 0.572\n",
      "[Epoch: 1, Batch: 18670 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 18680 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 18690 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 18700 / 55793], loss: 0.522\n",
      "[Epoch: 1, Batch: 18710 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 18720 / 55793], loss: 0.482\n",
      "[Epoch: 1, Batch: 18730 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 18740 / 55793], loss: 0.525\n",
      "[Epoch: 1, Batch: 18750 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 18760 / 55793], loss: 0.527\n",
      "[Epoch: 1, Batch: 18770 / 55793], loss: 0.544\n",
      "[Epoch: 1, Batch: 18780 / 55793], loss: 0.615\n",
      "[Epoch: 1, Batch: 18790 / 55793], loss: 0.508\n",
      "[Epoch: 1, Batch: 18800 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 18810 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 18820 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 18830 / 55793], loss: 0.599\n",
      "[Epoch: 1, Batch: 18840 / 55793], loss: 0.490\n",
      "[Epoch: 1, Batch: 18850 / 55793], loss: 0.514\n",
      "[Epoch: 1, Batch: 18860 / 55793], loss: 0.603\n",
      "[Epoch: 1, Batch: 18870 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 18880 / 55793], loss: 0.581\n",
      "[Epoch: 1, Batch: 18890 / 55793], loss: 0.513\n",
      "[Epoch: 1, Batch: 18900 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 18910 / 55793], loss: 0.680\n",
      "[Epoch: 1, Batch: 18920 / 55793], loss: 0.539\n",
      "[Epoch: 1, Batch: 18930 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 18940 / 55793], loss: 0.624\n",
      "[Epoch: 1, Batch: 18950 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 18960 / 55793], loss: 0.548\n",
      "[Epoch: 1, Batch: 18970 / 55793], loss: 0.538\n",
      "[Epoch: 1, Batch: 18980 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 18990 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 19000 / 55793], loss: 0.551\n",
      "[Epoch: 1, Batch: 19010 / 55793], loss: 0.696\n",
      "[Epoch: 1, Batch: 19020 / 55793], loss: 0.532\n",
      "[Epoch: 1, Batch: 19030 / 55793], loss: 0.532\n",
      "[Epoch: 1, Batch: 19040 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 19050 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 19060 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 19070 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 19080 / 55793], loss: 0.538\n",
      "[Epoch: 1, Batch: 19090 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 19100 / 55793], loss: 0.531\n",
      "[Epoch: 1, Batch: 19110 / 55793], loss: 0.638\n",
      "[Epoch: 1, Batch: 19120 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 19130 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 19140 / 55793], loss: 0.593\n",
      "[Epoch: 1, Batch: 19150 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 19160 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 19170 / 55793], loss: 0.538\n",
      "[Epoch: 1, Batch: 19180 / 55793], loss: 0.563\n",
      "[Epoch: 1, Batch: 19190 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 19200 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 19210 / 55793], loss: 0.581\n",
      "[Epoch: 1, Batch: 19220 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 19230 / 55793], loss: 0.547\n",
      "[Epoch: 1, Batch: 19240 / 55793], loss: 0.495\n",
      "[Epoch: 1, Batch: 19250 / 55793], loss: 0.549\n",
      "[Epoch: 1, Batch: 19260 / 55793], loss: 0.578\n",
      "[Epoch: 1, Batch: 19270 / 55793], loss: 0.536\n",
      "[Epoch: 1, Batch: 19280 / 55793], loss: 0.488\n",
      "[Epoch: 1, Batch: 19290 / 55793], loss: 0.634\n",
      "[Epoch: 1, Batch: 19300 / 55793], loss: 0.530\n",
      "[Epoch: 1, Batch: 19310 / 55793], loss: 0.529\n",
      "[Epoch: 1, Batch: 19320 / 55793], loss: 0.682\n",
      "[Epoch: 1, Batch: 19330 / 55793], loss: 0.645\n",
      "[Epoch: 1, Batch: 19340 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 19350 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 19360 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 19370 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch: 19380 / 55793], loss: 0.593\n",
      "[Epoch: 1, Batch: 19390 / 55793], loss: 0.553\n",
      "[Epoch: 1, Batch: 19400 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 19410 / 55793], loss: 0.520\n",
      "[Epoch: 1, Batch: 19420 / 55793], loss: 0.556\n",
      "[Epoch: 1, Batch: 19430 / 55793], loss: 0.668\n",
      "[Epoch: 1, Batch: 19440 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 19450 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 19460 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 19470 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 19480 / 55793], loss: 0.573\n",
      "[Epoch: 1, Batch: 19490 / 55793], loss: 0.612\n",
      "[Epoch: 1, Batch: 19500 / 55793], loss: 0.526\n",
      "[Epoch: 1, Batch: 19510 / 55793], loss: 0.570\n",
      "[Epoch: 1, Batch: 19520 / 55793], loss: 0.501\n",
      "[Epoch: 1, Batch: 19530 / 55793], loss: 0.539\n",
      "[Epoch: 1, Batch: 19540 / 55793], loss: 0.512\n",
      "[Epoch: 1, Batch: 19550 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 19560 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 19570 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 19580 / 55793], loss: 0.560\n",
      "[Epoch: 1, Batch: 19590 / 55793], loss: 0.540\n",
      "[Epoch: 1, Batch: 19600 / 55793], loss: 0.513\n",
      "[Epoch: 1, Batch: 19610 / 55793], loss: 0.463\n",
      "[Epoch: 1, Batch: 19620 / 55793], loss: 0.759\n",
      "[Epoch: 1, Batch: 19630 / 55793], loss: 0.513\n",
      "[Epoch: 1, Batch: 19640 / 55793], loss: 0.635\n",
      "[Epoch: 1, Batch: 19650 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 19660 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 19670 / 55793], loss: 0.484\n",
      "[Epoch: 1, Batch: 19680 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 19690 / 55793], loss: 0.520\n",
      "[Epoch: 1, Batch: 19700 / 55793], loss: 0.563\n",
      "[Epoch: 1, Batch: 19710 / 55793], loss: 0.545\n",
      "[Epoch: 1, Batch: 19720 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 19730 / 55793], loss: 0.549\n",
      "[Epoch: 1, Batch: 19740 / 55793], loss: 0.517\n",
      "[Epoch: 1, Batch: 19750 / 55793], loss: 0.498\n",
      "[Epoch: 1, Batch: 19760 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 19770 / 55793], loss: 0.535\n",
      "[Epoch: 1, Batch: 19780 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 19790 / 55793], loss: 0.522\n",
      "[Epoch: 1, Batch: 19800 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 19810 / 55793], loss: 0.562\n",
      "[Epoch: 1, Batch: 19820 / 55793], loss: 0.524\n",
      "[Epoch: 1, Batch: 19830 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 19840 / 55793], loss: 0.515\n",
      "[Epoch: 1, Batch: 19850 / 55793], loss: 0.548\n",
      "[Epoch: 1, Batch: 19860 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 19870 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 19880 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 19890 / 55793], loss: 0.470\n",
      "[Epoch: 1, Batch: 19900 / 55793], loss: 0.525\n",
      "[Epoch: 1, Batch: 19910 / 55793], loss: 0.532\n",
      "[Epoch: 1, Batch: 19920 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 19930 / 55793], loss: 0.556\n",
      "[Epoch: 1, Batch: 19940 / 55793], loss: 0.642\n",
      "[Epoch: 1, Batch: 19950 / 55793], loss: 0.648\n",
      "[Epoch: 1, Batch: 19960 / 55793], loss: 0.525\n",
      "[Epoch: 1, Batch: 19970 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 19980 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 19990 / 55793], loss: 0.528\n",
      "[Epoch: 1, Batch: 20000 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 20010 / 55793], loss: 0.583\n",
      "[Epoch: 1, Batch: 20020 / 55793], loss: 0.486\n",
      "[Epoch: 1, Batch: 20030 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 20040 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 20050 / 55793], loss: 0.537\n",
      "[Epoch: 1, Batch: 20060 / 55793], loss: 0.545\n",
      "[Epoch: 1, Batch: 20070 / 55793], loss: 0.515\n",
      "[Epoch: 1, Batch: 20080 / 55793], loss: 0.557\n",
      "[Epoch: 1, Batch: 20090 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 20100 / 55793], loss: 0.538\n",
      "[Epoch: 1, Batch: 20110 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 20120 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 20130 / 55793], loss: 0.547\n",
      "[Epoch: 1, Batch: 20140 / 55793], loss: 0.625\n",
      "[Epoch: 1, Batch: 20150 / 55793], loss: 0.538\n",
      "[Epoch: 1, Batch: 20160 / 55793], loss: 0.550\n",
      "[Epoch: 1, Batch: 20170 / 55793], loss: 0.619\n",
      "[Epoch: 1, Batch: 20180 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 20190 / 55793], loss: 0.603\n",
      "[Epoch: 1, Batch: 20200 / 55793], loss: 0.627\n",
      "[Epoch: 1, Batch: 20210 / 55793], loss: 0.537\n",
      "[Epoch: 1, Batch: 20220 / 55793], loss: 0.666\n",
      "[Epoch: 1, Batch: 20230 / 55793], loss: 0.519\n",
      "[Epoch: 1, Batch: 20240 / 55793], loss: 0.512\n",
      "[Epoch: 1, Batch: 20250 / 55793], loss: 0.560\n",
      "[Epoch: 1, Batch: 20260 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 20270 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 20280 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 20290 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 20300 / 55793], loss: 0.618\n",
      "[Epoch: 1, Batch: 20310 / 55793], loss: 0.521\n",
      "[Epoch: 1, Batch: 20320 / 55793], loss: 0.581\n",
      "[Epoch: 1, Batch: 20330 / 55793], loss: 0.547\n",
      "[Epoch: 1, Batch: 20340 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 20350 / 55793], loss: 0.553\n",
      "[Epoch: 1, Batch: 20360 / 55793], loss: 0.466\n",
      "[Epoch: 1, Batch: 20370 / 55793], loss: 0.547\n",
      "[Epoch: 1, Batch: 20380 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 20390 / 55793], loss: 0.611\n",
      "[Epoch: 1, Batch: 20400 / 55793], loss: 0.603\n",
      "[Epoch: 1, Batch: 20410 / 55793], loss: 0.531\n",
      "[Epoch: 1, Batch: 20420 / 55793], loss: 0.548\n",
      "[Epoch: 1, Batch: 20430 / 55793], loss: 0.578\n",
      "[Epoch: 1, Batch: 20440 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 20450 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 20460 / 55793], loss: 0.655\n",
      "[Epoch: 1, Batch: 20470 / 55793], loss: 0.540\n",
      "[Epoch: 1, Batch: 20480 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 20490 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 20500 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 20510 / 55793], loss: 0.534\n",
      "[Epoch: 1, Batch: 20520 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 20530 / 55793], loss: 0.567\n",
      "[Epoch: 1, Batch: 20540 / 55793], loss: 0.477\n",
      "[Epoch: 1, Batch: 20550 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 20560 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 20570 / 55793], loss: 0.581\n",
      "[Epoch: 1, Batch: 20580 / 55793], loss: 0.563\n",
      "[Epoch: 1, Batch: 20590 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 20600 / 55793], loss: 0.663\n",
      "[Epoch: 1, Batch: 20610 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 20620 / 55793], loss: 0.551\n",
      "[Epoch: 1, Batch: 20630 / 55793], loss: 0.468\n",
      "[Epoch: 1, Batch: 20640 / 55793], loss: 0.638\n",
      "[Epoch: 1, Batch: 20650 / 55793], loss: 0.560\n",
      "[Epoch: 1, Batch: 20660 / 55793], loss: 0.550\n",
      "[Epoch: 1, Batch: 20670 / 55793], loss: 0.521\n",
      "[Epoch: 1, Batch: 20680 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 20690 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 20700 / 55793], loss: 0.596\n",
      "[Epoch: 1, Batch: 20710 / 55793], loss: 0.481\n",
      "[Epoch: 1, Batch: 20720 / 55793], loss: 0.483\n",
      "[Epoch: 1, Batch: 20730 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 20740 / 55793], loss: 0.655\n",
      "[Epoch: 1, Batch: 20750 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 20760 / 55793], loss: 0.524\n",
      "[Epoch: 1, Batch: 20770 / 55793], loss: 0.630\n",
      "[Epoch: 1, Batch: 20780 / 55793], loss: 0.572\n",
      "[Epoch: 1, Batch: 20790 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 20800 / 55793], loss: 0.492\n",
      "[Epoch: 1, Batch: 20810 / 55793], loss: 0.494\n",
      "[Epoch: 1, Batch: 20820 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 20830 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 20840 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 20850 / 55793], loss: 0.639\n",
      "[Epoch: 1, Batch: 20860 / 55793], loss: 0.532\n",
      "[Epoch: 1, Batch: 20870 / 55793], loss: 0.567\n",
      "[Epoch: 1, Batch: 20880 / 55793], loss: 0.583\n",
      "[Epoch: 1, Batch: 20890 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 20900 / 55793], loss: 0.528\n",
      "[Epoch: 1, Batch: 20910 / 55793], loss: 0.532\n",
      "[Epoch: 1, Batch: 20920 / 55793], loss: 0.662\n",
      "[Epoch: 1, Batch: 20930 / 55793], loss: 0.578\n",
      "[Epoch: 1, Batch: 20940 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 20950 / 55793], loss: 0.553\n",
      "[Epoch: 1, Batch: 20960 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 20970 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 20980 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 20990 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 21000 / 55793], loss: 0.570\n",
      "[Epoch: 1, Batch: 21010 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 21020 / 55793], loss: 0.593\n",
      "[Epoch: 1, Batch: 21030 / 55793], loss: 0.511\n",
      "[Epoch: 1, Batch: 21040 / 55793], loss: 0.537\n",
      "[Epoch: 1, Batch: 21050 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 21060 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 21070 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 21080 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 21090 / 55793], loss: 0.480\n",
      "[Epoch: 1, Batch: 21100 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 21110 / 55793], loss: 0.635\n",
      "[Epoch: 1, Batch: 21120 / 55793], loss: 0.491\n",
      "[Epoch: 1, Batch: 21130 / 55793], loss: 0.464\n",
      "[Epoch: 1, Batch: 21140 / 55793], loss: 0.521\n",
      "[Epoch: 1, Batch: 21150 / 55793], loss: 0.599\n",
      "[Epoch: 1, Batch: 21160 / 55793], loss: 0.555\n",
      "[Epoch: 1, Batch: 21170 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 21180 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 21190 / 55793], loss: 0.681\n",
      "[Epoch: 1, Batch: 21200 / 55793], loss: 0.583\n",
      "[Epoch: 1, Batch: 21210 / 55793], loss: 0.599\n",
      "[Epoch: 1, Batch: 21220 / 55793], loss: 0.583\n",
      "[Epoch: 1, Batch: 21230 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 21240 / 55793], loss: 0.523\n",
      "[Epoch: 1, Batch: 21250 / 55793], loss: 0.537\n",
      "[Epoch: 1, Batch: 21260 / 55793], loss: 0.508\n",
      "[Epoch: 1, Batch: 21270 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 21280 / 55793], loss: 0.588\n",
      "[Epoch: 1, Batch: 21290 / 55793], loss: 0.651\n",
      "[Epoch: 1, Batch: 21300 / 55793], loss: 0.507\n",
      "[Epoch: 1, Batch: 21310 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 21320 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 21330 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 21340 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 21350 / 55793], loss: 0.535\n",
      "[Epoch: 1, Batch: 21360 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 21370 / 55793], loss: 0.671\n",
      "[Epoch: 1, Batch: 21380 / 55793], loss: 0.627\n",
      "[Epoch: 1, Batch: 21390 / 55793], loss: 0.578\n",
      "[Epoch: 1, Batch: 21400 / 55793], loss: 0.640\n",
      "[Epoch: 1, Batch: 21410 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 21420 / 55793], loss: 0.550\n",
      "[Epoch: 1, Batch: 21430 / 55793], loss: 0.668\n",
      "[Epoch: 1, Batch: 21440 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 21450 / 55793], loss: 0.502\n",
      "[Epoch: 1, Batch: 21460 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 21470 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 21480 / 55793], loss: 0.517\n",
      "[Epoch: 1, Batch: 21490 / 55793], loss: 0.509\n",
      "[Epoch: 1, Batch: 21500 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 21510 / 55793], loss: 0.598\n",
      "[Epoch: 1, Batch: 21520 / 55793], loss: 0.546\n",
      "[Epoch: 1, Batch: 21530 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 21540 / 55793], loss: 0.551\n",
      "[Epoch: 1, Batch: 21550 / 55793], loss: 0.570\n",
      "[Epoch: 1, Batch: 21560 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 21570 / 55793], loss: 0.549\n",
      "[Epoch: 1, Batch: 21580 / 55793], loss: 0.600\n",
      "[Epoch: 1, Batch: 21590 / 55793], loss: 0.546\n",
      "[Epoch: 1, Batch: 21600 / 55793], loss: 0.578\n",
      "[Epoch: 1, Batch: 21610 / 55793], loss: 0.545\n",
      "[Epoch: 1, Batch: 21620 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 21630 / 55793], loss: 0.535\n",
      "[Epoch: 1, Batch: 21640 / 55793], loss: 0.556\n",
      "[Epoch: 1, Batch: 21650 / 55793], loss: 0.669\n",
      "[Epoch: 1, Batch: 21660 / 55793], loss: 0.593\n",
      "[Epoch: 1, Batch: 21670 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 21680 / 55793], loss: 0.581\n",
      "[Epoch: 1, Batch: 21690 / 55793], loss: 0.603\n",
      "[Epoch: 1, Batch: 21700 / 55793], loss: 0.567\n",
      "[Epoch: 1, Batch: 21710 / 55793], loss: 0.598\n",
      "[Epoch: 1, Batch: 21720 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 21730 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 21740 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 21750 / 55793], loss: 0.532\n",
      "[Epoch: 1, Batch: 21760 / 55793], loss: 0.541\n",
      "[Epoch: 1, Batch: 21770 / 55793], loss: 0.479\n",
      "[Epoch: 1, Batch: 21780 / 55793], loss: 0.542\n",
      "[Epoch: 1, Batch: 21790 / 55793], loss: 0.542\n",
      "[Epoch: 1, Batch: 21800 / 55793], loss: 0.517\n",
      "[Epoch: 1, Batch: 21810 / 55793], loss: 0.621\n",
      "[Epoch: 1, Batch: 21820 / 55793], loss: 0.572\n",
      "[Epoch: 1, Batch: 21830 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 21840 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 21850 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 21860 / 55793], loss: 0.555\n",
      "[Epoch: 1, Batch: 21870 / 55793], loss: 0.489\n",
      "[Epoch: 1, Batch: 21880 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 21890 / 55793], loss: 0.482\n",
      "[Epoch: 1, Batch: 21900 / 55793], loss: 0.497\n",
      "[Epoch: 1, Batch: 21910 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 21920 / 55793], loss: 0.545\n",
      "[Epoch: 1, Batch: 21930 / 55793], loss: 0.457\n",
      "[Epoch: 1, Batch: 21940 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 21950 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 21960 / 55793], loss: 0.617\n",
      "[Epoch: 1, Batch: 21970 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 21980 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 21990 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 22000 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 22010 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 22020 / 55793], loss: 0.567\n",
      "[Epoch: 1, Batch: 22030 / 55793], loss: 0.564\n",
      "[Epoch: 1, Batch: 22040 / 55793], loss: 0.470\n",
      "[Epoch: 1, Batch: 22050 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 22060 / 55793], loss: 0.651\n",
      "[Epoch: 1, Batch: 22070 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 22080 / 55793], loss: 0.518\n",
      "[Epoch: 1, Batch: 22090 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 22100 / 55793], loss: 0.521\n",
      "[Epoch: 1, Batch: 22110 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 22120 / 55793], loss: 0.525\n",
      "[Epoch: 1, Batch: 22130 / 55793], loss: 0.504\n",
      "[Epoch: 1, Batch: 22140 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 22150 / 55793], loss: 0.555\n",
      "[Epoch: 1, Batch: 22160 / 55793], loss: 0.545\n",
      "[Epoch: 1, Batch: 22170 / 55793], loss: 0.472\n",
      "[Epoch: 1, Batch: 22180 / 55793], loss: 0.540\n",
      "[Epoch: 1, Batch: 22190 / 55793], loss: 0.444\n",
      "[Epoch: 1, Batch: 22200 / 55793], loss: 0.461\n",
      "[Epoch: 1, Batch: 22210 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 22220 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 22230 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 22240 / 55793], loss: 0.535\n",
      "[Epoch: 1, Batch: 22250 / 55793], loss: 0.527\n",
      "[Epoch: 1, Batch: 22260 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch: 22270 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 22280 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 22290 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 22300 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 22310 / 55793], loss: 0.550\n",
      "[Epoch: 1, Batch: 22320 / 55793], loss: 0.536\n",
      "[Epoch: 1, Batch: 22330 / 55793], loss: 0.466\n",
      "[Epoch: 1, Batch: 22340 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 22350 / 55793], loss: 0.572\n",
      "[Epoch: 1, Batch: 22360 / 55793], loss: 0.591\n",
      "[Epoch: 1, Batch: 22370 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 22380 / 55793], loss: 0.618\n",
      "[Epoch: 1, Batch: 22390 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 22400 / 55793], loss: 0.645\n",
      "[Epoch: 1, Batch: 22410 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 22420 / 55793], loss: 0.603\n",
      "[Epoch: 1, Batch: 22430 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 22440 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 22450 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 22460 / 55793], loss: 0.502\n",
      "[Epoch: 1, Batch: 22470 / 55793], loss: 0.565\n",
      "[Epoch: 1, Batch: 22480 / 55793], loss: 0.518\n",
      "[Epoch: 1, Batch: 22490 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 22500 / 55793], loss: 0.522\n",
      "[Epoch: 1, Batch: 22510 / 55793], loss: 0.514\n",
      "[Epoch: 1, Batch: 22520 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 22530 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 22540 / 55793], loss: 0.542\n",
      "[Epoch: 1, Batch: 22550 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 22560 / 55793], loss: 0.500\n",
      "[Epoch: 1, Batch: 22570 / 55793], loss: 0.519\n",
      "[Epoch: 1, Batch: 22580 / 55793], loss: 0.567\n",
      "[Epoch: 1, Batch: 22590 / 55793], loss: 0.644\n",
      "[Epoch: 1, Batch: 22600 / 55793], loss: 0.514\n",
      "[Epoch: 1, Batch: 22610 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 22620 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 22630 / 55793], loss: 0.471\n",
      "[Epoch: 1, Batch: 22640 / 55793], loss: 0.570\n",
      "[Epoch: 1, Batch: 22650 / 55793], loss: 0.547\n",
      "[Epoch: 1, Batch: 22660 / 55793], loss: 0.467\n",
      "[Epoch: 1, Batch: 22670 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 22680 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 22690 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 22700 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 22710 / 55793], loss: 0.546\n",
      "[Epoch: 1, Batch: 22720 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 22730 / 55793], loss: 0.524\n",
      "[Epoch: 1, Batch: 22740 / 55793], loss: 0.573\n",
      "[Epoch: 1, Batch: 22750 / 55793], loss: 0.563\n",
      "[Epoch: 1, Batch: 22760 / 55793], loss: 0.546\n",
      "[Epoch: 1, Batch: 22770 / 55793], loss: 0.508\n",
      "[Epoch: 1, Batch: 22780 / 55793], loss: 0.565\n",
      "[Epoch: 1, Batch: 22790 / 55793], loss: 0.521\n",
      "[Epoch: 1, Batch: 22800 / 55793], loss: 0.553\n",
      "[Epoch: 1, Batch: 22810 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 22820 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 22830 / 55793], loss: 0.515\n",
      "[Epoch: 1, Batch: 22840 / 55793], loss: 0.638\n",
      "[Epoch: 1, Batch: 22850 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 22860 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 22870 / 55793], loss: 0.618\n",
      "[Epoch: 1, Batch: 22880 / 55793], loss: 0.548\n",
      "[Epoch: 1, Batch: 22890 / 55793], loss: 0.541\n",
      "[Epoch: 1, Batch: 22900 / 55793], loss: 0.581\n",
      "[Epoch: 1, Batch: 22910 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 22920 / 55793], loss: 0.510\n",
      "[Epoch: 1, Batch: 22930 / 55793], loss: 0.578\n",
      "[Epoch: 1, Batch: 22940 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 22950 / 55793], loss: 0.542\n",
      "[Epoch: 1, Batch: 22960 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 22970 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 22980 / 55793], loss: 0.636\n",
      "[Epoch: 1, Batch: 22990 / 55793], loss: 0.649\n",
      "[Epoch: 1, Batch: 23000 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 23010 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 23020 / 55793], loss: 0.595\n",
      "[Epoch: 1, Batch: 23030 / 55793], loss: 0.540\n",
      "[Epoch: 1, Batch: 23040 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 23050 / 55793], loss: 0.581\n",
      "[Epoch: 1, Batch: 23060 / 55793], loss: 0.572\n",
      "[Epoch: 1, Batch: 23070 / 55793], loss: 0.561\n",
      "[Epoch: 1, Batch: 23080 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 23090 / 55793], loss: 0.676\n",
      "[Epoch: 1, Batch: 23100 / 55793], loss: 0.491\n",
      "[Epoch: 1, Batch: 23110 / 55793], loss: 0.509\n",
      "[Epoch: 1, Batch: 23120 / 55793], loss: 0.519\n",
      "[Epoch: 1, Batch: 23130 / 55793], loss: 0.553\n",
      "[Epoch: 1, Batch: 23140 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 23150 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 23160 / 55793], loss: 0.562\n",
      "[Epoch: 1, Batch: 23170 / 55793], loss: 0.512\n",
      "[Epoch: 1, Batch: 23180 / 55793], loss: 0.563\n",
      "[Epoch: 1, Batch: 23190 / 55793], loss: 0.506\n",
      "[Epoch: 1, Batch: 23200 / 55793], loss: 0.513\n",
      "[Epoch: 1, Batch: 23210 / 55793], loss: 0.556\n",
      "[Epoch: 1, Batch: 23220 / 55793], loss: 0.503\n",
      "[Epoch: 1, Batch: 23230 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 23240 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 23250 / 55793], loss: 0.625\n",
      "[Epoch: 1, Batch: 23260 / 55793], loss: 0.645\n",
      "[Epoch: 1, Batch: 23270 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 23280 / 55793], loss: 0.526\n",
      "[Epoch: 1, Batch: 23290 / 55793], loss: 0.535\n",
      "[Epoch: 1, Batch: 23300 / 55793], loss: 0.541\n",
      "[Epoch: 1, Batch: 23310 / 55793], loss: 0.620\n",
      "[Epoch: 1, Batch: 23320 / 55793], loss: 0.504\n",
      "[Epoch: 1, Batch: 23330 / 55793], loss: 0.592\n",
      "[Epoch: 1, Batch: 23340 / 55793], loss: 0.550\n",
      "[Epoch: 1, Batch: 23350 / 55793], loss: 0.554\n",
      "[Epoch: 1, Batch: 23360 / 55793], loss: 0.464\n",
      "[Epoch: 1, Batch: 23370 / 55793], loss: 0.551\n",
      "[Epoch: 1, Batch: 23380 / 55793], loss: 0.604\n",
      "[Epoch: 1, Batch: 23390 / 55793], loss: 0.497\n",
      "[Epoch: 1, Batch: 23400 / 55793], loss: 0.621\n",
      "[Epoch: 1, Batch: 23410 / 55793], loss: 0.527\n",
      "[Epoch: 1, Batch: 23420 / 55793], loss: 0.480\n",
      "[Epoch: 1, Batch: 23430 / 55793], loss: 0.496\n",
      "[Epoch: 1, Batch: 23440 / 55793], loss: 0.630\n",
      "[Epoch: 1, Batch: 23450 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 23460 / 55793], loss: 0.518\n",
      "[Epoch: 1, Batch: 23470 / 55793], loss: 0.524\n",
      "[Epoch: 1, Batch: 23480 / 55793], loss: 0.546\n",
      "[Epoch: 1, Batch: 23490 / 55793], loss: 0.578\n",
      "[Epoch: 1, Batch: 23500 / 55793], loss: 0.532\n",
      "[Epoch: 1, Batch: 23510 / 55793], loss: 0.616\n",
      "[Epoch: 1, Batch: 23520 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 23530 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 23540 / 55793], loss: 0.560\n",
      "[Epoch: 1, Batch: 23550 / 55793], loss: 0.492\n",
      "[Epoch: 1, Batch: 23560 / 55793], loss: 0.631\n",
      "[Epoch: 1, Batch: 23570 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 23580 / 55793], loss: 0.601\n",
      "[Epoch: 1, Batch: 23590 / 55793], loss: 0.507\n",
      "[Epoch: 1, Batch: 23600 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 23610 / 55793], loss: 0.529\n",
      "[Epoch: 1, Batch: 23620 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 23630 / 55793], loss: 0.591\n",
      "[Epoch: 1, Batch: 23640 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 23650 / 55793], loss: 0.646\n",
      "[Epoch: 1, Batch: 23660 / 55793], loss: 0.526\n",
      "[Epoch: 1, Batch: 23670 / 55793], loss: 0.457\n",
      "[Epoch: 1, Batch: 23680 / 55793], loss: 0.613\n",
      "[Epoch: 1, Batch: 23690 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 23700 / 55793], loss: 0.546\n",
      "[Epoch: 1, Batch: 23710 / 55793], loss: 0.485\n",
      "[Epoch: 1, Batch: 23720 / 55793], loss: 0.507\n",
      "[Epoch: 1, Batch: 23730 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 23740 / 55793], loss: 0.482\n",
      "[Epoch: 1, Batch: 23750 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 23760 / 55793], loss: 0.563\n",
      "[Epoch: 1, Batch: 23770 / 55793], loss: 0.607\n",
      "[Epoch: 1, Batch: 23780 / 55793], loss: 0.534\n",
      "[Epoch: 1, Batch: 23790 / 55793], loss: 0.585\n",
      "[Epoch: 1, Batch: 23800 / 55793], loss: 0.632\n",
      "[Epoch: 1, Batch: 23810 / 55793], loss: 0.563\n",
      "[Epoch: 1, Batch: 23820 / 55793], loss: 0.529\n",
      "[Epoch: 1, Batch: 23830 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 23840 / 55793], loss: 0.677\n",
      "[Epoch: 1, Batch: 23850 / 55793], loss: 0.622\n",
      "[Epoch: 1, Batch: 23860 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 23870 / 55793], loss: 0.536\n",
      "[Epoch: 1, Batch: 23880 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 23890 / 55793], loss: 0.627\n",
      "[Epoch: 1, Batch: 23900 / 55793], loss: 0.543\n",
      "[Epoch: 1, Batch: 23910 / 55793], loss: 0.635\n",
      "[Epoch: 1, Batch: 23920 / 55793], loss: 0.509\n",
      "[Epoch: 1, Batch: 23930 / 55793], loss: 0.598\n",
      "[Epoch: 1, Batch: 23940 / 55793], loss: 0.550\n",
      "[Epoch: 1, Batch: 23950 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 23960 / 55793], loss: 0.563\n",
      "[Epoch: 1, Batch: 23970 / 55793], loss: 0.594\n",
      "[Epoch: 1, Batch: 23980 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 23990 / 55793], loss: 0.615\n",
      "[Epoch: 1, Batch: 24000 / 55793], loss: 0.536\n",
      "[Epoch: 1, Batch: 24010 / 55793], loss: 0.584\n",
      "[Epoch: 1, Batch: 24020 / 55793], loss: 0.698\n",
      "[Epoch: 1, Batch: 24030 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 24040 / 55793], loss: 0.591\n",
      "[Epoch: 1, Batch: 24050 / 55793], loss: 0.617\n",
      "[Epoch: 1, Batch: 24060 / 55793], loss: 0.624\n",
      "[Epoch: 1, Batch: 24070 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 24080 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 24090 / 55793], loss: 0.551\n",
      "[Epoch: 1, Batch: 24100 / 55793], loss: 0.536\n",
      "[Epoch: 1, Batch: 24110 / 55793], loss: 0.551\n",
      "[Epoch: 1, Batch: 24120 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 24130 / 55793], loss: 0.613\n",
      "[Epoch: 1, Batch: 24140 / 55793], loss: 0.539\n",
      "[Epoch: 1, Batch: 24150 / 55793], loss: 0.530\n",
      "[Epoch: 1, Batch: 24160 / 55793], loss: 0.569\n",
      "[Epoch: 1, Batch: 24170 / 55793], loss: 0.557\n",
      "[Epoch: 1, Batch: 24180 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 24190 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 24200 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 24210 / 55793], loss: 0.544\n",
      "[Epoch: 1, Batch: 24220 / 55793], loss: 0.537\n",
      "[Epoch: 1, Batch: 24230 / 55793], loss: 0.600\n",
      "[Epoch: 1, Batch: 24240 / 55793], loss: 0.598\n",
      "[Epoch: 1, Batch: 24250 / 55793], loss: 0.639\n",
      "[Epoch: 1, Batch: 24260 / 55793], loss: 0.641\n",
      "[Epoch: 1, Batch: 24270 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 24280 / 55793], loss: 0.535\n",
      "[Epoch: 1, Batch: 24290 / 55793], loss: 0.609\n",
      "[Epoch: 1, Batch: 24300 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 24310 / 55793], loss: 0.528\n",
      "[Epoch: 1, Batch: 24320 / 55793], loss: 0.541\n",
      "[Epoch: 1, Batch: 24330 / 55793], loss: 0.568\n",
      "[Epoch: 1, Batch: 24340 / 55793], loss: 0.674\n",
      "[Epoch: 1, Batch: 24350 / 55793], loss: 0.566\n",
      "[Epoch: 1, Batch: 24360 / 55793], loss: 0.534\n",
      "[Epoch: 1, Batch: 24370 / 55793], loss: 0.573\n",
      "[Epoch: 1, Batch: 24380 / 55793], loss: 0.587\n",
      "[Epoch: 1, Batch: 24390 / 55793], loss: 0.548\n",
      "[Epoch: 1, Batch: 24400 / 55793], loss: 0.538\n",
      "[Epoch: 1, Batch: 24410 / 55793], loss: 0.557\n",
      "[Epoch: 1, Batch: 24420 / 55793], loss: 0.526\n",
      "[Epoch: 1, Batch: 24430 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 24440 / 55793], loss: 0.597\n",
      "[Epoch: 1, Batch: 24450 / 55793], loss: 0.634\n",
      "[Epoch: 1, Batch: 24460 / 55793], loss: 0.497\n",
      "[Epoch: 1, Batch: 24470 / 55793], loss: 0.548\n",
      "[Epoch: 1, Batch: 24480 / 55793], loss: 0.590\n",
      "[Epoch: 1, Batch: 24490 / 55793], loss: 0.644\n",
      "[Epoch: 1, Batch: 24500 / 55793], loss: 0.602\n",
      "[Epoch: 1, Batch: 24510 / 55793], loss: 0.565\n",
      "[Epoch: 1, Batch: 24520 / 55793], loss: 0.643\n",
      "[Epoch: 1, Batch: 24530 / 55793], loss: 0.575\n",
      "[Epoch: 1, Batch: 24540 / 55793], loss: 0.556\n",
      "[Epoch: 1, Batch: 24550 / 55793], loss: 0.573\n",
      "[Epoch: 1, Batch: 24560 / 55793], loss: 0.615\n",
      "[Epoch: 1, Batch: 24570 / 55793], loss: 0.490\n",
      "[Epoch: 1, Batch: 24580 / 55793], loss: 0.560\n",
      "[Epoch: 1, Batch: 24590 / 55793], loss: 0.586\n",
      "[Epoch: 1, Batch: 24600 / 55793], loss: 0.576\n",
      "[Epoch: 1, Batch: 24610 / 55793], loss: 0.541\n",
      "[Epoch: 1, Batch: 24620 / 55793], loss: 0.540\n",
      "[Epoch: 1, Batch: 24630 / 55793], loss: 0.608\n",
      "[Epoch: 1, Batch: 24640 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 24650 / 55793], loss: 0.483\n",
      "[Epoch: 1, Batch: 24660 / 55793], loss: 0.647\n",
      "[Epoch: 1, Batch: 24670 / 55793], loss: 0.552\n",
      "[Epoch: 1, Batch: 24680 / 55793], loss: 0.612\n",
      "[Epoch: 1, Batch: 24690 / 55793], loss: 0.582\n",
      "[Epoch: 1, Batch: 24700 / 55793], loss: 0.558\n",
      "[Epoch: 1, Batch: 24710 / 55793], loss: 0.631\n",
      "[Epoch: 1, Batch: 24720 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 24730 / 55793], loss: 0.530\n",
      "[Epoch: 1, Batch: 24740 / 55793], loss: 0.579\n",
      "[Epoch: 1, Batch: 24750 / 55793], loss: 0.682\n",
      "[Epoch: 1, Batch: 24760 / 55793], loss: 0.589\n",
      "[Epoch: 1, Batch: 24770 / 55793], loss: 0.614\n",
      "[Epoch: 1, Batch: 24780 / 55793], loss: 0.574\n",
      "[Epoch: 1, Batch: 24790 / 55793], loss: 0.577\n",
      "[Epoch: 1, Batch: 24800 / 55793], loss: 0.571\n",
      "[Epoch: 1, Batch: 24810 / 55793], loss: 0.580\n",
      "[Epoch: 1, Batch: 24820 / 55793], loss: 0.593\n",
      "[Epoch: 1, Batch: 24830 / 55793], loss: 0.527\n",
      "[Epoch: 1, Batch: 24840 / 55793], loss: 0.610\n",
      "[Epoch: 1, Batch: 24850 / 55793], loss: 0.559\n",
      "[Epoch: 1, Batch: 24860 / 55793], loss: 0.540\n",
      "[Epoch: 1, Batch: 24870 / 55793], loss: 0.549\n",
      "[Epoch: 1, Batch: 24880 / 55793], loss: 0.549\n",
      "[Epoch: 1, Batch: 24890 / 55793], loss: 0.533\n",
      "[Epoch: 1, Batch: 24900 / 55793], loss: 0.546\n",
      "[Epoch: 1, Batch: 24910 / 55793], loss: 0.588\n"
     ]
    }
   ],
   "source": [
    "train(pointnet, criterion, train_loader, device, val_loader, 1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_8744\\2966757700.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('../model_weights/save_epoch_0.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Recreate the model architecture\n",
    "model = PointNet(classes=2)  # Ensure the class arguments are the same as during training\n",
    "\n",
    "# Load the saved state_dict into the model\n",
    "model.load_state_dict(torch.load('../model_weights/save_epoch_0.pth'))\n",
    "\n",
    "# Move the model to the appropriate device and set it to evaluation mode\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [  10 /  698]\n",
      "Batch [  20 /  698]\n",
      "Batch [  30 /  698]\n",
      "Batch [  40 /  698]\n",
      "Batch [  50 /  698]\n",
      "Batch [  60 /  698]\n",
      "Batch [  70 /  698]\n",
      "Batch [  80 /  698]\n",
      "Batch [  90 /  698]\n",
      "Batch [ 100 /  698]\n",
      "Batch [ 110 /  698]\n",
      "Batch [ 120 /  698]\n",
      "Batch [ 130 /  698]\n",
      "Batch [ 140 /  698]\n",
      "Batch [ 150 /  698]\n",
      "Batch [ 160 /  698]\n",
      "Batch [ 170 /  698]\n",
      "Batch [ 180 /  698]\n",
      "Batch [ 190 /  698]\n",
      "Batch [ 200 /  698]\n",
      "Batch [ 210 /  698]\n",
      "Batch [ 220 /  698]\n",
      "Batch [ 230 /  698]\n",
      "Batch [ 240 /  698]\n",
      "Batch [ 250 /  698]\n",
      "Batch [ 260 /  698]\n",
      "Batch [ 270 /  698]\n",
      "Batch [ 280 /  698]\n",
      "Batch [ 290 /  698]\n",
      "Batch [ 300 /  698]\n",
      "Batch [ 310 /  698]\n",
      "Batch [ 320 /  698]\n",
      "Batch [ 330 /  698]\n",
      "Batch [ 340 /  698]\n",
      "Batch [ 350 /  698]\n",
      "Batch [ 360 /  698]\n",
      "Batch [ 370 /  698]\n",
      "Batch [ 380 /  698]\n",
      "Batch [ 390 /  698]\n",
      "Batch [ 400 /  698]\n",
      "Batch [ 410 /  698]\n",
      "Batch [ 420 /  698]\n",
      "Batch [ 430 /  698]\n",
      "Batch [ 440 /  698]\n",
      "Batch [ 450 /  698]\n",
      "Batch [ 460 /  698]\n",
      "Batch [ 470 /  698]\n",
      "Batch [ 480 /  698]\n",
      "Batch [ 490 /  698]\n",
      "Batch [ 500 /  698]\n",
      "Batch [ 510 /  698]\n",
      "Batch [ 520 /  698]\n",
      "Batch [ 530 /  698]\n",
      "Batch [ 540 /  698]\n",
      "Batch [ 550 /  698]\n",
      "Batch [ 560 /  698]\n",
      "Batch [ 570 /  698]\n",
      "Batch [ 580 /  698]\n",
      "Batch [ 590 /  698]\n",
      "Batch [ 600 /  698]\n",
      "Batch [ 610 /  698]\n",
      "Batch [ 620 /  698]\n",
      "Batch [ 630 /  698]\n",
      "Batch [ 640 /  698]\n",
      "Batch [ 650 /  698]\n",
      "Batch [ 660 /  698]\n",
      "Batch [ 670 /  698]\n",
      "Batch [ 680 /  698]\n",
      "Batch [ 690 /  698]\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    if device.type == \"cuda\":\n",
    "        for i, data in enumerate(test_loader):\n",
    "            if i % 10 == 9:\n",
    "                print('Batch [%4d / %4d]' % (i+1, len(test_loader)))\n",
    "            inputs = data['pointcloud'].float().to(device)\n",
    "            labels = data['category'].to(device)\n",
    "            outputs, __, __ = pointnet(inputs.transpose(1,2))\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            all_preds += list(preds.cpu().numpy())  # Move the tensor to CPU and convert to numpy\n",
    "            all_labels += list(labels.cpu().numpy())  # Similarly, move labels to CPU\n",
    "    else:\n",
    "        for i, data in enumerate(test_loader):\n",
    "            if i % 10 == 9:\n",
    "                print('Batch [%4d / %4d]' % (i+1, len(test_loader)))\n",
    "            inputs, labels = data['pointcloud'].float(), data['category']\n",
    "            outputs, __, __ = pointnet(inputs.transpose(1,2))\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            all_preds += list(preds.numpy())\n",
    "            all_labels += list(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from https://deeplizard.com/learn/video/0LhiS6yu2qQ\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[78147,  8827],\n",
       "       [ 2325,     0]], dtype=int64)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds);\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: \"Not Hillfort\",\n",
    "    1: \"Hillfort\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(cm, list(classes.keys()), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(cm, list(classes.keys()), normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpulocal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
