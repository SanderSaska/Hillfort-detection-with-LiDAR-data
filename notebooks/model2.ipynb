{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hillfort detection with LiDAR data\n",
    "## Data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "[Code](#code)\n",
    "\n",
    "1. [**Initializing and training the model**](#initializing-and-training-the-model)\n",
    "2. [**Evaluating the model**](#evaluating-the-model)\n",
    "3. [**Hyperparameter tuning**](#hyperparameter-tuning)\n",
    "4. [**Results**](#results)\n",
    "\n",
    "[End](#end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (2.0.3)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (1.23.5)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (3.7.2)\n",
      "Collecting laspy\n",
      "  Downloading laspy-2.5.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting geopandas\n",
      "  Downloading geopandas-0.13.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from matplotlib) (6.4.0)\n",
      "Collecting fiona>=1.8.19 (from geopandas)\n",
      "  Downloading fiona-1.10.1-cp38-cp38-macosx_10_15_x86_64.whl.metadata (56 kB)\n",
      "Collecting pyproj>=3.0.1 (from geopandas)\n",
      "  Downloading pyproj-3.5.0-cp38-cp38-macosx_10_9_x86_64.whl.metadata (28 kB)\n",
      "Collecting shapely>=1.7.1 (from geopandas)\n",
      "  Downloading shapely-2.0.6-cp38-cp38-macosx_10_9_x86_64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from fiona>=1.8.19->geopandas) (23.1.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from fiona>=1.8.19->geopandas) (2024.8.30)\n",
      "Collecting click~=8.0 (from fiona>=1.8.19->geopandas)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting click-plugins>=1.0 (from fiona>=1.8.19->geopandas)\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting cligj>=0.5 (from fiona>=1.8.19->geopandas)\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from fiona>=1.8.19->geopandas) (7.0.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/atm_2024/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading scikit_learn-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading laspy-2.5.4-py3-none-any.whl (84 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading geopandas-0.13.2-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fiona-1.10.1-cp38-cp38-macosx_10_15_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading pyproj-3.5.0-cp38-cp38-macosx_10_9_x86_64.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading shapely-2.0.6-cp38-cp38-macosx_10_9_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Installing collected packages: tqdm, threadpoolctl, shapely, pyproj, laspy, joblib, click, scikit-learn, cligj, click-plugins, fiona, geopandas\n",
      "Successfully installed click-8.1.7 click-plugins-1.1.1 cligj-0.7.2 fiona-1.10.1 geopandas-0.13.2 joblib-1.4.2 laspy-2.5.4 pyproj-3.5.0 scikit-learn-1.3.2 shapely-2.0.6 threadpoolctl-3.5.0 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn numpy matplotlib laspy tqdm geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have GPU\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "# Otherwise\n",
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_search(query_pts, support_pts, k):\n",
    "    \"\"\"\n",
    "    Naive CPU-based k-NN search returning indices of nearest neighbors.\n",
    "    For large-scale data, replace with GPU ops or specialized libs (Faiss, Open3D, etc.).\n",
    "    \n",
    "    Args:\n",
    "        query_pts: (B, N, 3)\n",
    "        support_pts: (B, M, 3)\n",
    "        k: int, number of neighbors\n",
    "    Returns:\n",
    "        neighbor_idx: (B, N, k) - indices in support_pts of the k nearest neighbors for each point\n",
    "    \"\"\"\n",
    "    # This naive approach does pairwise distance on CPU. Not efficient for large N, M.\n",
    "    # For demonstration only.\n",
    "    B, N, _ = query_pts.shape\n",
    "    _, M, _ = support_pts.shape\n",
    "    \n",
    "    # Expand dims to compute pairwise distances\n",
    "    query_expand = query_pts.unsqueeze(2)  # (B, N, 1, 3)\n",
    "    support_expand = support_pts.unsqueeze(1)  # (B, 1, M, 3)\n",
    "    dist = torch.sum((query_expand - support_expand) ** 2, dim=-1)  # (B, N, M)\n",
    "    \n",
    "    # topk with largest negative distances = smallest distances\n",
    "    _, neighbor_idx = torch.topk(dist, k, dim=-1, largest=False)  # (B, N, k)\n",
    "    return neighbor_idx\n",
    "\n",
    "def gather_neighborhood_features(features, neighbor_idx):\n",
    "    \"\"\"\n",
    "    Gather neighbor features given neighbor indices.\n",
    "    Args:\n",
    "        features: (B, M, C) - feature vectors for M points\n",
    "        neighbor_idx: (B, N, K) - neighbor indices in [0..M-1]\n",
    "    Returns:\n",
    "        neighbor_features: (B, N, K, C)\n",
    "    \"\"\"\n",
    "    B, M, C = features.shape\n",
    "    B, N, K = neighbor_idx.shape\n",
    "    \n",
    "    # Use torch.gather to index the features. We first reshape for convenience.\n",
    "    # neighbor_idx shape -> (B, N, K)\n",
    "    # We'll gather along dimension=1 (the second dimension of features).\n",
    "    # But we need neighbor_idx to broadcast across the channel dimension.\n",
    "    neighbor_idx_expand = neighbor_idx.unsqueeze(-1).expand(-1, -1, -1, C)  # (B, N, K, C)\n",
    "    \n",
    "    # Expand features to (B, M, 1, C) so we can gather\n",
    "    features_expand = features.unsqueeze(1).expand(-1, N, -1, -1)  # (B, N, M, C)\n",
    "    \n",
    "    neighbor_features = torch.gather(features_expand, 2, neighbor_idx_expand)  # (B, N, K, C)\n",
    "    return neighbor_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandLA-Net Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"A simple multi-layer perceptron block with [Conv1d -> BN -> ReLU], repeated.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels_list):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        c_prev = in_channels\n",
    "        for c_out in out_channels_list:\n",
    "            layers.append(nn.Conv1d(c_prev, c_out, kernel_size=1, bias=False))\n",
    "            layers.append(nn.BatchNorm1d(c_out))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            c_prev = c_out\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (B, in_channels, N)\n",
    "        return self.mlp(x)\n",
    "\n",
    "class LocalFeatureAggregation(nn.Module):\n",
    "    \"\"\"\n",
    "    The key RandLA-Net module. \n",
    "    1. Neighborhood search\n",
    "    2. MLP to encode local geometry\n",
    "    3. Attentive Pooling\n",
    "    4. Another MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: number of input channels\n",
    "            out_channels: number of output channels (e.g. 64, 128, etc.)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mlp1 = MLP(in_channels + 3, [out_channels//2, out_channels])  # incorporate relative positions\n",
    "        self.attention_mlp = MLP(out_channels, [out_channels//4, out_channels//4, out_channels//4, 1])  # to get attention score\n",
    "        self.mlp2 = MLP(out_channels, [out_channels, out_channels])\n",
    "        \n",
    "    def forward(self, pts, features, neighbor_idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pts: (B, N, 3) coordinates of the \"query\" set\n",
    "            features: (B, N, C) features for each point\n",
    "            neighbor_idx: (B, N, K) the neighbor indices\n",
    "        Returns:\n",
    "            updated_features: (B, N, out_channels)\n",
    "        \"\"\"\n",
    "        B, N, C_in = features.shape\n",
    "        K = neighbor_idx.shape[-1]\n",
    "        \n",
    "        # Gather neighbor points and features\n",
    "        neighbor_xyz = gather_neighborhood_features(pts, neighbor_idx)  # (B, N, K, 3)\n",
    "        neighbor_feats = gather_neighborhood_features(features, neighbor_idx)  # (B, N, K, C_in)\n",
    "        \n",
    "        # Compute relative positions: [neighbor_xyz - center_xyz]\n",
    "        center_xyz_expand = pts.unsqueeze(2).expand(-1, -1, K, -1)  # (B, N, K, 3)\n",
    "        relative_xyz = neighbor_xyz - center_xyz_expand  # (B, N, K, 3)\n",
    "        \n",
    "        # Concatenate neighbor_feats with relative_xyz\n",
    "        concat_feats = torch.cat([neighbor_feats, relative_xyz], dim=-1)  # (B, N, K, C_in + 3)\n",
    "\n",
    "        # Reshape for MLP1: (B, (C_in+3), N*K)\n",
    "        concat_feats = concat_feats.permute(0,1,3,2).contiguous()  # -> (B, N, C_in+3, K)\n",
    "        concat_feats = concat_feats.view(B, N, -1)                  # Flatten last two dims\n",
    "        concat_feats = concat_feats.permute(0,2,1)                  # -> (B, C_in+3, N*K)? Actually let's do this differently:\n",
    "\n",
    "        # Actually, let's do an MLP over the neighbor dimension, so let's keep shape as:\n",
    "        #    (B, C_in+3, N*K). But that means we need to reshape carefully.\n",
    "        concat_feats = concat_feats.view(B, -1, N*K)  # (B, C_in+3, N*K)\n",
    "\n",
    "        # MLP1\n",
    "        local_feats = self.mlp1(concat_feats)  # (B, out_channels, N*K)\n",
    "        \n",
    "        # Reshape back to group by neighbors\n",
    "        local_feats = local_feats.view(B, -1, N, K)  # (B, out_channels, N, K)\n",
    "\n",
    "        # Attentive pooling\n",
    "        # attention scores = MLP(local_feats) -> shape (B, 1, N, K)\n",
    "        att_scores = self.attention_mlp(local_feats)  # (B, 1, N, K)\n",
    "        att_scores = F.softmax(att_scores, dim=-1)    # softmax across K neighbors\n",
    "\n",
    "        # Weighted sum\n",
    "        aggregated_feats = torch.sum(local_feats * att_scores, dim=-1)  # (B, out_channels, N)\n",
    "\n",
    "        # MLP2\n",
    "        updated_feats = self.mlp2(aggregated_feats)  # (B, out_channels, N)\n",
    "        updated_feats = updated_feats.permute(0,2,1) # (B, N, out_channels)\n",
    "\n",
    "        return updated_feats\n",
    "\n",
    "\n",
    "class LocalSamplingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Random sampling layer. \n",
    "    For RandLA-Net, you typically downsample the points by a factor (e.g., 4).\n",
    "    \"\"\"\n",
    "    def __init__(self, ratio=0.25):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def forward(self, pts, features):\n",
    "        \"\"\"\n",
    "        Randomly sample a subset of points (and corresponding features).\n",
    "        Args:\n",
    "            pts: (B, N, 3)\n",
    "            features: (B, N, C)\n",
    "        Returns:\n",
    "            sub_pts: (B, N_sub, 3)\n",
    "            sub_features: (B, N_sub, C)\n",
    "        \"\"\"\n",
    "        B, N, _ = pts.shape\n",
    "        N_sub = int(N * self.ratio)\n",
    "\n",
    "        # Random sample indices\n",
    "        idx = torch.randperm(N)[:N_sub].to(pts.device)  # shape: (N_sub,)\n",
    "        # But to keep it consistent across the batch, we do this differently for each batch sample \n",
    "        # (though for demonstration, this might suffice for B=1).\n",
    "        \n",
    "        # More correct approach is to do random sampling per batch:\n",
    "        sub_pts_list = []\n",
    "        sub_features_list = []\n",
    "        for b in range(B):\n",
    "            perm = torch.randperm(N, device=pts.device)\n",
    "            chosen_idx = perm[:N_sub]\n",
    "            sub_pts_list.append(pts[b, chosen_idx, :].unsqueeze(0))\n",
    "            sub_features_list.append(features[b, chosen_idx, :].unsqueeze(0))\n",
    "        sub_pts = torch.cat(sub_pts_list, dim=0)\n",
    "        sub_features = torch.cat(sub_features_list, dim=0)\n",
    "        \n",
    "        return sub_pts, sub_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandLANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RandLANet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified RandLA-Net architecture:\n",
    "      - 4 hierarchical stages of downsampling + LocalFeatureAggregation\n",
    "      - Then upsampling with skip connections\n",
    "      - Output segmentation head\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, num_neighbors=16):\n",
    "        super().__init__()\n",
    "        self.num_neighbors = num_neighbors\n",
    "\n",
    "        # Downsample ratios\n",
    "        self.samplings = nn.ModuleList([\n",
    "            LocalSamplingLayer(ratio=0.25),  # DS1: 1/4\n",
    "            LocalSamplingLayer(ratio=0.25),  # DS2: 1/4\n",
    "            LocalSamplingLayer(ratio=0.25),  # DS3: 1/4\n",
    "            LocalSamplingLayer(ratio=0.25),  # DS4: 1/4\n",
    "        ])\n",
    "        \n",
    "        # Each stage: LocalFeatureAggregation\n",
    "        # The paper uses channels progression: 16->64->128->256->512, etc. \n",
    "        self.encoders = nn.ModuleList([\n",
    "            LocalFeatureAggregation(in_channels=3,   out_channels=32),   # Stage1\n",
    "            LocalFeatureAggregation(in_channels=32,  out_channels=64),   # Stage2\n",
    "            LocalFeatureAggregation(in_channels=64,  out_channels=128),  # Stage3\n",
    "            LocalFeatureAggregation(in_channels=128, out_channels=256),  # Stage4\n",
    "        ])\n",
    "\n",
    "        # Decoding / Upsampling path\n",
    "        # A typical approach is nearest-neighbor interpolation to upsample the sub_points\n",
    "        # and a LocalFeatureAggregation or MLP to fuse skip features.\n",
    "        self.decoders = nn.ModuleList([\n",
    "            MLP(256+128, [256, 128]),\n",
    "            MLP(128+64, [128, 64]),\n",
    "            MLP(64+32, [64, 32]),\n",
    "            MLP(32+32, [32, 32]),\n",
    "        ])\n",
    "\n",
    "        # Final segmentation head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv1d(32, 32, 1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, num_classes, 1)\n",
    "        )\n",
    "\n",
    "    def _nearest_interpolate(self, sub_pts, pts, sub_feats):\n",
    "        \"\"\"\n",
    "        Nearest neighbor interpolation from sub_pts to pts.\n",
    "        sub_pts: (B, N_sub, 3)\n",
    "        pts: (B, N, 3)\n",
    "        sub_feats: (B, N_sub, C)\n",
    "        returns: up_feats (B, N, C)\n",
    "        \"\"\"\n",
    "        # For each point in pts, find its nearest neighbor in sub_pts\n",
    "        # Then gather the features from sub_feats\n",
    "        neighbor_idx = knn_search(pts, sub_pts, k=1)  # (B, N, 1)\n",
    "        # gather sub_feats\n",
    "        B, N, _ = neighbor_idx.shape\n",
    "        C = sub_feats.shape[-1]\n",
    "        neighbor_idx_expand = neighbor_idx.expand(-1, -1, C)  # (B, N, C)\n",
    "        up_feats = torch.gather(sub_feats, 1, neighbor_idx_expand)  # (B, N, C)\n",
    "        return up_feats\n",
    "\n",
    "    def forward(self, pts):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pts: shape (B, N, 3) - input point coordinates. \n",
    "                 For RandLA-Net, we might also have features like color or intensity, but let's keep it simple.\n",
    "        Returns:\n",
    "            logits: (B, num_classes, N)\n",
    "        \"\"\"\n",
    "        B, N, _ = pts.shape\n",
    "        \n",
    "        # Initially, features = coordinates\n",
    "        feats = pts.clone()   # (B, N, 3)\n",
    "\n",
    "        # Lists to store intermediate results for skip connections\n",
    "        pts_list    = [pts]\n",
    "        feats_list  = [feats]\n",
    "\n",
    "        # ---------------- Encoder ----------------\n",
    "        # Stage 1\n",
    "        neighbor_idx = knn_search(pts_list[-1], pts_list[-1], self.num_neighbors)\n",
    "        feats_down = self.encoders[0](pts_list[-1], feats_list[-1], neighbor_idx)\n",
    "        sub_pts, sub_feats = self.samplings[0](pts_list[-1], feats_down)\n",
    "        pts_list.append(sub_pts)\n",
    "        feats_list.append(sub_feats)\n",
    "\n",
    "        # Stage 2\n",
    "        neighbor_idx = knn_search(pts_list[-1], pts_list[-1], self.num_neighbors)\n",
    "        feats_down = self.encoders[1](pts_list[-1], feats_list[-1], neighbor_idx)\n",
    "        sub_pts, sub_feats = self.samplings[1](pts_list[-1], feats_down)\n",
    "        pts_list.append(sub_pts)\n",
    "        feats_list.append(sub_feats)\n",
    "\n",
    "        # Stage 3\n",
    "        neighbor_idx = knn_search(pts_list[-1], pts_list[-1], self.num_neighbors)\n",
    "        feats_down = self.encoders[2](pts_list[-1], feats_list[-1], neighbor_idx)\n",
    "        sub_pts, sub_feats = self.samplings[2](pts_list[-1], feats_down)\n",
    "        pts_list.append(sub_pts)\n",
    "        feats_list.append(sub_feats)\n",
    "\n",
    "        # Stage 4\n",
    "        neighbor_idx = knn_search(pts_list[-1], pts_list[-1], self.num_neighbors)\n",
    "        feats_down = self.encoders[3](pts_list[-1], feats_list[-1], neighbor_idx)\n",
    "        sub_pts, sub_feats = self.samplings[3](pts_list[-1], feats_down)\n",
    "        pts_list.append(sub_pts)\n",
    "        feats_list.append(sub_feats)\n",
    "\n",
    "        # ---------------- Decoder ----------------\n",
    "        # We upsample from stage 4 -> 3, fuse features\n",
    "        for stage_idx in range(4):\n",
    "            i = 4 - stage_idx - 1  # if stage_idx=0 -> i=3, if stage_idx=1-> i=2, ...\n",
    "            sub_pts = pts_list[i+1]\n",
    "            sub_feats = feats_list[i+1]\n",
    "            up_pts = pts_list[i]\n",
    "            up_feats_pre = feats_list[i]  # skip connection\n",
    "\n",
    "            # nearest interpolation from sub_pts to up_pts\n",
    "            up_feats_post = self._nearest_interpolate(sub_pts, up_pts, sub_feats)\n",
    "            # fuse\n",
    "            fused_feats = torch.cat([up_feats_pre, up_feats_post], dim=-1).transpose(1,2)  # (B, C', N)\n",
    "            fused_feats = self.decoders[stage_idx](fused_feats)  # (B, X, N)\n",
    "            feats_list[i] = fused_feats.transpose(1,2)  # (B, N, X)\n",
    "\n",
    "        # After final decoder, feats_list[0] has shape (B, N, 32)\n",
    "        final_feats = feats_list[0].transpose(1,2)  # (B, 32, N)\n",
    "\n",
    "        # Classification head\n",
    "        logits = self.classifier(final_feats)  # (B, num_classes, N)\n",
    "\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files - Train: 93, Validation: 11, Test: 27\n",
      "Train data shape: (7141378, 3) (7141378,)\n",
      "Validation data shape: (731422, 3) (731422,)\n",
      "Test data shape: (1946003, 3) (1946003,)\n"
     ]
    }
   ],
   "source": [
    "laz_file_dir = '../data/downsampled_class_lazFiles/'\n",
    "\n",
    "# Step 1: Get the list of LiDAR files\n",
    "all_files = [os.path.join(laz_file_dir, f) for f in os.listdir(laz_file_dir) if f.endswith('.laz')]\n",
    "\n",
    "# Step 2: Split the files into train, validation, and test sets\n",
    "train_files, test_files = sk.model_selection.train_test_split(all_files, test_size=0.2, random_state=42)  # 20% for testing\n",
    "train_files, val_files = sk.model_selection.train_test_split(train_files, test_size=0.1, random_state=42)  # 10% of train for validation\n",
    "\n",
    "print(f\"Number of files - Train: {len(train_files)}, Validation: {len(val_files)}, Test: {len(test_files)}\")\n",
    "\n",
    "# Step 3: Load and group data based on the splits\n",
    "def load_grouped_data(file_list):\n",
    "    X, y = [], []\n",
    "    for file in file_list:\n",
    "        las = laspy.read(file)\n",
    "        xyz = las.xyz\n",
    "        labels = (las.points.array['classification'] == 12).astype(int)  # Hillfort class\n",
    "        X.append(xyz)\n",
    "        y.append(labels)\n",
    "    return X, y\n",
    "\n",
    "# Load data for each split\n",
    "X_train, y_train = load_grouped_data(train_files)\n",
    "X_val, y_val = load_grouped_data(val_files)\n",
    "X_test, y_test = load_grouped_data(test_files)\n",
    "\n",
    "# Optionally, combine all points into single arrays for each split\n",
    "X_train_combined = np.vstack(X_train)\n",
    "y_train_combined = np.concatenate(y_train)\n",
    "\n",
    "X_val_combined = np.vstack(X_val)\n",
    "y_val_combined = np.concatenate(y_val)\n",
    "\n",
    "X_test_combined = np.vstack(X_test)\n",
    "y_test_combined = np.concatenate(y_test)\n",
    "\n",
    "# Print final shapes\n",
    "print(\"Train data shape:\", X_train_combined.shape, y_train_combined.shape)\n",
    "print(\"Validation data shape:\", X_val_combined.shape, y_val_combined.shape)\n",
    "print(\"Test data shape:\", X_test_combined.shape, y_test_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train_combined))\n",
    "print(np.unique(y_val_combined))\n",
    "print(np.unique(y_test_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5103787  24.58779661]\n"
     ]
    }
   ],
   "source": [
    "class_weights = sk.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train_combined), y=y_train_combined)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).cuda() if device.type == \"cuda\" else torch.tensor(class_weights, dtype=torch.float32)\n",
    "criterion = torch.nn.NLLLoss(weight=class_weights_tensor)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.from_numpy(X_train_combined)\n",
    "train_y = torch.from_numpy(y_train_combined).long()\n",
    "val_X = torch.from_numpy(X_val_combined)\n",
    "val_y = torch.from_numpy(y_val_combined).long()\n",
    "test_X = torch.from_numpy(X_test_combined)\n",
    "test_y = torch.from_numpy(y_test_combined).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudSegDataset(Dataset):\n",
    "    def __init__(self, xyz, labels, num_points=4096):\n",
    "        \"\"\"\n",
    "        xyz: (total_points, 3)\n",
    "        labels: (total_points,)\n",
    "        num_points: number of points in each chunk or random sample\n",
    "        \"\"\"\n",
    "        self.xyz = xyz\n",
    "        self.labels = labels\n",
    "        self.num_points = num_points\n",
    "\n",
    "    def __len__(self):\n",
    "        # Instead of each point being an item, let's define how many items we want:\n",
    "        # For demonstration, treat each call as a random sample from the entire set.\n",
    "        # We'll just artificially define a length, e.g. 1000 \"epochs\" worth of random samples.\n",
    "        return 1000  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Randomly sample 'num_points' indices\n",
    "        total_points = self.xyz.shape[0]\n",
    "        rand_indices = torch.randperm(total_points)[:self.num_points]\n",
    "\n",
    "        pts = self.xyz[rand_indices, :]         # shape (num_points, 3)\n",
    "        seg_labels = self.labels[rand_indices]  # shape (num_points,)\n",
    "\n",
    "        # Return as (B=1, N, 3) inside the batch, but DataLoader will batch multiple samples\n",
    "        # Typically we just return the raw arrays here; DataLoader collates them into a batch\n",
    "        return {\n",
    "            'points': pts,          # (num_points, 3)\n",
    "            'labels': seg_labels    # (num_points,)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 4096  # RandLA-Net uses bigger chunks typically (e.g. 4k..8k)\n",
    "train_dataset = PointCloudSegDataset(train_X, train_y, num_points=num_points)\n",
    "val_dataset   = PointCloudSegDataset(val_X,   val_y,   num_points=num_points)\n",
    "test_dataset  = PointCloudSegDataset(test_X,  test_y,  num_points=num_points)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=8, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RandLANet(num_classes=2, num_neighbors=16).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# For segmentation, each point is classified among 'num_classes'.\n",
    "# If you're weighting classes, you can pass a weight tensor here too:\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.99))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        pts  = batch_data['points'].float().to(device)   # (B, N, 3)\n",
    "        seg  = batch_data['labels'].long().to(device)    # (B, N)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(pts)  # shape (B, num_classes, N)\n",
    "\n",
    "        # Reshape to compute CE loss: \n",
    "        # CrossEntropy expects (B*N, num_classes) and (B*N) for labels\n",
    "        logits = logits.permute(0, 2, 1).contiguous()   # (B, N, num_classes)\n",
    "        loss = criterion(logits.view(-1, 2), seg.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {total_loss/10:.4f}\")\n",
    "            total_loss = 0.0\n",
    "\n",
    "    # Validation step (optional)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in val_loader:\n",
    "            pts  = batch_data['points'].float().to(device)\n",
    "            seg  = batch_data['labels'].long().to(device)\n",
    "            logits = model(pts)\n",
    "            logits = logits.permute(0, 2, 1)\n",
    "            loss = criterion(logits.view(-1, 2), seg.view(-1))\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader) if len(val_loader) > 0 else 1\n",
    "    print(f\"Validation Loss after epoch {epoch+1}: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total_points = 0\n",
    "with torch.no_grad():\n",
    "    for batch_data in test_loader:\n",
    "        pts = batch_data['points'].float().to(device)\n",
    "        seg = batch_data['labels'].long().to(device)\n",
    "        logits = model(pts)\n",
    "        logits = logits.permute(0,2,1)  # (B,N,2)\n",
    "        loss = criterion(logits.view(-1, 2), seg.view(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Predictions\n",
    "        preds = logits.argmax(dim=-1)  # (B, N)\n",
    "        correct += (preds == seg).sum().item()\n",
    "        total_points += seg.numel()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "accuracy = 100.0 * correct / total_points\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes & Caveats\n",
    "Naive k-NN:\n",
    "The included knn_search is CPU-based and scales poorly for large \n",
    "𝑁\n",
    "N. For production or large LiDAR scans, you’ll want a GPU-accelerated neighbor search library (e.g., Faiss, Open3D, or a custom CUDA kernel).\n",
    "Random sampling:\n",
    "Our LocalSamplingLayer and the dataset approach both do random sampling. This might be redundant or suboptimal. RandLA-Net typically does a structured approach (like farthest point sampling) for each hierarchical stage. The code here is just a demonstration.\n",
    "Memory:\n",
    "If you have millions of points, you may need advanced sampling strategies or chunking the point cloud into smaller tiles.\n",
    "Hyperparameters:\n",
    "Adjust num_points, batch size, and learning rate for your dataset. RandLA-Net typically uses big mini-batches of 4k–8k points each.\n",
    "With the dataset + training loop above, you should now be able to initialize and train your simplified RandLA-Net model on LiDAR data to perform binary segmentation (hillfort vs. non-hillfort) at the point level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpulocal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
