{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Hillfort detection with LiDAR data\n",
    "## Data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Table of contents\n",
    "\n",
    "[Code](#code)\n",
    "\n",
    "1. [**Defined functions**](#defined-functions)\n",
    "2. [**Data gathering**](#data-gathering)\n",
    "3. [**Data preprocessing**](#data-preprocessing)\n",
    "4. [**Data augmentation**](#data-augmentation)\n",
    "\n",
    "[End](#end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "# import re\n",
    "# import csv\n",
    "# import typing\n",
    "# import itertools\n",
    "# import json\n",
    "import logging\n",
    "import zipfile\n",
    "# import warnings\n",
    "# import evaluate\n",
    "# import types\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import torch\n",
    "# import math\n",
    "import shapely\n",
    "import matplotlib.pyplot as plt\n",
    "import laspy # Reading LAS file format\n",
    "from tqdm import tqdm # Loading bars\n",
    "import geopandas as gpd\n",
    "import lib.download_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Direct logs to the notebook's output\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lidar_and_polygons(hillfort_polygons_df, input_laz_dir, output_laz_dir):\n",
    "    \"\"\"\n",
    "    Normalizes LiDAR data and corresponding polygons based on the unnormalized bounding box of each LAS file.\n",
    "\n",
    "    Parameters:\n",
    "        hillfort_polygons_df (pd.DataFrame): DataFrame with 'laz_file' and 'polygons' columns.\n",
    "        input_laz_dir (str): Directory containing input LAS files.\n",
    "        output_laz_dir (str): Directory to save normalized LAS files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with normalized polygons.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_laz_dir):\n",
    "        os.makedirs(output_laz_dir)\n",
    "\n",
    "    normalized_polygons = []\n",
    "\n",
    "    for _, row in tqdm(hillfort_polygons_df.iterrows(), total=hillfort_polygons_df.shape[0]):\n",
    "        laz_file = row['laz_file']\n",
    "        polygons = row['polygons']\n",
    "\n",
    "        input_path = os.path.join(input_laz_dir, laz_file)\n",
    "        output_path = os.path.join(output_laz_dir, laz_file)\n",
    "\n",
    "        # Read LAS file\n",
    "        las = laspy.read(input_path)\n",
    "\n",
    "        # Compute bounding box from unnormalized LiDAR data\n",
    "        x_min, x_max = las.header.x_min, las.header.x_max\n",
    "        y_min, y_max = las.header.y_min, las.header.y_max\n",
    "        z_min, z_max = las.header.z_min, las.header.z_max\n",
    "\n",
    "        # Normalize LiDAR data\n",
    "        las.x = (las.x - x_min) / (x_max - x_min)\n",
    "        las.y = (las.y - y_min) / (y_max - y_min)\n",
    "        las.z = (las.z - z_min) / (z_max - z_min)\n",
    "\n",
    "        # Save normalized LAS file\n",
    "        las.write(output_path)\n",
    "\n",
    "        # Normalize polygons using the same bounding box\n",
    "        normalized = []\n",
    "        for polygon in polygons:\n",
    "            scaled = shapely.affinity.scale(\n",
    "                polygon,\n",
    "                xfact=1 / (x_max - x_min),\n",
    "                yfact=1 / (y_max - y_min),\n",
    "                origin=(x_min, y_min)\n",
    "            )\n",
    "            normed = shapely.affinity.translate(scaled, xoff=-x_min, yoff=-y_min)\n",
    "            normalized.append(normed)\n",
    "\n",
    "        # Append normalized polygons with laz_file reference\n",
    "        normalized_polygons.append({'laz_file': laz_file, 'polygons': normalized})\n",
    "\n",
    "    # Convert normalized polygons back to a DataFrame\n",
    "    normalized_polygons_df = pd.DataFrame(normalized_polygons)\n",
    "\n",
    "    print(f\"Normalized LAS files saved to: {output_laz_dir}\")\n",
    "    return normalized_polygons_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_hillforts(laz_files_path='../data/lazFiles/', map_numbers_file='../data/linnamagede_ruudunumbrid.csv', hillfort_polygons_file='../data/inspire/PS_ProtectedSite_malestisedPolygon.shp'):\n",
    "    \"\"\"\n",
    "    Processes hillfort polygon data and associates it with LiDAR laz files.\n",
    "\n",
    "    This function reads hillfort polygon data and matches each polygon to the appropriate\n",
    "    LiDAR laz files based on square numbers. The resulting dataset links each laz file\n",
    "    to the polygons it intersects.\n",
    "\n",
    "    Parameters:\n",
    "        laz_files (list, optional): Path to laz files. If None, reads files from '../data/lazFiles/'.\n",
    "        map_numbers_file (str, optional): Path to the CSV file containing square numbers and hillfort IDs.\n",
    "        hillfort_polygons_file (str, optional): Path to the shapefile containing hillfort polygon geometries.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Optimized DataFrame with columns:\n",
    "            'laz_file': Name of the LiDAR laz file.\n",
    "            'polygons': List of shapely.Polygon objects associated with each laz file.\n",
    "    \"\"\"\n",
    "    # Load laz files\n",
    "    laz_files = os.listdir('../data/lazFiles/')\n",
    "\n",
    "    # Read hillfort data file\n",
    "    gdf_inspire = gpd.read_file(hillfort_polygons_file)\n",
    "    # Read lidar map numbers file\n",
    "    map_numbers = pd.read_csv(map_numbers_file, sep=',')\n",
    "    # Remove rows if no map number or INSPIRE id\n",
    "    map_numbers = map_numbers.dropna(subset=['Ruudunumber', 'INSPIRE id'], how='any')\n",
    "\n",
    "    # Find hillfort polygons\n",
    "    hillfort_polygons = []\n",
    "    for _, row in map_numbers.iterrows():\n",
    "        polygon_series = gdf_inspire[gdf_inspire['inspireid_'] == row['INSPIRE id']]['geometry']\n",
    "        if polygon_series.empty:\n",
    "            logging.info(f\"No polygon found for hillfort: {row['Linnam√§gi']}\")\n",
    "            continue\n",
    "        # Add polygons as individual geometries\n",
    "        hillfort_polygons.append([row['Ruudunumber'], polygon_series.values.tolist()])\n",
    "    \n",
    "    # Convert hillfort polygons to DataFrame\n",
    "    hillfort_polygons_df = pd.DataFrame(hillfort_polygons, columns=['square_nr', 'polygon'])\n",
    "\n",
    "    # Match laz files based on square_nr\n",
    "    def match_laz_files(square_nr):\n",
    "        square_nums = [num.strip() for num in str(square_nr).split(\",\")]\n",
    "        matching_files = [file for file in laz_files if any(num in file for num in square_nums)]\n",
    "        return matching_files\n",
    "\n",
    "    # Add laz_files column to polygons DataFrame\n",
    "    hillfort_polygons_df['laz_files'] = hillfort_polygons_df['square_nr'].apply(match_laz_files)\n",
    "\n",
    "    # Normalize the DataFrame: explode laz_files\n",
    "    normalized_df = hillfort_polygons_df.explode('laz_files')[['laz_files', 'polygon']]\n",
    "\n",
    "    # Group by laz_file and aggregate polygons into a flat list\n",
    "    def flatten_polygon_lists(polygons):\n",
    "        # Flatten nested lists of polygons\n",
    "        return [geom for sublist in polygons for geom in sublist]\n",
    "\n",
    "    # Optimize: group by laz_file and aggregate polygons into a list\n",
    "    optimized_df = normalized_df.groupby('laz_files', as_index=False).agg({'polygon': flatten_polygon_lists})\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    optimized_df.rename(columns={'laz_files': 'laz_file', 'polygon': 'polygons'}, inplace=True)\n",
    "\n",
    "    return optimized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify LAS points' classification inside the polygon\n",
    "def classify_points_in_multiple_areas(las, polygons, new_classification, output_path):\n",
    "    \"\"\"\n",
    "    Updates the classification of LAS file points within multiple polygons.\n",
    "\n",
    "    Parameters:\n",
    "        las (str): Input LAS file.\n",
    "        polygons (list): List of shapely.Polygon objects to classify points inside.\n",
    "        new_classification (int): New classification value for points within the polygons.\n",
    "        output_path (str): Path to save the modified LAS file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract LAS point X, Y coordinates\n",
    "    points = np.column_stack((las.x, las.y))\n",
    "\n",
    "    # Convert points to a Shapely GeometryArray (vectorized)\n",
    "    points_geom = shapely.points(points)\n",
    "\n",
    "    # Create a cumulative mask for all polygons\n",
    "    mask = np.zeros(len(points), dtype=bool)\n",
    "\n",
    "    for polygon in polygons:\n",
    "        # Pre-filter points using the polygon's bounding box\n",
    "        bbox_mask = shapely.contains(shapely.box(*polygon.bounds), points_geom)\n",
    "\n",
    "        # Apply fine-grained point-in-polygon test to remaining points\n",
    "        final_mask = shapely.contains(polygon, points_geom[bbox_mask])\n",
    "\n",
    "        # Combine masks\n",
    "        mask[bbox_mask] |= final_mask\n",
    "\n",
    "    # Apply the new classification to points within any polygon\n",
    "    las.classification[mask] = new_classification\n",
    "\n",
    "    # Write modified points to a new LAS file\n",
    "    las.write(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_polygons_in_las(input_laz_dir, output_laz_dir, polygons, new_classification):\n",
    "    \"\"\"Labels all the points in the given polygons with the new classification in each las file.\n",
    "\n",
    "    Args:\n",
    "        input_laz_dir (string): Path to the laz files directory\n",
    "        output_laz_dir (string): Path to the modified laz files directory\n",
    "        polygons (Polygon): Hillfort area polygons\n",
    "        new_classification (int): The new number to label the points with\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_laz_dir):\n",
    "        os.makedirs(output_laz_dir)\n",
    "\n",
    "    for _, row in tqdm(polygons.iterrows(), total=polygons.shape[0]):\n",
    "        laz_file = row['laz_file']\n",
    "        polygons = row['polygons']\n",
    "\n",
    "        # Paths\n",
    "        input_path = os.path.join(input_laz_dir, laz_file)\n",
    "        output_path = os.path.join(output_laz_dir, laz_file)\n",
    "\n",
    "        # Read the LAS file\n",
    "        las = laspy.read(input_path)\n",
    "\n",
    "        # Classify points within polygons\n",
    "        classify_points_in_multiple_areas(las, polygons, new_classification, output_path)\n",
    "\n",
    "        del las\n",
    "    print(f\"Processed files and saved to {output_laz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize a shapely polygon for overlay\n",
    "def rasterize_polygon(polygon, x_min, x_max, y_min, y_max, resolution):\n",
    "    \"\"\"\n",
    "    Converts a polygon into a binary mask over a 2D grid.\n",
    "\n",
    "    Parameters:\n",
    "        polygon (shapely.Polygon): Polygon to rasterize.\n",
    "        x_min, x_max, y_min, y_max (float): Bounding box of the grid.\n",
    "        resolution (int): Resolution of the grid (number of rows/columns).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 2D binary mask (1 inside the polygon, 0 outside).\n",
    "    \"\"\"\n",
    "    # Initialize an empty grid\n",
    "    grid = np.zeros((resolution, resolution), dtype=np.uint8)\n",
    "    \n",
    "    # Create evenly spaced coordinates within the bounding box\n",
    "    x_range = np.linspace(x_min, x_max, resolution)\n",
    "    y_range = np.linspace(y_min, y_max, resolution)\n",
    "    \n",
    "    # Iterate through each grid cell and check if it is within the polygon\n",
    "    for i, y in enumerate(y_range):\n",
    "        for j, x in enumerate(x_range):\n",
    "            if polygon.contains(shapely.Point(x, y)):  # Check if the grid point is inside the polygon\n",
    "                grid[i, j] = 1  # Mark the grid cell as inside the polygon\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to blend mask into an image with transparency\n",
    "def apply_transparency(image, mask, mask_color, alpha):\n",
    "    \"\"\"\n",
    "    Applies a semi-transparent mask overlay to an image.\n",
    "\n",
    "    Parameters:\n",
    "        image (np.ndarray): Original RGB image (H x W x 3).\n",
    "        mask (np.ndarray): Binary mask (H x W) where 1 indicates the mask region.\n",
    "        mask_color (list): RGB color for the mask (e.g., [255, 0, 0] for red).\n",
    "        alpha (float): Transparency level (0 to 1, where 1 is fully opaque).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: RGB image with the mask applied.\n",
    "    \"\"\"\n",
    "    # Create a copy of the original image to avoid overwriting\n",
    "    blended_image = image.copy()\n",
    "    \n",
    "    # Get indices where the mask is active\n",
    "    mask_indices = mask == 1  # Binary mask where 1 indicates the mask area\n",
    "    \n",
    "    # Blend mask color with the original image in the masked region\n",
    "    for channel in range(3):  # Loop over RGB channels\n",
    "        blended_image[..., channel][mask_indices] = (\n",
    "            (1 - alpha) * blended_image[..., channel][mask_indices] + alpha * mask_color[channel]\n",
    "        )\n",
    "    \n",
    "    return blended_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_color_image(laz_files, resolution=512, additional_features=[]):\n",
    "    \"\"\"\n",
    "    Generates a color image from multiple LAZ files.\n",
    "\n",
    "    Parameters:\n",
    "        laz_files (list): List of LAZ objects containing point cloud data.\n",
    "        resolution (int): Resolution for the output grid.\n",
    "        additional_features (list): List of additional features to include (e.g., 'z', 'classification').\n",
    "\n",
    "    Returns:\n",
    "        tuple: (color_image, additional_feature_maps)\n",
    "            - color_image: 3D numpy array (H x W x 3) for RGB.\n",
    "            - additional_feature_maps: Dictionary of feature maps for additional features.\n",
    "    \"\"\"\n",
    "    # Initialize color image\n",
    "    color_image = np.zeros((resolution, resolution, 3), dtype=np.uint8)\n",
    "\n",
    "    # Additional feature maps\n",
    "    feature_maps = {feature: np.zeros((resolution, resolution)) for feature in additional_features}\n",
    "\n",
    "    for las in laz_files:\n",
    "        # Extract X, Y, RGB\n",
    "        x, y = las.x, las.y\n",
    "        red, green, blue = las.red, las.green, las.blue\n",
    "\n",
    "        # Extract additional features\n",
    "        features = {feature: getattr(las, feature) for feature in additional_features}\n",
    "\n",
    "        # Normalize X, Y to fit into a 2D grid\n",
    "        x_min, x_max = np.min(las.header.x_min), np.max(las.header.x_max)\n",
    "        y_min, y_max = np.min(las.header.y_min), np.max(las.header.y_max)\n",
    "\n",
    "        x_norm = ((x - x_min) / (x_max - x_min)) * (resolution - 1)\n",
    "        y_norm = ((y - y_min) / (y_max - y_min)) * (resolution - 1)\n",
    "\n",
    "        # Convert to integers for indexing\n",
    "        x_indices = np.clip(x_norm.astype(np.int32), 0, resolution - 1)\n",
    "        y_indices = np.clip(y_norm.astype(np.int32), 0, resolution - 1)\n",
    "\n",
    "        # Flatten indices\n",
    "        flat_indices = y_indices * resolution + x_indices\n",
    "        unique_indices, inverse_indices = np.unique(flat_indices, return_inverse=True)\n",
    "\n",
    "        # Aggregate RGB values\n",
    "        red_aggregated = np.zeros(unique_indices.shape, dtype=np.float32)\n",
    "        green_aggregated = np.zeros(unique_indices.shape, dtype=np.float32)\n",
    "        blue_aggregated = np.zeros(unique_indices.shape, dtype=np.float32)\n",
    "\n",
    "        np.add.at(red_aggregated, inverse_indices, red / 65535 * 255)\n",
    "        np.add.at(green_aggregated, inverse_indices, green / 65535 * 255)\n",
    "        np.add.at(blue_aggregated, inverse_indices, blue / 65535 * 255)\n",
    "\n",
    "        # Assign to color image\n",
    "        color_image.flat[unique_indices * 3] = (red_aggregated / np.bincount(inverse_indices)).astype(np.uint8)\n",
    "        color_image.flat[unique_indices * 3 + 1] = (green_aggregated / np.bincount(inverse_indices)).astype(np.uint8)\n",
    "        color_image.flat[unique_indices * 3 + 2] = (blue_aggregated / np.bincount(inverse_indices)).astype(np.uint8)\n",
    "\n",
    "        # Aggregate and assign additional features\n",
    "        for feature, feature_map in feature_maps.items():\n",
    "            feature_aggregated = np.zeros(unique_indices.shape, dtype=np.float32)\n",
    "            np.add.at(feature_aggregated, inverse_indices, features[feature])\n",
    "            feature_map.flat[unique_indices] = feature_aggregated / np.bincount(inverse_indices)\n",
    "\n",
    "    return color_image, feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_color_images(color_image, masked_color_image, title1='Original Colorized Image', title2='Masked Colorized Image'):\n",
    "    \"\"\"\n",
    "    Plots the color image and masked color image side by side.\n",
    "\n",
    "    Parameters:\n",
    "        color_image (numpy.ndarray): Original color image (H x W x 3).\n",
    "        masked_color_image (numpy.ndarray): Masked color image (H x W x 3).\n",
    "        title1 (str): Title for the first subplot.\n",
    "        title2 (str): Title for the second subplot.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(24, 18))\n",
    "\n",
    "    # Original Colorized Image\n",
    "    axs[0].imshow(np.flip(color_image, axis=0))\n",
    "    axs[0].set_title(title1)\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Masked Colorized Image\n",
    "    axs[1].imshow(np.flip(masked_color_image, axis=0))\n",
    "    axs[1].set_title(title2)\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_laz_files(square_nr, laz_files):\n",
    "    \"\"\"For each unique laz file gathers all the polygons related to it\n",
    "\n",
    "    Args:\n",
    "        square_nr (string): Square numbers in the data file\n",
    "        laz_files (list): List of laz files to match polygons with\n",
    "\n",
    "    Returns:\n",
    "        list: Unique list of unique laz files with all the polygons related to them\n",
    "    \"\"\"\n",
    "    # Split the square_nr string into individual numbers\n",
    "    square_nums = [num.strip() for num in str(square_nr).split(\",\")]\n",
    "    # Check if any square number is part of the laz file name\n",
    "    matching_files = [file for file in laz_files if any(num in file for num in square_nums)]\n",
    "    return matching_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip zip file\n",
    "if not os.path.exists('../data/inspire/PS_ProtectedSite_malestisedPolygon.shp'):\n",
    "    with zipfile.ZipFile(os.path.join('../data/inspire/PS_ProtectedSite_malestised.zip'), 'r') as zip:\n",
    "        zip.extractall('../data/inspire/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "map_numbers = '../data/linnamagede_ruudunumbrid.csv'\n",
    "hillfort_polygons = '../data/inspire/PS_ProtectedSite_malestisedPolygon.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download lidar maps\n",
    "if not os.path.exists('../data/lazFiles/'):\n",
    "    os.makedirs('../data/lazFiles/', exist_ok=True)\n",
    "    print(\"Downloading lidar maps\")\n",
    "    lib.download_maps.process_csv(input_csv=map_numbers, output_dir='../data/lazFiles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_laz_dir = '../data/lazFiles/'\n",
    "output_laz_dir = '../data/normalized_lazFiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>laz_file</th>\n",
       "      <th>polygons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((689215.4800090133 6397533.169809564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402642_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((642559.2100067963 6402702.98980967,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>402674_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((674706.6800083275 6402314.859809687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>407676_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((676672.7800084257 6407956.299809841...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>407700_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((700119.2900095425 6407732.169809859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>596650_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((650948.8500073353 6596229.389815392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>596651_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((650948.8500073353 6596229.389815392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>598586_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((586201.6800041909 6598158.139815415...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>600638_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((638810.0200067485 6600382.429815512...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((639654.4100067897 6600836.849815527...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 laz_file                                           polygons\n",
       "0    397689_2023_tava.laz  [POLYGON ((689215.4800090133 6397533.169809564...\n",
       "1    402642_2023_tava.laz  [POLYGON ((642559.2100067963 6402702.98980967,...\n",
       "2    402674_2023_tava.laz  [POLYGON ((674706.6800083275 6402314.859809687...\n",
       "3    407676_2023_tava.laz  [POLYGON ((676672.7800084257 6407956.299809841...\n",
       "4    407700_2023_tava.laz  [POLYGON ((700119.2900095425 6407732.169809859...\n",
       "..                    ...                                                ...\n",
       "126  596650_2022_tava.laz  [POLYGON ((650948.8500073353 6596229.389815392...\n",
       "127  596651_2022_tava.laz  [POLYGON ((650948.8500073353 6596229.389815392...\n",
       "128  598586_2022_tava.laz  [POLYGON ((586201.6800041909 6598158.139815415...\n",
       "129  600638_2022_tava.laz  [POLYGON ((638810.0200067485 6600382.429815512...\n",
       "130  600639_2022_tava.laz  [POLYGON ((639654.4100067897 6600836.849815527...\n",
       "\n",
       "[131 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gather laz files and their associated polygons\n",
    "polygons = gather_hillforts(\n",
    "    laz_files_path=input_laz_dir,\n",
    "    map_numbers_file=map_numbers, \n",
    "    hillfort_polygons_file=hillfort_polygons)\n",
    "display(polygons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 131/131 [02:48<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized LAS files saved to: ../data/normalized_lazFiles/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "normalized_polygons = normalize_lidar_and_polygons(polygons, input_laz_dir, output_laz_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>laz_file</th>\n",
       "      <th>polygons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.2154821638250724 0.53317514155060...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402642_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.559215598856099 0.702996839769184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>402674_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.7066870752023533 0.31486295908689...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>407676_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.6727867362787947 0.95630937255918...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>407700_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.119291202398017 0.732177131809294...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>596650_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.9488594959257171 0.22939210943877...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>596651_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((-0.0511505041504279 0.2293921094387...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>598586_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.2016820210264996 0.15814139693975...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>600638_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.8100281070219353 0.38243363983929...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.654416550998576 0.836858185008168...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 laz_file                                           polygons\n",
       "0    397689_2023_tava.laz  [POLYGON ((0.2154821638250724 0.53317514155060...\n",
       "1    402642_2023_tava.laz  [POLYGON ((0.559215598856099 0.702996839769184...\n",
       "2    402674_2023_tava.laz  [POLYGON ((0.7066870752023533 0.31486295908689...\n",
       "3    407676_2023_tava.laz  [POLYGON ((0.6727867362787947 0.95630937255918...\n",
       "4    407700_2023_tava.laz  [POLYGON ((0.119291202398017 0.732177131809294...\n",
       "..                    ...                                                ...\n",
       "126  596650_2022_tava.laz  [POLYGON ((0.9488594959257171 0.22939210943877...\n",
       "127  596651_2022_tava.laz  [POLYGON ((-0.0511505041504279 0.2293921094387...\n",
       "128  598586_2022_tava.laz  [POLYGON ((0.2016820210264996 0.15814139693975...\n",
       "129  600638_2022_tava.laz  [POLYGON ((0.8100281070219353 0.38243363983929...\n",
       "130  600639_2022_tava.laz  [POLYGON ((0.654416550998576 0.836858185008168...\n",
       "\n",
       "[131 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(normalized_polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of polygons to a list of WKT strings\n",
    "normalized_polygons['polygons_wkt'] = normalized_polygons['polygons'].apply(lambda polygons: [p.wkt for p in polygons])\n",
    "\n",
    "# Save to CSV\n",
    "normalized_polygons[['laz_file', 'polygons_wkt']].to_csv('../data/normalized_polygons.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_laz_dir = '../data/normalized_lazFiles/'\n",
    "output_laz_dir = '../data/classified_lazFiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 131/131 [07:13<00:00,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed files and saved to ../data/classified_lazFiles/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mark_polygons_in_las(input_laz_dir, output_laz_dir, normalized_polygons, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpulocal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
