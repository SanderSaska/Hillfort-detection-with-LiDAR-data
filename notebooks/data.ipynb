{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Hillfort detection with LiDAR data\n",
    "## Data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Table of contents\n",
    "\n",
    "[Code](#code)\n",
    "\n",
    "1. [**Defined functions**](#defined-functions)\n",
    "2. [**Data gathering**](#data-gathering)\n",
    "3. [**Data preprocessing**](#data-preprocessing)\n",
    "4. [**Data augmentation**](#data-augmentation)\n",
    "\n",
    "[End](#end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module installs\n",
    "!pip install zipfile pandas numpy shapely matplotlib laspy tqdm geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have GPU\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "# Otherwise\n",
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shapely # For optimized shape functions\n",
    "import matplotlib.pyplot as plt\n",
    "import laspy # Reading LAS file format\n",
    "from tqdm import tqdm # Loading bars\n",
    "import geopandas as gpd # Reading .shp files\n",
    "import lib.download_maps # Custom downloading script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Direct logs to the notebook's output\n",
    "    ]\n",
    ")\n",
    "\n",
    "GLOBAL_MIN_X, GLOBAL_MAX_X = 6374000, 6641000\n",
    "GLOBAL_MIN_Y, GLOBAL_MAX_Y = 364000, 742000\n",
    "GLOBAL_MIN_Z, GLOBAL_MAX_Z = -125, 320 # Munamägi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lidar_and_polygons(hillfort_polygons_df, input_laz_dir, output_laz_dir, output_csv_filepath):\n",
    "    \"\"\"\n",
    "    Normalizes LiDAR data and corresponding polygons based on the unnormalized bounding box of each LAS file.\n",
    "\n",
    "    Parameters:\n",
    "        hillfort_polygons_df (pd.DataFrame): DataFrame with 'laz_file' and 'polygons' columns.\n",
    "        input_laz_dir (str): Directory containing input LAS files.\n",
    "        output_laz_dir (str): Directory to save normalized LAS files.\n",
    "        output_csv_file (str): CSV file containing normalized output polygons.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_laz_dir):\n",
    "        os.makedirs(output_laz_dir)\n",
    "\n",
    "    normalized_polygons = []\n",
    "\n",
    "    for _, row in tqdm(hillfort_polygons_df.iterrows(), total=hillfort_polygons_df.shape[0]):\n",
    "        laz_file = row['laz_file']\n",
    "        polygons = row['polygons']\n",
    "\n",
    "        input_path = os.path.join(input_laz_dir, laz_file)\n",
    "        output_path = os.path.join(output_laz_dir, laz_file)\n",
    "\n",
    "        # Read LAS file\n",
    "        las = laspy.read(input_path)\n",
    "\n",
    "        # Normalize LiDAR data\n",
    "        # Normalize x, y, z\n",
    "        new_x = (las.x - GLOBAL_MIN_Y) / (GLOBAL_MAX_Y - GLOBAL_MIN_Y)\n",
    "        new_y = (las.y - GLOBAL_MIN_X) / (GLOBAL_MAX_X - GLOBAL_MIN_X)\n",
    "        new_z = (las.z - GLOBAL_MIN_Z) / (GLOBAL_MAX_Z - GLOBAL_MIN_Z)\n",
    "\n",
    "        # Update the header for new scale and offset\n",
    "        las.header.offsets = [0.0, 0.0, 0.0]  # Reset offsets to 0.0\n",
    "        las.header.scales = [1e-9, 1e-9, 1e-9]  # Small scale to preserve precision\n",
    "\n",
    "        # Assign normalized coordinates back\n",
    "        las.x = new_x\n",
    "        las.y = new_y\n",
    "        las.z = new_z\n",
    "\n",
    "        # Save normalized LAS file\n",
    "        las.write(output_path)\n",
    "\n",
    "        # Normalize polygons using the same bounding box\n",
    "        normalized = []\n",
    "        for polygon in polygons:\n",
    "            scaled = shapely.affinity.scale(\n",
    "                polygon,\n",
    "                xfact=1 / (GLOBAL_MAX_Y - GLOBAL_MIN_Y),\n",
    "                yfact=1 / (GLOBAL_MAX_X - GLOBAL_MIN_X),\n",
    "                origin=(GLOBAL_MIN_Y, GLOBAL_MIN_X)\n",
    "            )\n",
    "            normed = shapely.affinity.translate(scaled, xoff=-GLOBAL_MIN_Y, yoff=-GLOBAL_MIN_X)\n",
    "            normalized.append(normed)\n",
    "\n",
    "        # Append normalized polygons with laz_file reference\n",
    "        normalized_polygons.append({'laz_file': laz_file, 'polygons': normalized})\n",
    "\n",
    "    # Convert normalized polygons back to a DataFrame\n",
    "    normalized_polygons_df = pd.DataFrame(normalized_polygons)\n",
    "    \n",
    "    # Convert the list of polygons to a list of WKT strings\n",
    "    normalized_polygons_df['polygons_wkt'] = normalized_polygons_df['polygons'].apply(lambda polygons: [p.wkt for p in polygons])\n",
    "\n",
    "    # Save to CSV\n",
    "    normalized_polygons_df[['laz_file', 'polygons_wkt']].to_csv(output_csv_filepath, index=False)\n",
    "\n",
    "    print(f\"Normalized LAS files saved to: {output_laz_dir}\")\n",
    "    print(f\"Normalized polygons saved to: {output_csv_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_lidar(input_laz_dir, grid_size=1e-4, output_laz_dir='../data/downsampled_class_lazFiles'):\n",
    "    \"\"\"\n",
    "    Downsamples a LAS file using grid method and saves them to specified path.\n",
    "\n",
    "    Parameters:\n",
    "        input_laz_dir (str): Directory containing input LAS files.\n",
    "        grid_size (float): Size of grid cells for grid-based downsampling.\n",
    "        output_laz_dir (str): Directory to save normalized LAS files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_laz_dir):\n",
    "        os.makedirs(output_laz_dir)\n",
    "\n",
    "    laz_files = os.listdir(input_laz_dir)\n",
    "\n",
    "    for laz_file in tqdm(laz_files, total=len(laz_files)):\n",
    "\n",
    "        input_path = os.path.join(input_laz_dir, laz_file)\n",
    "        output_path = os.path.join(output_laz_dir, laz_file)\n",
    "\n",
    "        # Read LAS file\n",
    "        las = laspy.read(input_path)\n",
    "\n",
    "        # Calculate grid indices for each point\n",
    "        grid_x = np.floor(las.x / grid_size)\n",
    "        grid_y = np.floor(las.y / grid_size)\n",
    "\n",
    "        # Combine grid indices into tuples\n",
    "        grid_indices = list(zip(grid_x, grid_y))\n",
    "\n",
    "        # Group points by grid cell\n",
    "        grid_point_map = {}\n",
    "        for idx, grid in enumerate(grid_indices):\n",
    "            if grid not in grid_point_map:\n",
    "                grid_point_map[grid] = []\n",
    "            grid_point_map[grid].append(idx)\n",
    "\n",
    "        # Downsample by keeping a representative sample of points in each grid cell\n",
    "        sampled_indices = []\n",
    "        for points_in_cell in grid_point_map.values():\n",
    "            # Option 1: Keep a fixed percentage of points per cell\n",
    "            sampled_indices.extend(random.sample(points_in_cell, max(1, len(points_in_cell) // 50)))\n",
    "\n",
    "            # Option 2: Keep a fixed number of points per cell (e.g., up to 5)\n",
    "            # sampled_indices.extend(points_in_cell[:5])  # Adjust this value as needed\n",
    "\n",
    "        # Create the downsampled LAS file\n",
    "        downsampled_las = laspy.create(file_version=las.header.version, point_format=las.header.point_format)\n",
    "\n",
    "        # Copy header information (scale and offset) explicitly\n",
    "        downsampled_las.header.scales = las.header.scales\n",
    "        downsampled_las.header.offsets = las.header.offsets\n",
    "\n",
    "        # Copy downsampled point data\n",
    "        for dim in las.point_format.dimension_names:\n",
    "            setattr(downsampled_las, dim, getattr(las, dim)[sampled_indices])\n",
    "\n",
    "        # Save the downsampled LAS file\n",
    "        downsampled_las.write(output_path, do_compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_hillforts(laz_files_path='../data/lazFiles/', map_numbers_file='../data/linnamagede_ruudunumbrid.csv', hillfort_polygons_file='../data/inspire/PS_ProtectedSite_malestisedPolygon.shp'):\n",
    "    \"\"\"\n",
    "    Processes hillfort polygon data and associates it with LiDAR laz files.\n",
    "\n",
    "    This function reads hillfort polygon data and matches each polygon to the appropriate\n",
    "    LiDAR laz files based on square numbers. The resulting dataset links each laz file\n",
    "    to the polygons it intersects.\n",
    "\n",
    "    Parameters:\n",
    "        laz_files (list, optional): Path to laz files. If None, reads files from '../data/lazFiles/'.\n",
    "        map_numbers_file (str, optional): Path to the CSV file containing square numbers and hillfort IDs.\n",
    "        hillfort_polygons_file (str, optional): Path to the shapefile containing hillfort polygon geometries.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Optimized DataFrame with columns:\n",
    "            'laz_file': Name of the LiDAR laz file.\n",
    "            'polygons': List of shapely.Polygon objects associated with each laz file.\n",
    "    \"\"\"\n",
    "    # Load laz files\n",
    "    laz_files = os.listdir('../data/lazFiles/')\n",
    "\n",
    "    # Read hillfort data file\n",
    "    gdf_inspire = gpd.read_file(hillfort_polygons_file)\n",
    "    # Read lidar map numbers file\n",
    "    map_numbers = pd.read_csv(map_numbers_file, sep=',')\n",
    "    # Remove rows if no map number or INSPIRE id\n",
    "    map_numbers = map_numbers.dropna(subset=['Ruudunumber', 'INSPIRE id'], how='any')\n",
    "\n",
    "    # Find hillfort polygons\n",
    "    hillfort_polygons = []\n",
    "    for _, row in map_numbers.iterrows():\n",
    "        polygon_series = gdf_inspire[gdf_inspire['inspireid_'] == row['INSPIRE id']]['geometry']\n",
    "        if polygon_series.empty:\n",
    "            logging.info(f\"No polygon found for hillfort: {row['Linnamägi']}\")\n",
    "            continue\n",
    "        # Add polygons as individual geometries\n",
    "        hillfort_polygons.append([row['Ruudunumber'], polygon_series.values.tolist()])\n",
    "    \n",
    "    # Convert hillfort polygons to DataFrame\n",
    "    hillfort_polygons_df = pd.DataFrame(hillfort_polygons, columns=['square_nr', 'polygon'])\n",
    "\n",
    "    # Match laz files based on square_nr\n",
    "    def match_laz_files(square_nr):\n",
    "        square_nums = [num.strip() for num in str(square_nr).split(\",\")]\n",
    "        matching_files = [file for file in laz_files if any(num in file for num in square_nums)]\n",
    "        return matching_files\n",
    "\n",
    "    # Add laz_files column to polygons DataFrame\n",
    "    hillfort_polygons_df['laz_files'] = hillfort_polygons_df['square_nr'].apply(match_laz_files)\n",
    "\n",
    "    # Normalize the DataFrame: explode laz_files\n",
    "    normalized_df = hillfort_polygons_df.explode('laz_files')[['laz_files', 'polygon']]\n",
    "\n",
    "    # Group by laz_file and aggregate polygons into a flat list\n",
    "    def flatten_polygon_lists(polygons):\n",
    "        # Flatten nested lists of polygons\n",
    "        return [geom for sublist in polygons for geom in sublist]\n",
    "\n",
    "    # Optimize: group by laz_file and aggregate polygons into a list\n",
    "    optimized_df = normalized_df.groupby('laz_files', as_index=False).agg({'polygon': flatten_polygon_lists})\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    optimized_df.rename(columns={'laz_files': 'laz_file', 'polygon': 'polygons'}, inplace=True)\n",
    "\n",
    "    return optimized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify LAS points' classification inside the polygon\n",
    "def classify_points_in_multiple_areas(las, polygons, new_classification, output_path):\n",
    "    \"\"\"\n",
    "    Updates the classification of LAS file points within multiple polygons.\n",
    "\n",
    "    Parameters:\n",
    "        las (str): Input LAS file.\n",
    "        polygons (list): List of shapely.Polygon objects to classify points inside.\n",
    "        new_classification (int): New classification value for points within the polygons.\n",
    "        output_path (str): Path to save the modified LAS file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract LAS point X, Y coordinates\n",
    "    points = np.column_stack((las.x, las.y))\n",
    "\n",
    "    # Convert points to a Shapely GeometryArray (vectorized)\n",
    "    points_geom = shapely.points(points)\n",
    "\n",
    "    # Create a cumulative mask for all polygons\n",
    "    mask = np.zeros(len(points), dtype=bool)\n",
    "\n",
    "    for polygon in polygons:\n",
    "        # Pre-filter points using the polygon's bounding box\n",
    "        bbox_mask = shapely.contains(shapely.box(*polygon.bounds), points_geom)\n",
    "\n",
    "        # Apply fine-grained point-in-polygon test to remaining points\n",
    "        final_mask = shapely.contains(polygon, points_geom[bbox_mask])\n",
    "\n",
    "        # Combine masks\n",
    "        mask[bbox_mask] |= final_mask\n",
    "\n",
    "    # Apply the new classification to points within any polygon\n",
    "    las.classification[mask] = new_classification\n",
    "\n",
    "    # Write modified points to a new LAS file\n",
    "    las.write(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_polygons_in_las(input_laz_dir, output_laz_dir, polygons, new_classification):\n",
    "    \"\"\"Labels all the points in the given polygons with the new classification in each las file.\n",
    "\n",
    "    Parameters:\n",
    "        input_laz_dir (string): Path to the laz files directory\n",
    "        output_laz_dir (string): Path to the modified laz files directory\n",
    "        polygons (Polygon): Hillfort area polygons\n",
    "        new_classification (int): The new number to label the points with\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_laz_dir):\n",
    "        os.makedirs(output_laz_dir)\n",
    "\n",
    "    for _, row in tqdm(polygons.iterrows(), total=polygons.shape[0]):\n",
    "        laz_file = row['laz_file']\n",
    "        polygons = row['polygons']\n",
    "\n",
    "        # Paths\n",
    "        input_path = os.path.join(input_laz_dir, laz_file)\n",
    "        output_path = os.path.join(output_laz_dir, laz_file)\n",
    "\n",
    "        # Read the LAS file\n",
    "        las = laspy.read(input_path)\n",
    "\n",
    "        # Classify points within polygons\n",
    "        classify_points_in_multiple_areas(las, polygons, new_classification, output_path)\n",
    "\n",
    "        del las\n",
    "    print(f\"Processed files and saved to {output_laz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize a shapely polygon for overlay\n",
    "def rasterize_polygon(polygon, x_min, x_max, y_min, y_max, resolution):\n",
    "    \"\"\"\n",
    "    Converts a polygon into a binary mask over a 2D grid.\n",
    "\n",
    "    Parameters:\n",
    "        polygon (shapely.Polygon): Polygon to rasterize.\n",
    "        x_min, x_max, y_min, y_max (float): Bounding box of the grid.\n",
    "        resolution (int): Resolution of the grid (number of rows/columns).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 2D binary mask (1 inside the polygon, 0 outside).\n",
    "    \"\"\"\n",
    "    # Initialize an empty grid\n",
    "    grid = np.zeros((resolution, resolution), dtype=np.uint8)\n",
    "    \n",
    "    # Create evenly spaced coordinates within the bounding box\n",
    "    x_range = np.linspace(x_min, x_max, resolution)\n",
    "    y_range = np.linspace(y_min, y_max, resolution)\n",
    "    \n",
    "    # Iterate through each grid cell and check if it is within the polygon\n",
    "    for i, y in enumerate(y_range):\n",
    "        for j, x in enumerate(x_range):\n",
    "            if polygon.contains(shapely.Point(x, y)):  # Check if the grid point is inside the polygon\n",
    "                grid[i, j] = 1  # Mark the grid cell as inside the polygon\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to blend mask into an image with transparency\n",
    "def apply_transparency(image, mask, mask_color, alpha):\n",
    "    \"\"\"\n",
    "    Applies a semi-transparent mask overlay to an image.\n",
    "\n",
    "    Parameters:\n",
    "        image (np.ndarray): Original RGB image (H x W x 3).\n",
    "        mask (np.ndarray): Binary mask (H x W) where 1 indicates the mask region.\n",
    "        mask_color (list): RGB color for the mask (e.g., [255, 0, 0] for red).\n",
    "        alpha (float): Transparency level (0 to 1, where 1 is fully opaque).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: RGB image with the mask applied.\n",
    "    \"\"\"\n",
    "    # Create a copy of the original image to avoid overwriting\n",
    "    blended_image = image.copy()\n",
    "    \n",
    "    # Get indices where the mask is active\n",
    "    mask_indices = mask == 1  # Binary mask where 1 indicates the mask area\n",
    "    \n",
    "    # Blend mask color with the original image in the masked region\n",
    "    for channel in range(3):  # Loop over RGB channels\n",
    "        blended_image[..., channel][mask_indices] = (\n",
    "            (1 - alpha) * blended_image[..., channel][mask_indices] + alpha * mask_color[channel]\n",
    "        )\n",
    "    \n",
    "    return blended_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_color_image(laz_files, resolution=512, additional_features=[]):\n",
    "    \"\"\"\n",
    "    Generates a color image from multiple LAZ files.\n",
    "\n",
    "    Parameters:\n",
    "        laz_files (list): List of LAZ objects containing point cloud data.\n",
    "        resolution (int): Resolution for the output grid.\n",
    "        additional_features (list): List of additional features to include (e.g., 'z', 'classification').\n",
    "\n",
    "    Returns:\n",
    "        tuple: (color_image, additional_feature_maps)\n",
    "            - color_image: 3D numpy array (H x W x 3) for RGB.\n",
    "            - additional_feature_maps: Dictionary of feature maps for additional features.\n",
    "    \"\"\"\n",
    "    # Initialize color image\n",
    "    color_image = np.zeros((resolution, resolution, 3), dtype=np.uint8)\n",
    "\n",
    "    # Additional feature maps\n",
    "    feature_maps = {feature: np.zeros((resolution, resolution)) for feature in additional_features}\n",
    "\n",
    "    for las in laz_files:\n",
    "        # Extract X, Y, RGB\n",
    "        x, y = las.x, las.y\n",
    "        red, green, blue = las.red, las.green, las.blue\n",
    "\n",
    "        # Extract additional features\n",
    "        features = {feature: getattr(las, feature) for feature in additional_features}\n",
    "\n",
    "        # Normalize X, Y to fit into a 2D grid\n",
    "        x_min, x_max = np.min(las.header.x_min), np.max(las.header.x_max)\n",
    "        y_min, y_max = np.min(las.header.y_min), np.max(las.header.y_max)\n",
    "\n",
    "        x_norm = ((x - x_min) / (x_max - x_min)) * (resolution - 1)\n",
    "        y_norm = ((y - y_min) / (y_max - y_min)) * (resolution - 1)\n",
    "\n",
    "        # Convert to integers for indexing\n",
    "        x_indices = np.clip(x_norm.astype(np.int32), 0, resolution - 1)\n",
    "        y_indices = np.clip(y_norm.astype(np.int32), 0, resolution - 1)\n",
    "\n",
    "        # Flatten indices\n",
    "        flat_indices = y_indices * resolution + x_indices\n",
    "        unique_indices, inverse_indices = np.unique(flat_indices, return_inverse=True)\n",
    "\n",
    "        # Aggregate RGB values\n",
    "        red_aggregated = np.zeros(unique_indices.shape, dtype=np.float32)\n",
    "        green_aggregated = np.zeros(unique_indices.shape, dtype=np.float32)\n",
    "        blue_aggregated = np.zeros(unique_indices.shape, dtype=np.float32)\n",
    "\n",
    "        np.add.at(red_aggregated, inverse_indices, red / 65535 * 255)\n",
    "        np.add.at(green_aggregated, inverse_indices, green / 65535 * 255)\n",
    "        np.add.at(blue_aggregated, inverse_indices, blue / 65535 * 255)\n",
    "\n",
    "        # Assign to color image\n",
    "        color_image.flat[unique_indices * 3] = (red_aggregated / np.bincount(inverse_indices)).astype(np.uint8)\n",
    "        color_image.flat[unique_indices * 3 + 1] = (green_aggregated / np.bincount(inverse_indices)).astype(np.uint8)\n",
    "        color_image.flat[unique_indices * 3 + 2] = (blue_aggregated / np.bincount(inverse_indices)).astype(np.uint8)\n",
    "\n",
    "        # Aggregate and assign additional features\n",
    "        for feature, feature_map in feature_maps.items():\n",
    "            feature_aggregated = np.zeros(unique_indices.shape, dtype=np.float32)\n",
    "            np.add.at(feature_aggregated, inverse_indices, features[feature])\n",
    "            feature_map.flat[unique_indices] = feature_aggregated / np.bincount(inverse_indices)\n",
    "\n",
    "    return color_image, feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_color_images(color_image, masked_color_image, title1='Original Colorized Image', title2='Masked Colorized Image'):\n",
    "    \"\"\"\n",
    "    Plots the color image and masked color image side by side.\n",
    "\n",
    "    Parameters:\n",
    "        color_image (numpy.ndarray): Original color image (H x W x 3).\n",
    "        masked_color_image (numpy.ndarray): Masked color image (H x W x 3).\n",
    "        title1 (str): Title for the first subplot.\n",
    "        title2 (str): Title for the second subplot.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(24, 18))\n",
    "\n",
    "    # Original Colorized Image\n",
    "    axs[0].imshow(np.flip(color_image, axis=0))\n",
    "    axs[0].set_title(title1)\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Masked Colorized Image\n",
    "    axs[1].imshow(np.flip(masked_color_image, axis=0))\n",
    "    axs[1].set_title(title2)\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_laz_files(square_nr, laz_files):\n",
    "    \"\"\"For each unique laz file gathers all the polygons related to it\n",
    "\n",
    "    Parameters:\n",
    "        square_nr (string): Square numbers in the data file\n",
    "        laz_files (list): List of laz files to match polygons with\n",
    "\n",
    "    Returns:\n",
    "        list: Unique list of unique laz files with all the polygons related to them\n",
    "    \"\"\"\n",
    "    # Split the square_nr string into individual numbers\n",
    "    square_nums = [num.strip() for num in str(square_nr).split(\",\")]\n",
    "    # Check if any square number is part of the laz file name\n",
    "    matching_files = [file for file in laz_files if any(num in file for num in square_nums)]\n",
    "    return matching_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip zip file\n",
    "if not os.path.exists('../data/inspire/PS_ProtectedSite_malestisedPolygon.shp'):\n",
    "    with zipfile.ZipFile(os.path.join('../data/inspire/PS_ProtectedSite_malestised.zip'), 'r') as zip:\n",
    "        zip.extractall('../data/inspire/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "map_numbers = '../data/linnamagede_ruudunumbrid.csv'\n",
    "hillfort_polygons = '../data/inspire/PS_ProtectedSite_malestisedPolygon.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download lidar maps\n",
    "if not os.path.exists('../data/lazFiles/'):\n",
    "    os.makedirs('../data/lazFiles/', exist_ok=True)\n",
    "    print(\"Downloading lidar maps\")\n",
    "    lib.download_maps.process_csv(input_csv=map_numbers, output_dir='../data/lazFiles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_laz_dir = '../data/lazFiles/'\n",
    "output_laz_dir = '../data/classified_lazFiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>laz_file</th>\n",
       "      <th>polygons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((689215.4800090133 6397533.169809564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402642_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((642559.2100067963 6402702.98980967,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>402674_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((674706.6800083275 6402314.859809687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>407676_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((676672.7800084257 6407956.299809841...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>407700_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((700119.2900095425 6407732.169809859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>596650_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((650948.8500073353 6596229.389815392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>596651_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((650948.8500073353 6596229.389815392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>598586_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((586201.6800041909 6598158.139815415...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>600638_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((638810.0200067485 6600382.429815512...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((639654.4100067897 6600836.849815527...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 laz_file                                           polygons\n",
       "0    397689_2023_tava.laz  [POLYGON ((689215.4800090133 6397533.169809564...\n",
       "1    402642_2023_tava.laz  [POLYGON ((642559.2100067963 6402702.98980967,...\n",
       "2    402674_2023_tava.laz  [POLYGON ((674706.6800083275 6402314.859809687...\n",
       "3    407676_2023_tava.laz  [POLYGON ((676672.7800084257 6407956.299809841...\n",
       "4    407700_2023_tava.laz  [POLYGON ((700119.2900095425 6407732.169809859...\n",
       "..                    ...                                                ...\n",
       "126  596650_2022_tava.laz  [POLYGON ((650948.8500073353 6596229.389815392...\n",
       "127  596651_2022_tava.laz  [POLYGON ((650948.8500073353 6596229.389815392...\n",
       "128  598586_2022_tava.laz  [POLYGON ((586201.6800041909 6598158.139815415...\n",
       "129  600638_2022_tava.laz  [POLYGON ((638810.0200067485 6600382.429815512...\n",
       "130  600639_2022_tava.laz  [POLYGON ((639654.4100067897 6600836.849815527...\n",
       "\n",
       "[131 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gather laz files and their associated polygons\n",
    "polygons = gather_hillforts(\n",
    "    laz_files_path=input_laz_dir,\n",
    "    map_numbers_file=map_numbers, \n",
    "    hillfort_polygons_file=hillfort_polygons)\n",
    "display(polygons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [07:10<00:00,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed files and saved to ../data/classified_lazFiles/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mark_polygons_in_las(input_laz_dir, output_laz_dir, polygons, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_laz_dir = '../data/classified_lazFiles/'\n",
    "output_laz_dir = '../data/normalized_lazFiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [02:39<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized LAS files saved to: ../data/normalized_lazFiles/\n",
      "Normalized polygons saved to: ../data/normalized_polygons.csv\n"
     ]
    }
   ],
   "source": [
    "normalize_lidar_and_polygons(polygons, input_laz_dir, output_laz_dir, '../data/normalized_polygons.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>laz_file</th>\n",
       "      <th>polygons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.8603584127267823 0.08813921269029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402642_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.7369291270151734 0.10750183463096...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>402674_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.8219753439188935 0.10604816488921...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>407676_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.8271766667021438 0.12717715278267...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>407700_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.8892044709064066 0.12633771542459...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>596650_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.7591239418252371 0.83231981284916...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>596651_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.7591239418252371 0.83231981284916...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>598586_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.5878351322608069 0.83954359497874...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>600638_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.727010634902399 0.847874268889427...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.7292444708873518 0.84957621712237...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 laz_file                                           polygons\n",
       "0    397689_2023_tava.laz  [POLYGON ((0.8603584127267823 0.08813921269029...\n",
       "1    402642_2023_tava.laz  [POLYGON ((0.7369291270151734 0.10750183463096...\n",
       "2    402674_2023_tava.laz  [POLYGON ((0.8219753439188935 0.10604816488921...\n",
       "3    407676_2023_tava.laz  [POLYGON ((0.8271766667021438 0.12717715278267...\n",
       "4    407700_2023_tava.laz  [POLYGON ((0.8892044709064066 0.12633771542459...\n",
       "..                    ...                                                ...\n",
       "126  596650_2022_tava.laz  [POLYGON ((0.7591239418252371 0.83231981284916...\n",
       "127  596651_2022_tava.laz  [POLYGON ((0.7591239418252371 0.83231981284916...\n",
       "128  598586_2022_tava.laz  [POLYGON ((0.5878351322608069 0.83954359497874...\n",
       "129  600638_2022_tava.laz  [POLYGON ((0.727010634902399 0.847874268889427...\n",
       "130  600639_2022_tava.laz  [POLYGON ((0.7292444708873518 0.84957621712237...\n",
       "\n",
       "[131 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"0.8599969982448965 0.08705005634576082 0.0007467442215420839 0.0011310469731688616\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,0.1752311596646905)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"2.2620939463377233e-05\" opacity=\"0.6\" d=\"M 0.8603584127267823,0.0881392126902938 L 0.8606247089919634,0.0879507111385465 L 0.8606968518579379,0.0877036321908236 L 0.8607018518378027,0.0873470036312938 L 0.8606201058137231,0.0870919469743967 L 0.8604165873257443,0.0870966659858823 L 0.8602497618994676,0.0873635578900576 L 0.8600388888735324,0.0877370778471232 L 0.8603584127267823,0.0881392126902938 z\" /></g></svg>"
      ],
      "text/plain": [
       "<POLYGON ((0.86 0.088, 0.861 0.088, 0.861 0.088, 0.861 0.087, 0.861 0.087, 0...>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the CSV\n",
    "normalized_polygons = pd.read_csv('../data/normalized_polygons.csv')\n",
    "\n",
    "# Convert the list of WKT strings back to shapely Polygons\n",
    "normalized_polygons['polygons'] = normalized_polygons['polygons_wkt'].apply(lambda wkt_list: [shapely.wkt.loads(wkt_str) for wkt_str in eval(wkt_list)])\n",
    "normalized_polygons = normalized_polygons.drop('polygons_wkt', axis=1)\n",
    "display(normalized_polygons)\n",
    "test_poly = normalized_polygons['polygons'][0][0]\n",
    "display(test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_laz_dir = '../data/normalized_lazFiles/'\n",
    "output_laz_dir = '../data/downsampled_class_lazFiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [05:17<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "downsample_lidar(input_laz_dir=input_laz_dir, output_laz_dir=output_laz_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpulocal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
