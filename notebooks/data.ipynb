{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Hillfort detection with LiDAR data\n",
    "## Data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Table of contents\n",
    "\n",
    "[Code](#code)\n",
    "\n",
    "1. [**Defined functions**](#defined-functions)\n",
    "2. [**Data gathering**](#data-gathering)\n",
    "3. [**Data preprocessing**](#data-preprocessing)\n",
    "4. [**Data augmentation**](#data-augmentation)\n",
    "\n",
    "[End](#end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement zipfile (from versions: none)\n",
      "ERROR: No matching distribution found for zipfile\n"
     ]
    }
   ],
   "source": [
    "# Module installs\n",
    "!pip install zipfile pandas numpy shapely matplotlib laspy tqdm geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\teetegert\\appdata\\local\\anaconda3\\envs\\machine_learning_2\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\teetegert\\appdata\\local\\anaconda3\\envs\\machine_learning_2\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\teetegert\\appdata\\local\\anaconda3\\envs\\machine_learning_2\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\teetegert\\appdata\\local\\anaconda3\\envs\\machine_learning_2\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\teetegert\\appdata\\local\\anaconda3\\envs\\machine_learning_2\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\teetegert\\appdata\\local\\anaconda3\\envs\\machine_learning_2\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\teetegert\\appdata\\local\\anaconda3\\envs\\machine_learning_2\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\teetegert\\appdata\\local\\anaconda3\\envs\\machine_learning_2\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\teetegert\\appdata\\local\\anaconda3\\envs\\machine_learning_2\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\teetegert\\appdata\\local\\anaconda3\\envs\\machine_learning_2\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\teetegert\\appdata\\local\\anaconda3\\envs\\machine_learning_2\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\teetegert\\appdata\\local\\anaconda3\\envs\\machine_learning_2\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\teetegert\\appdata\\local\\anaconda3\\envs\\machine_learning_2\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\teetegert\\appdata\\local\\anaconda3\\envs\\machine_learning_2\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# If you have GPU\n",
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "# Otherwise\n",
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shapely # For optimized shape functions\n",
    "import matplotlib.pyplot as plt\n",
    "import laspy # Reading LAS file format\n",
    "from tqdm import tqdm # Loading bars\n",
    "import geopandas as gpd # Reading .shp files\n",
    "import lib.download_maps # Custom downloading script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Direct logs to the notebook's output\n",
    "    ]\n",
    ")\n",
    "\n",
    "GLOBAL_MIN_X, GLOBAL_MAX_X = 6374000, 6641000\n",
    "GLOBAL_MIN_Y, GLOBAL_MAX_Y = 364000, 742000\n",
    "GLOBAL_MIN_Z, GLOBAL_MAX_Z = -125, 320 # Munam√§gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lidar_and_polygons(hillfort_polygons_df, input_laz_dir, output_laz_dir, output_csv_filepath):\n",
    "    \"\"\"\n",
    "    Normalizes LiDAR data and corresponding polygons based on the unnormalized global bounding box.\n",
    "\n",
    "    Parameters:\n",
    "        hillfort_polygons_df (pd.DataFrame): DataFrame with 'laz_file' and 'polygons' columns.\n",
    "        input_laz_dir (str): Directory containing input LAS files.\n",
    "        output_laz_dir (str): Directory to save normalized LAS files.\n",
    "        output_csv_file (str): CSV file containing normalized output polygons.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_laz_dir):\n",
    "        os.makedirs(output_laz_dir)\n",
    "\n",
    "    normalized_polygons = []\n",
    "\n",
    "    for _, row in tqdm(hillfort_polygons_df.iterrows(), total=hillfort_polygons_df.shape[0]):\n",
    "        laz_file = row['laz_file']\n",
    "        polygons = row['polygons']\n",
    "\n",
    "        input_path = os.path.join(input_laz_dir, laz_file)\n",
    "        output_path = os.path.join(output_laz_dir, laz_file)\n",
    "\n",
    "        # Read LAS file\n",
    "        las = laspy.read(input_path)\n",
    "\n",
    "        # Normalize LiDAR data\n",
    "        # Normalize x, y, z\n",
    "        new_x = (las.x - GLOBAL_MIN_Y) / (GLOBAL_MAX_Y - GLOBAL_MIN_Y)\n",
    "        new_y = (las.y - GLOBAL_MIN_X) / (GLOBAL_MAX_X - GLOBAL_MIN_X)\n",
    "        new_z = (las.z - GLOBAL_MIN_Z) / (GLOBAL_MAX_Z - GLOBAL_MIN_Z)\n",
    "\n",
    "        # Update the header for new scale and offset\n",
    "        las.header.offsets = [0.0, 0.0, 0.0]  # Reset offsets to 0.0\n",
    "        las.header.scales = [1e-8, 1e-9, 1e-5]  # Small scale to preserve precision\n",
    "\n",
    "        # Assign normalized coordinates back\n",
    "        las.x = new_x\n",
    "        las.y = new_y\n",
    "        las.z = new_z\n",
    "\n",
    "        # Save normalized LAS file\n",
    "        las.write(output_path)\n",
    "\n",
    "        # Normalize polygons using the same bounding box\n",
    "        normalized = []\n",
    "        for polygon in polygons:\n",
    "            scaled = shapely.affinity.scale(\n",
    "                polygon,\n",
    "                xfact=1 / (GLOBAL_MAX_Y - GLOBAL_MIN_Y),\n",
    "                yfact=1 / (GLOBAL_MAX_X - GLOBAL_MIN_X),\n",
    "                origin=(GLOBAL_MIN_Y, GLOBAL_MIN_X)\n",
    "            )\n",
    "            normed = shapely.affinity.translate(scaled, xoff=-GLOBAL_MIN_Y, yoff=-GLOBAL_MIN_X)\n",
    "            normalized.append(normed)\n",
    "\n",
    "        # Append normalized polygons with laz_file reference\n",
    "        normalized_polygons.append({'laz_file': laz_file, 'polygons': normalized})\n",
    "\n",
    "    # Convert normalized polygons back to a DataFrame\n",
    "    normalized_polygons_df = pd.DataFrame(normalized_polygons)\n",
    "    \n",
    "    # Convert the list of polygons to a list of WKT strings\n",
    "    normalized_polygons_df['polygons_wkt'] = normalized_polygons_df['polygons'].apply(lambda polygons: [p.wkt for p in polygons])\n",
    "\n",
    "    # Save to CSV\n",
    "    normalized_polygons_df[['laz_file', 'polygons_wkt']].to_csv(output_csv_filepath, index=False)\n",
    "\n",
    "    print(f\"Normalized LAS files saved to: {output_laz_dir}\")\n",
    "    print(f\"Normalized polygons saved to: {output_csv_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lidar(input_laz_dir, output_laz_dir):\n",
    "    \"\"\"\n",
    "    Normalizes LiDAR data and corresponding polygons based on the unnormalized global bounding box.\n",
    "\n",
    "    Parameters:\n",
    "        input_laz_dir (str): Directory containing input LAS files.\n",
    "        output_laz_dir (str): Directory to save normalized LAS files.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(output_laz_dir):\n",
    "        os.makedirs(output_laz_dir)\n",
    "\n",
    "    laz_files = os.listdir(input_laz_dir)\n",
    "\n",
    "    for laz_file in tqdm(laz_files, total=len(laz_files)):\n",
    "\n",
    "        input_path = os.path.join(input_laz_dir, laz_file)\n",
    "        output_path = os.path.join(output_laz_dir, laz_file)\n",
    "\n",
    "        # Read LAS file\n",
    "        las = laspy.read(input_path)\n",
    "\n",
    "        # Normalize LiDAR data\n",
    "        # Normalize x, y, z\n",
    "        new_x = (las.x - GLOBAL_MIN_Y) / (GLOBAL_MAX_Y - GLOBAL_MIN_Y)\n",
    "        new_y = (las.y - GLOBAL_MIN_X) / (GLOBAL_MAX_X - GLOBAL_MIN_X)\n",
    "        new_z = (las.z - GLOBAL_MIN_Z) / (GLOBAL_MAX_Z - GLOBAL_MIN_Z)\n",
    "\n",
    "        # Update the header for new scale and offset\n",
    "        las.header.offsets = [0.0, 0.0, 0.0]  # Reset offsets to 0.0\n",
    "        las.header.scales = [1e-8, 1e-9, 1e-5]  # Small scale to preserve precision\n",
    "\n",
    "        # Assign normalized coordinates back\n",
    "        las.x = new_x\n",
    "        las.y = new_y\n",
    "        las.z = new_z\n",
    "\n",
    "        # Save normalized LAS file\n",
    "        las.write(output_path)\n",
    "\n",
    "    print(f\"Normalized LAS files saved to: {output_laz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_lidar(input_laz_dir, grid_size=1e-4, output_laz_dir='../data/downsampled_class_lazFiles'):\n",
    "    \"\"\"\n",
    "    Downsamples a LAS file using grid method and saves them to specified path.\n",
    "\n",
    "    Parameters:\n",
    "        input_laz_dir (str): Directory containing input LAS files.\n",
    "        grid_size (float): Size of grid cells for grid-based downsampling.\n",
    "        output_laz_dir (str): Directory to save normalized LAS files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_laz_dir):\n",
    "        os.makedirs(output_laz_dir)\n",
    "\n",
    "    laz_files = os.listdir(input_laz_dir)\n",
    "\n",
    "    for laz_file in tqdm(laz_files, total=len(laz_files)):\n",
    "\n",
    "        input_path = os.path.join(input_laz_dir, laz_file)\n",
    "        output_path = os.path.join(output_laz_dir, laz_file)\n",
    "\n",
    "        # Read LAS file\n",
    "        las = laspy.read(input_path)\n",
    "\n",
    "        # Calculate grid indices for each point\n",
    "        grid_x = np.floor(las.x / grid_size)\n",
    "        grid_y = np.floor(las.y / grid_size)\n",
    "\n",
    "        # Combine grid indices into tuples\n",
    "        grid_indices = list(zip(grid_x, grid_y))\n",
    "\n",
    "        # Group points by grid cell\n",
    "        grid_point_map = {}\n",
    "        for idx, grid in enumerate(grid_indices):\n",
    "            if grid not in grid_point_map:\n",
    "                grid_point_map[grid] = []\n",
    "            grid_point_map[grid].append(idx)\n",
    "\n",
    "        # Downsample by keeping a representative sample of points in each grid cell\n",
    "        sampled_indices = []\n",
    "        for points_in_cell in grid_point_map.values():\n",
    "            # Option 1: Keep a fixed percentage of points per cell\n",
    "            # sampled_indices.extend(random.sample(points_in_cell, max(1, len(points_in_cell) // 50)))\n",
    "\n",
    "            # Option 2: Keep a fixed number of points per cell (e.g., up to 5)\n",
    "            sampled_indices.extend(points_in_cell[:20])  # Adjust this value as needed\n",
    "\n",
    "        # Create the downsampled LAS file\n",
    "        downsampled_las = laspy.create(file_version=las.header.version, point_format=las.header.point_format)\n",
    "\n",
    "        # Copy header information (scale and offset) explicitly\n",
    "        downsampled_las.header.scales = las.header.scales\n",
    "        downsampled_las.header.offsets = las.header.offsets\n",
    "\n",
    "        # Copy downsampled point data\n",
    "        for dim in las.point_format.dimension_names:\n",
    "            setattr(downsampled_las, dim, getattr(las, dim)[sampled_indices])\n",
    "\n",
    "        # Save the downsampled LAS file\n",
    "        downsampled_las.write(output_path, do_compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_hillforts(laz_files_path='../data/lazFiles/', map_numbers_file='../data/linnamagede_ruudunumbrid.csv', hillfort_polygons_file='../data/inspire/PS_ProtectedSite_malestisedPolygon.shp'):\n",
    "    \"\"\"\n",
    "    Processes hillfort polygon data and associates it with LiDAR laz files.\n",
    "\n",
    "    This function reads hillfort polygon data and matches each polygon to the appropriate\n",
    "    LiDAR laz files based on square numbers. The resulting dataset links each laz file\n",
    "    to the polygons it intersects.\n",
    "\n",
    "    Parameters:\n",
    "        laz_files (list, optional): Path to laz files. If None, reads files from '../data/lazFiles/'.\n",
    "        map_numbers_file (str, optional): Path to the CSV file containing square numbers and hillfort IDs.\n",
    "        hillfort_polygons_file (str, optional): Path to the shapefile containing hillfort polygon geometries.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Optimized DataFrame with columns:\n",
    "            'laz_file': Name of the LiDAR laz file.\n",
    "            'polygons': List of shapely.Polygon objects associated with each laz file.\n",
    "    \"\"\"\n",
    "    # Load laz files\n",
    "    laz_files = os.listdir(laz_files_path)\n",
    "\n",
    "    # Read hillfort data file\n",
    "    gdf_inspire = gpd.read_file(hillfort_polygons_file)\n",
    "    # Read lidar map numbers file\n",
    "    map_numbers = pd.read_csv(map_numbers_file, sep=',')\n",
    "    # Remove rows if no map number or INSPIRE id\n",
    "    map_numbers = map_numbers.dropna(subset=['Ruudunumber', 'INSPIRE id'], how='any')\n",
    "\n",
    "    # Find hillfort polygons\n",
    "    hillfort_polygons = []\n",
    "    for _, row in map_numbers.iterrows():\n",
    "        polygon_series = gdf_inspire[gdf_inspire['inspireid_'] == row['INSPIRE id']]['geometry']\n",
    "        if polygon_series.empty:\n",
    "            logging.info(f\"No polygon found for hillfort: {row['Linnam√§gi']}\")\n",
    "            continue\n",
    "        # Add polygons as individual geometries\n",
    "        hillfort_polygons.append([row['Ruudunumber'], polygon_series.values.tolist()])\n",
    "    \n",
    "    # Convert hillfort polygons to DataFrame\n",
    "    hillfort_polygons_df = pd.DataFrame(hillfort_polygons, columns=['square_nr', 'polygon'])\n",
    "\n",
    "    # Match laz files based on square_nr\n",
    "    def match_laz_files(square_nr):\n",
    "        square_nums = [num.strip() for num in str(square_nr).split(\",\")]\n",
    "        matching_files = [file for file in laz_files if any(num in file for num in square_nums)]\n",
    "        return matching_files\n",
    "\n",
    "    # Add laz_files column to polygons DataFrame\n",
    "    hillfort_polygons_df['laz_files'] = hillfort_polygons_df['square_nr'].apply(match_laz_files)\n",
    "\n",
    "    # Normalize the DataFrame: explode laz_files\n",
    "    normalized_df = hillfort_polygons_df.explode('laz_files')[['laz_files', 'polygon']]\n",
    "\n",
    "    # Group by laz_file and aggregate polygons into a flat list\n",
    "    def flatten_polygon_lists(polygons):\n",
    "        # Flatten nested lists of polygons\n",
    "        return [geom for sublist in polygons for geom in sublist]\n",
    "\n",
    "    # Optimize: group by laz_file and aggregate polygons into a list\n",
    "    optimized_df = normalized_df.groupby('laz_files', as_index=False).agg({'polygon': flatten_polygon_lists})\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    optimized_df.rename(columns={'laz_files': 'laz_file', 'polygon': 'polygons'}, inplace=True)\n",
    "\n",
    "    return optimized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify LAS points' classification inside the polygon\n",
    "def classify_points_in_area(las, polygon, new_classification, output_path):\n",
    "    \"\"\"\n",
    "    Updates the classification of LAS file points within a given polygon.\n",
    "\n",
    "    Parameters:\n",
    "        las (str): Input LAS file.\n",
    "        polygon (shapely.Polygon): Polygon to classify points inside.\n",
    "        new_classification (int): New classification value for points within the polygon.\n",
    "        output_path (str): Path to save the modified LAS file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract LAS point X, Y coordinates\n",
    "    points = np.column_stack((las.x, las.y))\n",
    "\n",
    "    # Convert points to a Shapely GeometryArray (vectorized)\n",
    "    points_geom = shapely.points(points)\n",
    "\n",
    "    # Pre-filter points using the polygon's bounding box\n",
    "    bbox_mask = shapely.contains(shapely.box(*polygon.bounds), points_geom)\n",
    "\n",
    "    # Apply fine-grained point-in-polygon test to remaining points\n",
    "    final_mask = shapely.contains(polygon, points_geom[bbox_mask])\n",
    "\n",
    "    # Combine masks\n",
    "    mask = np.zeros(len(points), dtype=bool)\n",
    "    mask[bbox_mask] = final_mask\n",
    "\n",
    "    # Apply the new classification to points within the polygon\n",
    "    las.classification[mask] = new_classification\n",
    "\n",
    "    # Write modified points to a new LAS file\n",
    "    las.write(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify LAS points' classification inside the polygon\n",
    "def classify_points_in_multiple_areas(las, polygons, new_classification, output_path):\n",
    "    \"\"\"\n",
    "    Updates the classification of LAS file points within multiple polygons.\n",
    "\n",
    "    Parameters:\n",
    "        las (str): Input LAS file.\n",
    "        polygons (list): List of shapely.Polygon objects to classify points inside.\n",
    "        new_classification (int): New classification value for points within the polygons.\n",
    "        output_path (str): Path to save the modified LAS file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract LAS point X, Y coordinates\n",
    "    points = np.column_stack((las.x, las.y))\n",
    "\n",
    "    # Convert points to a Shapely GeometryArray (vectorized)\n",
    "    points_geom = shapely.points(points)\n",
    "\n",
    "    # Create a cumulative mask for all polygons\n",
    "    mask = np.zeros(len(points), dtype=bool)\n",
    "\n",
    "    for polygon in polygons:\n",
    "        # Pre-filter points using the polygon's bounding box\n",
    "        bbox_mask = shapely.contains(shapely.box(*polygon.bounds), points_geom)\n",
    "\n",
    "        # Apply fine-grained point-in-polygon test to remaining points\n",
    "        final_mask = shapely.contains(polygon, points_geom[bbox_mask])\n",
    "\n",
    "        # Combine masks\n",
    "        mask[bbox_mask] |= final_mask\n",
    "\n",
    "    # Apply the new classification to points within any polygon\n",
    "    las.classification[mask] = new_classification\n",
    "\n",
    "    # Write modified points to a new LAS file\n",
    "    las.write(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_polygons_in_las(input_laz_dir, output_laz_dir, polygons, new_classification):\n",
    "    \"\"\"Labels all the points in the given polygons with the new classification in each las file.\n",
    "\n",
    "    Parameters:\n",
    "        input_laz_dir (string): Path to the laz files directory\n",
    "        output_laz_dir (string): Path to the modified laz files directory\n",
    "        polygons (Polygon): Hillfort area polygons\n",
    "        new_classification (int): The new number to label the points with\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_laz_dir):\n",
    "        os.makedirs(output_laz_dir)\n",
    "\n",
    "    for _, row in tqdm(polygons.iterrows(), total=polygons.shape[0]):\n",
    "        laz_file = row['laz_file']\n",
    "        polygons = row['polygons']\n",
    "\n",
    "        # Paths\n",
    "        input_path = os.path.join(input_laz_dir, laz_file)\n",
    "        output_path = os.path.join(output_laz_dir, laz_file)\n",
    "\n",
    "        # Read the LAS file\n",
    "        las = laspy.read(input_path)\n",
    "\n",
    "        # Classify points within polygons\n",
    "        classify_points_in_multiple_areas(las, polygons, new_classification, output_path)\n",
    "\n",
    "        del las\n",
    "    print(f\"Processed files and saved to {output_laz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_laz_files(square_nr, laz_files):\n",
    "    \"\"\"For each unique laz file gathers all the polygons related to it\n",
    "\n",
    "    Parameters:\n",
    "        square_nr (string): Square numbers in the data file\n",
    "        laz_files (list): List of laz files to match polygons with\n",
    "\n",
    "    Returns:\n",
    "        list: Unique list of unique laz files with all the polygons related to them\n",
    "    \"\"\"\n",
    "    # Split the square_nr string into individual numbers\n",
    "    square_nums = [num.strip() for num in str(square_nr).split(\",\")]\n",
    "    # Check if any square number is part of the laz file name\n",
    "    matching_files = [file for file in laz_files if any(num in file for num in square_nums)]\n",
    "    return matching_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip zip file\n",
    "if not os.path.exists('../data/inspire/PS_ProtectedSite_malestisedPolygon.shp'):\n",
    "    with zipfile.ZipFile(os.path.join('../data/inspire/PS_ProtectedSite_malestised.zip'), 'r') as zip:\n",
    "        zip.extractall('../data/inspire/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "map_numbers = '../data/linnamagede_ruudunumbrid.csv'\n",
    "hillfort_polygons = '../data/inspire/PS_ProtectedSite_malestisedPolygon.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading lidar maps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "137it [1:20:18, 35.17s/it]\n",
      "2025-04-03 16:43:00,789 - INFO - Total files: 153\n"
     ]
    }
   ],
   "source": [
    "# Download lidar maps\n",
    "if not os.path.exists('../data/lazFiles/'):\n",
    "    os.makedirs('../data/lazFiles/', exist_ok=True)\n",
    "    print(\"Downloading lidar maps\")\n",
    "    lib.download_maps.process_csv(input_csv=map_numbers, output_dir='../data/lazFiles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_laz_dir = '../data/lazFiles/'\n",
    "output_laz_dir = '../data/classified_lazFiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>laz_file</th>\n",
       "      <th>polygons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((689215.4800090133 6397533.169809564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402642_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((642559.2100067963 6402702.98980967,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>402674_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((674706.6800083275 6402314.859809687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>407676_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((676672.7800084257 6407956.299809841...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>407700_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((700119.2900095425 6407732.169809859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>596650_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((650948.8500073353 6596229.389815392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>596651_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((650948.8500073353 6596229.389815392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>598586_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((586201.6800041909 6598158.139815415...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>600638_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((638810.0200067485 6600382.429815512...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((639654.4100067897 6600836.849815527...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 laz_file                                           polygons\n",
       "0    397689_2023_tava.laz  [POLYGON ((689215.4800090133 6397533.169809564...\n",
       "1    402642_2023_tava.laz  [POLYGON ((642559.2100067963 6402702.98980967,...\n",
       "2    402674_2023_tava.laz  [POLYGON ((674706.6800083275 6402314.859809687...\n",
       "3    407676_2023_tava.laz  [POLYGON ((676672.7800084257 6407956.299809841...\n",
       "4    407700_2023_tava.laz  [POLYGON ((700119.2900095425 6407732.169809859...\n",
       "..                    ...                                                ...\n",
       "126  596650_2022_tava.laz  [POLYGON ((650948.8500073353 6596229.389815392...\n",
       "127  596651_2022_tava.laz  [POLYGON ((650948.8500073353 6596229.389815392...\n",
       "128  598586_2022_tava.laz  [POLYGON ((586201.6800041909 6598158.139815415...\n",
       "129  600638_2022_tava.laz  [POLYGON ((638810.0200067485 6600382.429815512...\n",
       "130  600639_2022_tava.laz  [POLYGON ((639654.4100067897 6600836.849815527...\n",
       "\n",
       "[131 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gather laz files and their associated polygons\n",
    "polygons = gather_hillforts(\n",
    "    laz_files_path=input_laz_dir,\n",
    "    map_numbers_file=map_numbers, \n",
    "    hillfort_polygons_file=hillfort_polygons)\n",
    "display(polygons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/131 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "LaspyException",
     "evalue": "No LazBackend selected, cannot decompress data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaspyException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmark_polygons_in_las\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_laz_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_laz_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolygons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m, in \u001b[0;36mmark_polygons_in_las\u001b[1;34m(input_laz_dir, output_laz_dir, polygons, new_classification)\u001b[0m\n\u001b[0;32m     19\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_laz_dir, laz_file)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Read the LAS file\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m las \u001b[38;5;241m=\u001b[39m \u001b[43mlaspy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Classify points within polygons\u001b[39;00m\n\u001b[0;32m     25\u001b[0m classify_points_in_multiple_areas(las, polygons, new_classification, output_path)\n",
      "File \u001b[1;32mc:\\Users\\teetegert\\AppData\\Local\\anaconda3\\envs\\machine_learning_2\\Lib\\site-packages\\laspy\\lib.py:255\u001b[0m, in \u001b[0;36mread_las\u001b[1;34m(source, closefd, laz_backend, decompression_selection)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Entry point for reading las data in laspy\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03mReads the whole file into memory.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m    The ``decompression_selection`` parameter.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m open_las(\n\u001b[0;32m    250\u001b[0m     source,\n\u001b[0;32m    251\u001b[0m     closefd\u001b[38;5;241m=\u001b[39mclosefd,\n\u001b[0;32m    252\u001b[0m     laz_backend\u001b[38;5;241m=\u001b[39mlaz_backend,\n\u001b[0;32m    253\u001b[0m     decompression_selection\u001b[38;5;241m=\u001b[39mdecompression_selection,\n\u001b[0;32m    254\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\teetegert\\AppData\\Local\\anaconda3\\envs\\machine_learning_2\\Lib\\site-packages\\laspy\\lasreader.py:126\u001b[0m, in \u001b[0;36mLasReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LasData:\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    Reads all the points that are not read and returns a LasData object\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    This will also read EVLRS\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m     points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_points\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     las_data \u001b[38;5;241m=\u001b[39m LasData(header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader, points\u001b[38;5;241m=\u001b[39mpoints)\n\u001b[0;32m    129\u001b[0m     shall_read_evlr \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mminor \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mnumber_of_evlrs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevlrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\teetegert\\AppData\\Local\\anaconda3\\envs\\machine_learning_2\\Lib\\site-packages\\laspy\\lasreader.py:107\u001b[0m, in \u001b[0;36mLasReader.read_points\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n, points_left)\n\u001b[0;32m    106\u001b[0m r \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mPackedPointRecord\u001b[38;5;241m.\u001b[39mfrom_buffer(\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoint_source\u001b[49m\u001b[38;5;241m.\u001b[39mread_n_points(n), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mpoint_format\n\u001b[0;32m    108\u001b[0m )\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;241m<\u001b[39m n:\n\u001b[0;32m    110\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould only read \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(r)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of the requested \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m points\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\teetegert\\AppData\\Local\\anaconda3\\envs\\machine_learning_2\\Lib\\site-packages\\laspy\\lasreader.py:75\u001b[0m, in \u001b[0;36mLasReader.point_source\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpoint_source\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPointReader\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_point_source \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_point_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_point_source\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_point_source\n",
      "File \u001b[1;32mc:\\Users\\teetegert\\AppData\\Local\\anaconda3\\envs\\machine_learning_2\\Lib\\site-packages\\laspy\\lasreader.py:291\u001b[0m, in \u001b[0;36mLasReader._create_point_source\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mpoint_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mare_points_compressed:\n\u001b[1;32m--> 291\u001b[0m         point_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_laz_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m point_source \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    293\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mLaspyException(\n\u001b[0;32m    294\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData is compressed, but no LazBacked could be initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\teetegert\\AppData\\Local\\anaconda3\\envs\\machine_learning_2\\Lib\\site-packages\\laspy\\lasreader.py:259\u001b[0m, in \u001b[0;36mLasReader._create_laz_backend\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates the laz backend to use according to `self.laz_backend`.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03mIf `self.laz_backend` contains mutilple backends, this functions will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03mIf none could be constructed, the error of the last backend tried wil be raised\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaz_backend:\n\u001b[1;32m--> 259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mLaspyException(\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo LazBackend selected, cannot decompress data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    261\u001b[0m     )\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     backends \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaz_backend)\n",
      "\u001b[1;31mLaspyException\u001b[0m: No LazBackend selected, cannot decompress data"
     ]
    }
   ],
   "source": [
    "mark_polygons_in_las(input_laz_dir, output_laz_dir, polygons, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_laz_dir = '../data/classified_lazFiles/' \n",
    "output_laz_dir = '../data/normalized_lazFiles/'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 131/131 [04:11<00:00,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized LAS files saved to: ../data/normalized_lazFiles/\n",
      "Normalized polygons saved to: ../data/normalized_polygons.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "normalize_lidar_and_polygons(polygons, input_laz_dir, output_laz_dir, '../data/normalized_polygons.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>laz_file</th>\n",
       "      <th>polygons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.8603584127267823 0.08813921269029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402642_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.7369291270151734 0.10750183463096...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>402674_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.8219753439188935 0.10604816488921...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>407676_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.8271766667021438 0.12717715278267...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>407700_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.8892044709064066 0.12633771542459...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>596650_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.7591239418252371 0.83231981284916...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>596651_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.7591239418252371 0.83231981284916...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>598586_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.5878351322608069 0.83954359497874...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>600638_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.727010634902399 0.847874268889427...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.7292444708873518 0.84957621712237...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 laz_file                                           polygons\n",
       "0    397689_2023_tava.laz  [POLYGON ((0.8603584127267823 0.08813921269029...\n",
       "1    402642_2023_tava.laz  [POLYGON ((0.7369291270151734 0.10750183463096...\n",
       "2    402674_2023_tava.laz  [POLYGON ((0.8219753439188935 0.10604816488921...\n",
       "3    407676_2023_tava.laz  [POLYGON ((0.8271766667021438 0.12717715278267...\n",
       "4    407700_2023_tava.laz  [POLYGON ((0.8892044709064066 0.12633771542459...\n",
       "..                    ...                                                ...\n",
       "126  596650_2022_tava.laz  [POLYGON ((0.7591239418252371 0.83231981284916...\n",
       "127  596651_2022_tava.laz  [POLYGON ((0.7591239418252371 0.83231981284916...\n",
       "128  598586_2022_tava.laz  [POLYGON ((0.5878351322608069 0.83954359497874...\n",
       "129  600638_2022_tava.laz  [POLYGON ((0.727010634902399 0.847874268889427...\n",
       "130  600639_2022_tava.laz  [POLYGON ((0.7292444708873518 0.84957621712237...\n",
       "\n",
       "[131 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"0.8599969982448965 0.08705005634576082 0.0007467442215420839 0.0011310469731688616\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,0.1752311596646905)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"2.2620939463377233e-05\" opacity=\"0.6\" d=\"M 0.8603584127267823,0.0881392126902938 L 0.8606247089919634,0.0879507111385465 L 0.8606968518579379,0.0877036321908236 L 0.8607018518378027,0.0873470036312938 L 0.8606201058137231,0.0870919469743967 L 0.8604165873257443,0.0870966659858823 L 0.8602497618994676,0.0873635578900576 L 0.8600388888735324,0.0877370778471232 L 0.8603584127267823,0.0881392126902938 z\" /></g></svg>"
      ],
      "text/plain": [
       "<POLYGON ((0.86 0.088, 0.861 0.088, 0.861 0.088, 0.861 0.087, 0.861 0.087, 0...>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the CSV\n",
    "normalized_polygons = pd.read_csv('../data/normalized_polygons.csv')\n",
    "\n",
    "# Convert the list of WKT strings back to shapely Polygons\n",
    "normalized_polygons['polygons'] = normalized_polygons['polygons_wkt'].apply(lambda wkt_list: [shapely.wkt.loads(wkt_str) for wkt_str in eval(wkt_list)])\n",
    "normalized_polygons = normalized_polygons.drop('polygons_wkt', axis=1)\n",
    "display(normalized_polygons)\n",
    "test_poly = normalized_polygons['polygons'][0][0]\n",
    "display(test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_laz_dir = '../data/normalized_lazFiles/'\n",
    "output_laz_dir = '../data/downsampled_class_lazFiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 131/131 [07:19<00:00,  3.36s/it]\n"
     ]
    }
   ],
   "source": [
    "downsample_lidar(input_laz_dir=input_laz_dir, output_laz_dir=output_laz_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_las = os.listdir('../data/downsampled_class_lazFiles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_las = []\n",
    "for las_file in all_las:\n",
    "    las = laspy.read(os.path.join('../data/downsampled_class_lazFiles', las_file))\n",
    "    las_xyz = las.xyz\n",
    "    las_x = las_xyz[:, 0]\n",
    "    las_y = las_xyz[:, 1]\n",
    "    las_z = las_xyz[:, 2]\n",
    "    las_r = las.points.array['red'] / 65535 * 255\n",
    "    las_g = las.points.array['green'] / 65535 * 255\n",
    "    las_b = las.points.array['blue'] / 65535 * 255\n",
    "    # Create a DataFrame for the current LAS file\n",
    "    las_df = pd.DataFrame({\n",
    "        'x': las_x,\n",
    "        'y': las_y,\n",
    "        'z': las_z,\n",
    "        'r': las_r,\n",
    "        'g': las_g,\n",
    "        'b': las_b,\n",
    "        'file': las_file\n",
    "    })\n",
    "\n",
    "    # Append to the list of DataFrames\n",
    "    data_las.append(las_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>r</th>\n",
       "      <th>g</th>\n",
       "      <th>b</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.860053</td>\n",
       "      <td>0.086148</td>\n",
       "      <td>0.76088</td>\n",
       "      <td>84.669261</td>\n",
       "      <td>119.533074</td>\n",
       "      <td>66.739300</td>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.860053</td>\n",
       "      <td>0.086149</td>\n",
       "      <td>0.75153</td>\n",
       "      <td>70.723735</td>\n",
       "      <td>104.591440</td>\n",
       "      <td>51.797665</td>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.860053</td>\n",
       "      <td>0.086150</td>\n",
       "      <td>0.74117</td>\n",
       "      <td>59.766537</td>\n",
       "      <td>74.708171</td>\n",
       "      <td>42.832685</td>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.860052</td>\n",
       "      <td>0.086146</td>\n",
       "      <td>0.75569</td>\n",
       "      <td>119.533074</td>\n",
       "      <td>149.416342</td>\n",
       "      <td>74.708171</td>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.860052</td>\n",
       "      <td>0.086148</td>\n",
       "      <td>0.72836</td>\n",
       "      <td>84.669261</td>\n",
       "      <td>119.533074</td>\n",
       "      <td>66.739300</td>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758508</th>\n",
       "      <td>0.729998</td>\n",
       "      <td>0.850114</td>\n",
       "      <td>0.31901</td>\n",
       "      <td>102.599222</td>\n",
       "      <td>97.618677</td>\n",
       "      <td>77.696498</td>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758509</th>\n",
       "      <td>0.729998</td>\n",
       "      <td>0.850113</td>\n",
       "      <td>0.32933</td>\n",
       "      <td>156.389105</td>\n",
       "      <td>154.396887</td>\n",
       "      <td>133.478599</td>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758510</th>\n",
       "      <td>0.729997</td>\n",
       "      <td>0.850106</td>\n",
       "      <td>0.29492</td>\n",
       "      <td>137.463035</td>\n",
       "      <td>140.451362</td>\n",
       "      <td>113.556420</td>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758511</th>\n",
       "      <td>0.729998</td>\n",
       "      <td>0.850114</td>\n",
       "      <td>0.33418</td>\n",
       "      <td>102.599222</td>\n",
       "      <td>97.618677</td>\n",
       "      <td>77.696498</td>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758512</th>\n",
       "      <td>0.729998</td>\n",
       "      <td>0.850111</td>\n",
       "      <td>0.32022</td>\n",
       "      <td>121.525292</td>\n",
       "      <td>123.517510</td>\n",
       "      <td>99.610895</td>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2758513 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                x         y        z           r           g           b  \\\n",
       "0        0.860053  0.086148  0.76088   84.669261  119.533074   66.739300   \n",
       "1        0.860053  0.086149  0.75153   70.723735  104.591440   51.797665   \n",
       "2        0.860053  0.086150  0.74117   59.766537   74.708171   42.832685   \n",
       "3        0.860052  0.086146  0.75569  119.533074  149.416342   74.708171   \n",
       "4        0.860052  0.086148  0.72836   84.669261  119.533074   66.739300   \n",
       "...           ...       ...      ...         ...         ...         ...   \n",
       "2758508  0.729998  0.850114  0.31901  102.599222   97.618677   77.696498   \n",
       "2758509  0.729998  0.850113  0.32933  156.389105  154.396887  133.478599   \n",
       "2758510  0.729997  0.850106  0.29492  137.463035  140.451362  113.556420   \n",
       "2758511  0.729998  0.850114  0.33418  102.599222   97.618677   77.696498   \n",
       "2758512  0.729998  0.850111  0.32022  121.525292  123.517510   99.610895   \n",
       "\n",
       "                         file  \n",
       "0        397689_2023_tava.laz  \n",
       "1        397689_2023_tava.laz  \n",
       "2        397689_2023_tava.laz  \n",
       "3        397689_2023_tava.laz  \n",
       "4        397689_2023_tava.laz  \n",
       "...                       ...  \n",
       "2758508  600639_2022_tava.laz  \n",
       "2758509  600639_2022_tava.laz  \n",
       "2758510  600639_2022_tava.laz  \n",
       "2758511  600639_2022_tava.laz  \n",
       "2758512  600639_2022_tava.laz  \n",
       "\n",
       "[2758513 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Concatenate all DataFrames into one (if processing multiple files)\n",
    "final_df = pd.concat(data_las, ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(final_df)\n",
    "\n",
    "final_df.to_parquet('../data/downsampled_points.parquet', engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
