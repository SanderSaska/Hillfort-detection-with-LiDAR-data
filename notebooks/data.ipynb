{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Hillfort detection with LiDAR data\n",
    "## Data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Table of contents\n",
    "\n",
    "[Code](#code)\n",
    "\n",
    "1. [**Defined functions**](#defined-functions)\n",
    "2. [**Data gathering**](#data-gathering)\n",
    "3. [**Data preprocessing**](#data-preprocessing)\n",
    "4. [**Data augmentation**](#data-augmentation)\n",
    "\n",
    "[End](#end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module installs\n",
    "!pip install zipfile pandas numpy shapely matplotlib laspy tqdm geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have GPU\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "# Otherwise\n",
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shapely # For optimized shape functions\n",
    "import matplotlib.pyplot as plt\n",
    "import laspy # Reading LAS file format\n",
    "from tqdm import tqdm # Loading bars\n",
    "import geopandas as gpd # Reading .shp files\n",
    "import lib.download_maps # Custom downloading script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Direct logs to the notebook's output\n",
    "    ]\n",
    ")\n",
    "\n",
    "GLOBAL_MIN_X, GLOBAL_MAX_X = 6374000, 6641000\n",
    "GLOBAL_MIN_Y, GLOBAL_MAX_Y = 364000, 742000\n",
    "GLOBAL_MIN_Z, GLOBAL_MAX_Z = -125, 320 # Munam√§gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lidar_and_polygons(hillfort_polygons_df, input_laz_dir, output_laz_dir, output_csv_filepath):\n",
    "    \"\"\"\n",
    "    Normalizes LiDAR data and corresponding polygons based on the unnormalized global bounding box.\n",
    "\n",
    "    Parameters:\n",
    "        hillfort_polygons_df (pd.DataFrame): DataFrame with 'laz_file' and 'polygons' columns.\n",
    "        input_laz_dir (str): Directory containing input LAS files.\n",
    "        output_laz_dir (str): Directory to save normalized LAS files.\n",
    "        output_csv_file (str): CSV file containing normalized output polygons.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_laz_dir):\n",
    "        os.makedirs(output_laz_dir)\n",
    "\n",
    "    normalized_polygons = []\n",
    "\n",
    "    for _, row in tqdm(hillfort_polygons_df.iterrows(), total=hillfort_polygons_df.shape[0]):\n",
    "        laz_file = row['laz_file']\n",
    "        polygons = row['polygons']\n",
    "\n",
    "        input_path = os.path.join(input_laz_dir, laz_file)\n",
    "        output_path = os.path.join(output_laz_dir, laz_file)\n",
    "\n",
    "        # Read LAS file\n",
    "        las = laspy.read(input_path)\n",
    "\n",
    "        # Normalize LiDAR data\n",
    "        # Normalize x, y, z\n",
    "        new_x = (las.x - GLOBAL_MIN_Y) / (GLOBAL_MAX_Y - GLOBAL_MIN_Y)\n",
    "        new_y = (las.y - GLOBAL_MIN_X) / (GLOBAL_MAX_X - GLOBAL_MIN_X)\n",
    "        new_z = (las.z - GLOBAL_MIN_Z) / (GLOBAL_MAX_Z - GLOBAL_MIN_Z)\n",
    "\n",
    "        # Update the header for new scale and offset\n",
    "        las.header.offsets = [0.0, 0.0, 0.0]  # Reset offsets to 0.0\n",
    "        las.header.scales = [1e-8, 1e-9, 1e-5]  # Small scale to preserve precision\n",
    "\n",
    "        # Assign normalized coordinates back\n",
    "        las.x = new_x\n",
    "        las.y = new_y\n",
    "        las.z = new_z\n",
    "\n",
    "        # Save normalized LAS file\n",
    "        las.write(output_path)\n",
    "\n",
    "        # Normalize polygons using the same bounding box\n",
    "        normalized = []\n",
    "        for polygon in polygons:\n",
    "            scaled = shapely.affinity.scale(\n",
    "                polygon,\n",
    "                xfact=1 / (GLOBAL_MAX_Y - GLOBAL_MIN_Y),\n",
    "                yfact=1 / (GLOBAL_MAX_X - GLOBAL_MIN_X),\n",
    "                origin=(GLOBAL_MIN_Y, GLOBAL_MIN_X)\n",
    "            )\n",
    "            normed = shapely.affinity.translate(scaled, xoff=-GLOBAL_MIN_Y, yoff=-GLOBAL_MIN_X)\n",
    "            normalized.append(normed)\n",
    "\n",
    "        # Append normalized polygons with laz_file reference\n",
    "        normalized_polygons.append({'laz_file': laz_file, 'polygons': normalized})\n",
    "\n",
    "    # Convert normalized polygons back to a DataFrame\n",
    "    normalized_polygons_df = pd.DataFrame(normalized_polygons)\n",
    "    \n",
    "    # Convert the list of polygons to a list of WKT strings\n",
    "    normalized_polygons_df['polygons_wkt'] = normalized_polygons_df['polygons'].apply(lambda polygons: [p.wkt for p in polygons])\n",
    "\n",
    "    # Save to CSV\n",
    "    normalized_polygons_df[['laz_file', 'polygons_wkt']].to_csv(output_csv_filepath, index=False)\n",
    "\n",
    "    print(f\"Normalized LAS files saved to: {output_laz_dir}\")\n",
    "    print(f\"Normalized polygons saved to: {output_csv_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lidar(input_laz_dir, output_laz_dir):\n",
    "    \"\"\"\n",
    "    Normalizes LiDAR data and corresponding polygons based on the unnormalized global bounding box.\n",
    "\n",
    "    Parameters:\n",
    "        input_laz_dir (str): Directory containing input LAS files.\n",
    "        output_laz_dir (str): Directory to save normalized LAS files.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(output_laz_dir):\n",
    "        os.makedirs(output_laz_dir)\n",
    "\n",
    "    laz_files = os.listdir(input_laz_dir)\n",
    "\n",
    "    for laz_file in tqdm(laz_files, total=len(laz_files)):\n",
    "\n",
    "        input_path = os.path.join(input_laz_dir, laz_file)\n",
    "        output_path = os.path.join(output_laz_dir, laz_file)\n",
    "\n",
    "        # Read LAS file\n",
    "        las = laspy.read(input_path)\n",
    "\n",
    "        # Normalize LiDAR data\n",
    "        # Normalize x, y, z\n",
    "        new_x = (las.x - GLOBAL_MIN_Y) / (GLOBAL_MAX_Y - GLOBAL_MIN_Y)\n",
    "        new_y = (las.y - GLOBAL_MIN_X) / (GLOBAL_MAX_X - GLOBAL_MIN_X)\n",
    "        new_z = (las.z - GLOBAL_MIN_Z) / (GLOBAL_MAX_Z - GLOBAL_MIN_Z)\n",
    "\n",
    "        # Update the header for new scale and offset\n",
    "        las.header.offsets = [0.0, 0.0, 0.0]  # Reset offsets to 0.0\n",
    "        las.header.scales = [1e-8, 1e-9, 1e-5]  # Small scale to preserve precision\n",
    "\n",
    "        # Assign normalized coordinates back\n",
    "        las.x = new_x\n",
    "        las.y = new_y\n",
    "        las.z = new_z\n",
    "\n",
    "        # Save normalized LAS file\n",
    "        las.write(output_path)\n",
    "\n",
    "    print(f\"Normalized LAS files saved to: {output_laz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_lidar(input_laz_dir, grid_size=1e-4, output_laz_dir='../data/downsampled_class_lazFiles'):\n",
    "    \"\"\"\n",
    "    Downsamples a LAS file using grid method and saves them to specified path.\n",
    "\n",
    "    Parameters:\n",
    "        input_laz_dir (str): Directory containing input LAS files.\n",
    "        grid_size (float): Size of grid cells for grid-based downsampling.\n",
    "        output_laz_dir (str): Directory to save normalized LAS files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_laz_dir):\n",
    "        os.makedirs(output_laz_dir)\n",
    "\n",
    "    laz_files = os.listdir(input_laz_dir)\n",
    "\n",
    "    for laz_file in tqdm(laz_files, total=len(laz_files)):\n",
    "\n",
    "        input_path = os.path.join(input_laz_dir, laz_file)\n",
    "        output_path = os.path.join(output_laz_dir, laz_file)\n",
    "\n",
    "        # Read LAS file\n",
    "        las = laspy.read(input_path)\n",
    "\n",
    "        # Calculate grid indices for each point\n",
    "        grid_x = np.floor(las.x / grid_size)\n",
    "        grid_y = np.floor(las.y / grid_size)\n",
    "\n",
    "        # Combine grid indices into tuples\n",
    "        grid_indices = list(zip(grid_x, grid_y))\n",
    "\n",
    "        # Group points by grid cell\n",
    "        grid_point_map = {}\n",
    "        for idx, grid in enumerate(grid_indices):\n",
    "            if grid not in grid_point_map:\n",
    "                grid_point_map[grid] = []\n",
    "            grid_point_map[grid].append(idx)\n",
    "\n",
    "        # Downsample by keeping a representative sample of points in each grid cell\n",
    "        sampled_indices = []\n",
    "        for points_in_cell in grid_point_map.values():\n",
    "            # Option 1: Keep a fixed percentage of points per cell\n",
    "            sampled_indices.extend(random.sample(points_in_cell, max(1, len(points_in_cell) // 50)))\n",
    "\n",
    "            # Option 2: Keep a fixed number of points per cell (e.g., up to 5)\n",
    "            # sampled_indices.extend(points_in_cell[:5])  # Adjust this value as needed\n",
    "\n",
    "        # Create the downsampled LAS file\n",
    "        downsampled_las = laspy.create(file_version=las.header.version, point_format=las.header.point_format)\n",
    "\n",
    "        # Copy header information (scale and offset) explicitly\n",
    "        downsampled_las.header.scales = las.header.scales\n",
    "        downsampled_las.header.offsets = las.header.offsets\n",
    "\n",
    "        # Copy downsampled point data\n",
    "        for dim in las.point_format.dimension_names:\n",
    "            setattr(downsampled_las, dim, getattr(las, dim)[sampled_indices])\n",
    "\n",
    "        # Save the downsampled LAS file\n",
    "        downsampled_las.write(output_path, do_compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_hillforts(laz_files_path='../data/lazFiles/', map_numbers_file='../data/linnamagede_ruudunumbrid.csv', hillfort_polygons_file='../data/inspire/PS_ProtectedSite_malestisedPolygon.shp'):\n",
    "    \"\"\"\n",
    "    Processes hillfort polygon data and associates it with LiDAR laz files.\n",
    "\n",
    "    This function reads hillfort polygon data and matches each polygon to the appropriate\n",
    "    LiDAR laz files based on square numbers. The resulting dataset links each laz file\n",
    "    to the polygons it intersects.\n",
    "\n",
    "    Parameters:\n",
    "        laz_files (list, optional): Path to laz files. If None, reads files from '../data/lazFiles/'.\n",
    "        map_numbers_file (str, optional): Path to the CSV file containing square numbers and hillfort IDs.\n",
    "        hillfort_polygons_file (str, optional): Path to the shapefile containing hillfort polygon geometries.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Optimized DataFrame with columns:\n",
    "            'laz_file': Name of the LiDAR laz file.\n",
    "            'polygons': List of shapely.Polygon objects associated with each laz file.\n",
    "    \"\"\"\n",
    "    # Load laz files\n",
    "    laz_files = os.listdir(laz_files_path)\n",
    "\n",
    "    # Read hillfort data file\n",
    "    gdf_inspire = gpd.read_file(hillfort_polygons_file)\n",
    "    # Read lidar map numbers file\n",
    "    map_numbers = pd.read_csv(map_numbers_file, sep=',')\n",
    "    # Remove rows if no map number or INSPIRE id\n",
    "    map_numbers = map_numbers.dropna(subset=['Ruudunumber', 'INSPIRE id'], how='any')\n",
    "\n",
    "    # Find hillfort polygons\n",
    "    hillfort_polygons = []\n",
    "    for _, row in map_numbers.iterrows():\n",
    "        polygon_series = gdf_inspire[gdf_inspire['inspireid_'] == row['INSPIRE id']]['geometry']\n",
    "        if polygon_series.empty:\n",
    "            logging.info(f\"No polygon found for hillfort: {row['Linnam√§gi']}\")\n",
    "            continue\n",
    "        # Add polygons as individual geometries\n",
    "        hillfort_polygons.append([row['Ruudunumber'], polygon_series.values.tolist()])\n",
    "    \n",
    "    # Convert hillfort polygons to DataFrame\n",
    "    hillfort_polygons_df = pd.DataFrame(hillfort_polygons, columns=['square_nr', 'polygon'])\n",
    "\n",
    "    # Match laz files based on square_nr\n",
    "    def match_laz_files(square_nr):\n",
    "        square_nums = [num.strip() for num in str(square_nr).split(\",\")]\n",
    "        matching_files = [file for file in laz_files if any(num in file for num in square_nums)]\n",
    "        return matching_files\n",
    "\n",
    "    # Add laz_files column to polygons DataFrame\n",
    "    hillfort_polygons_df['laz_files'] = hillfort_polygons_df['square_nr'].apply(match_laz_files)\n",
    "\n",
    "    # Normalize the DataFrame: explode laz_files\n",
    "    normalized_df = hillfort_polygons_df.explode('laz_files')[['laz_files', 'polygon']]\n",
    "\n",
    "    # Group by laz_file and aggregate polygons into a flat list\n",
    "    def flatten_polygon_lists(polygons):\n",
    "        # Flatten nested lists of polygons\n",
    "        return [geom for sublist in polygons for geom in sublist]\n",
    "\n",
    "    # Optimize: group by laz_file and aggregate polygons into a list\n",
    "    optimized_df = normalized_df.groupby('laz_files', as_index=False).agg({'polygon': flatten_polygon_lists})\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    optimized_df.rename(columns={'laz_files': 'laz_file', 'polygon': 'polygons'}, inplace=True)\n",
    "\n",
    "    return optimized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify LAS points' classification inside the polygon\n",
    "def classify_points_in_area(las, polygon, new_classification, output_path):\n",
    "    \"\"\"\n",
    "    Updates the classification of LAS file points within a given polygon.\n",
    "\n",
    "    Parameters:\n",
    "        las (str): Input LAS file.\n",
    "        polygon (shapely.Polygon): Polygon to classify points inside.\n",
    "        new_classification (int): New classification value for points within the polygon.\n",
    "        output_path (str): Path to save the modified LAS file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract LAS point X, Y coordinates\n",
    "    points = np.column_stack((las.x, las.y))\n",
    "\n",
    "    # Convert points to a Shapely GeometryArray (vectorized)\n",
    "    points_geom = shapely.points(points)\n",
    "\n",
    "    # Pre-filter points using the polygon's bounding box\n",
    "    bbox_mask = shapely.contains(shapely.box(*polygon.bounds), points_geom)\n",
    "\n",
    "    # Apply fine-grained point-in-polygon test to remaining points\n",
    "    final_mask = shapely.contains(polygon, points_geom[bbox_mask])\n",
    "\n",
    "    # Combine masks\n",
    "    mask = np.zeros(len(points), dtype=bool)\n",
    "    mask[bbox_mask] = final_mask\n",
    "\n",
    "    # Apply the new classification to points within the polygon\n",
    "    las.classification[mask] = new_classification\n",
    "\n",
    "    # Write modified points to a new LAS file\n",
    "    las.write(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify LAS points' classification inside the polygon\n",
    "def classify_points_in_multiple_areas(las, polygons, new_classification, output_path):\n",
    "    \"\"\"\n",
    "    Updates the classification of LAS file points within multiple polygons.\n",
    "\n",
    "    Parameters:\n",
    "        las (str): Input LAS file.\n",
    "        polygons (list): List of shapely.Polygon objects to classify points inside.\n",
    "        new_classification (int): New classification value for points within the polygons.\n",
    "        output_path (str): Path to save the modified LAS file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract LAS point X, Y coordinates\n",
    "    points = np.column_stack((las.x, las.y))\n",
    "\n",
    "    # Convert points to a Shapely GeometryArray (vectorized)\n",
    "    points_geom = shapely.points(points)\n",
    "\n",
    "    # Create a cumulative mask for all polygons\n",
    "    mask = np.zeros(len(points), dtype=bool)\n",
    "\n",
    "    for polygon in polygons:\n",
    "        # Pre-filter points using the polygon's bounding box\n",
    "        bbox_mask = shapely.contains(shapely.box(*polygon.bounds), points_geom)\n",
    "\n",
    "        # Apply fine-grained point-in-polygon test to remaining points\n",
    "        final_mask = shapely.contains(polygon, points_geom[bbox_mask])\n",
    "\n",
    "        # Combine masks\n",
    "        mask[bbox_mask] |= final_mask\n",
    "\n",
    "    # Apply the new classification to points within any polygon\n",
    "    las.classification[mask] = new_classification\n",
    "\n",
    "    # Write modified points to a new LAS file\n",
    "    las.write(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_polygons_in_las(input_laz_dir, output_laz_dir, polygons, new_classification):\n",
    "    \"\"\"Labels all the points in the given polygons with the new classification in each las file.\n",
    "\n",
    "    Parameters:\n",
    "        input_laz_dir (string): Path to the laz files directory\n",
    "        output_laz_dir (string): Path to the modified laz files directory\n",
    "        polygons (Polygon): Hillfort area polygons\n",
    "        new_classification (int): The new number to label the points with\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_laz_dir):\n",
    "        os.makedirs(output_laz_dir)\n",
    "\n",
    "    for _, row in tqdm(polygons.iterrows(), total=polygons.shape[0]):\n",
    "        laz_file = row['laz_file']\n",
    "        polygons = row['polygons']\n",
    "\n",
    "        # Paths\n",
    "        input_path = os.path.join(input_laz_dir, laz_file)\n",
    "        output_path = os.path.join(output_laz_dir, laz_file)\n",
    "\n",
    "        # Read the LAS file\n",
    "        las = laspy.read(input_path)\n",
    "\n",
    "        # Classify points within polygons\n",
    "        classify_points_in_multiple_areas(las, polygons, new_classification, output_path)\n",
    "\n",
    "        del las\n",
    "    print(f\"Processed files and saved to {output_laz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_laz_files(square_nr, laz_files):\n",
    "    \"\"\"For each unique laz file gathers all the polygons related to it\n",
    "\n",
    "    Parameters:\n",
    "        square_nr (string): Square numbers in the data file\n",
    "        laz_files (list): List of laz files to match polygons with\n",
    "\n",
    "    Returns:\n",
    "        list: Unique list of unique laz files with all the polygons related to them\n",
    "    \"\"\"\n",
    "    # Split the square_nr string into individual numbers\n",
    "    square_nums = [num.strip() for num in str(square_nr).split(\",\")]\n",
    "    # Check if any square number is part of the laz file name\n",
    "    matching_files = [file for file in laz_files if any(num in file for num in square_nums)]\n",
    "    return matching_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip zip file\n",
    "if not os.path.exists('../data/inspire/PS_ProtectedSite_malestisedPolygon.shp'):\n",
    "    with zipfile.ZipFile(os.path.join('../data/inspire/PS_ProtectedSite_malestised.zip'), 'r') as zip:\n",
    "        zip.extractall('../data/inspire/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "map_numbers = '../data/linnamagede_ruudunumbrid.csv'\n",
    "hillfort_polygons = '../data/inspire/PS_ProtectedSite_malestisedPolygon.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download lidar maps\n",
    "if not os.path.exists('../data/lazFiles/'):\n",
    "    os.makedirs('../data/lazFiles/', exist_ok=True)\n",
    "    print(\"Downloading lidar maps\")\n",
    "    lib.download_maps.process_csv(input_csv=map_numbers, output_dir='../data/lazFiles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_laz_dir = '../data/lazFiles/'\n",
    "output_laz_dir = '../data/classified_lazFiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>laz_file</th>\n",
       "      <th>polygons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((689215.4800090133 6397533.169809564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402642_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((642559.2100067963 6402702.98980967,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>402674_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((674706.6800083275 6402314.859809687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>407676_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((676672.7800084257 6407956.299809841...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>407700_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((700119.2900095425 6407732.169809859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>596650_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((650948.8500073353 6596229.389815392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>596651_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((650948.8500073353 6596229.389815392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>598586_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((586201.6800041909 6598158.139815415...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>600638_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((638810.0200067485 6600382.429815512...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((639654.4100067897 6600836.849815527...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 laz_file                                           polygons\n",
       "0    397689_2023_tava.laz  [POLYGON ((689215.4800090133 6397533.169809564...\n",
       "1    402642_2023_tava.laz  [POLYGON ((642559.2100067963 6402702.98980967,...\n",
       "2    402674_2023_tava.laz  [POLYGON ((674706.6800083275 6402314.859809687...\n",
       "3    407676_2023_tava.laz  [POLYGON ((676672.7800084257 6407956.299809841...\n",
       "4    407700_2023_tava.laz  [POLYGON ((700119.2900095425 6407732.169809859...\n",
       "..                    ...                                                ...\n",
       "126  596650_2022_tava.laz  [POLYGON ((650948.8500073353 6596229.389815392...\n",
       "127  596651_2022_tava.laz  [POLYGON ((650948.8500073353 6596229.389815392...\n",
       "128  598586_2022_tava.laz  [POLYGON ((586201.6800041909 6598158.139815415...\n",
       "129  600638_2022_tava.laz  [POLYGON ((638810.0200067485 6600382.429815512...\n",
       "130  600639_2022_tava.laz  [POLYGON ((639654.4100067897 6600836.849815527...\n",
       "\n",
       "[131 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gather laz files and their associated polygons\n",
    "polygons = gather_hillforts(\n",
    "    laz_files_path=input_laz_dir,\n",
    "    map_numbers_file=map_numbers, \n",
    "    hillfort_polygons_file=hillfort_polygons)\n",
    "display(polygons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 131/131 [07:10<00:00,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed files and saved to ../data/classified_lazFiles/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mark_polygons_in_las(input_laz_dir, output_laz_dir, polygons, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_laz_dir = '../data/test/'\n",
    "output_laz_dir = '../data/normalized_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 131/131 [02:20<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized LAS files saved to: ../data/normalized_lazFiles/\n",
      "Normalized polygons saved to: ../data/normalized_polygons.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "normalize_lidar_and_polygons(polygons, input_laz_dir, output_laz_dir, '../data/normalized_polygons.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>laz_file</th>\n",
       "      <th>polygons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.8603584127267823 0.08813921269029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402642_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.7369291270151734 0.10750183463096...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>402674_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.8219753439188935 0.10604816488921...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>407676_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.8271766667021438 0.12717715278267...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>407700_2023_tava.laz</td>\n",
       "      <td>[POLYGON ((0.8892044709064066 0.12633771542459...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>596650_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.7591239418252371 0.83231981284916...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>596651_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.7591239418252371 0.83231981284916...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>598586_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.5878351322608069 0.83954359497874...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>600638_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.727010634902399 0.847874268889427...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "      <td>[POLYGON ((0.7292444708873518 0.84957621712237...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 laz_file                                           polygons\n",
       "0    397689_2023_tava.laz  [POLYGON ((0.8603584127267823 0.08813921269029...\n",
       "1    402642_2023_tava.laz  [POLYGON ((0.7369291270151734 0.10750183463096...\n",
       "2    402674_2023_tava.laz  [POLYGON ((0.8219753439188935 0.10604816488921...\n",
       "3    407676_2023_tava.laz  [POLYGON ((0.8271766667021438 0.12717715278267...\n",
       "4    407700_2023_tava.laz  [POLYGON ((0.8892044709064066 0.12633771542459...\n",
       "..                    ...                                                ...\n",
       "126  596650_2022_tava.laz  [POLYGON ((0.7591239418252371 0.83231981284916...\n",
       "127  596651_2022_tava.laz  [POLYGON ((0.7591239418252371 0.83231981284916...\n",
       "128  598586_2022_tava.laz  [POLYGON ((0.5878351322608069 0.83954359497874...\n",
       "129  600638_2022_tava.laz  [POLYGON ((0.727010634902399 0.847874268889427...\n",
       "130  600639_2022_tava.laz  [POLYGON ((0.7292444708873518 0.84957621712237...\n",
       "\n",
       "[131 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"0.8599969982448965 0.08705005634576082 0.0007467442215420839 0.0011310469731688616\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,0.1752311596646905)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"2.2620939463377233e-05\" opacity=\"0.6\" d=\"M 0.8603584127267823,0.0881392126902938 L 0.8606247089919634,0.0879507111385465 L 0.8606968518579379,0.0877036321908236 L 0.8607018518378027,0.0873470036312938 L 0.8606201058137231,0.0870919469743967 L 0.8604165873257443,0.0870966659858823 L 0.8602497618994676,0.0873635578900576 L 0.8600388888735324,0.0877370778471232 L 0.8603584127267823,0.0881392126902938 z\" /></g></svg>"
      ],
      "text/plain": [
       "<POLYGON ((0.86 0.088, 0.861 0.088, 0.861 0.088, 0.861 0.087, 0.861 0.087, 0...>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the CSV\n",
    "normalized_polygons = pd.read_csv('../data/normalized_polygons.csv')\n",
    "\n",
    "# Convert the list of WKT strings back to shapely Polygons\n",
    "normalized_polygons['polygons'] = normalized_polygons['polygons_wkt'].apply(lambda wkt_list: [shapely.wkt.loads(wkt_str) for wkt_str in eval(wkt_list)])\n",
    "normalized_polygons = normalized_polygons.drop('polygons_wkt', axis=1)\n",
    "display(normalized_polygons)\n",
    "test_poly = normalized_polygons['polygons'][0][0]\n",
    "display(test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_laz_dir = '../data/normalized_lazFiles/'\n",
    "output_laz_dir = '../data/downsampled_class_lazFiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 131/131 [05:26<00:00,  2.49s/it]\n"
     ]
    }
   ],
   "source": [
    "downsample_lidar(input_laz_dir=input_laz_dir, output_laz_dir=output_laz_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_las = os.listdir('../data/downsampled_class_lazFiles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_las = []\n",
    "for las_file in all_las:\n",
    "    las = laspy.read(os.path.join('../data/downsampled_class_lazFiles', las_file))\n",
    "    las_xyz = las.xyz\n",
    "    las_x = las_xyz[:, 0]\n",
    "    las_y = las_xyz[:, 1]\n",
    "    las_z = las_xyz[:, 2]\n",
    "    las_r = las.points.array['red'] / 65535 * 255\n",
    "    las_g = las.points.array['green'] / 65535 * 255\n",
    "    las_b = las.points.array['blue'] / 65535 * 255\n",
    "    # Create a DataFrame for the current LAS file\n",
    "    las_df = pd.DataFrame({\n",
    "        'x': las_x,\n",
    "        'y': las_y,\n",
    "        'z': las_z,\n",
    "        'r': las_r,\n",
    "        'g': las_g,\n",
    "        'b': las_b,\n",
    "        'file': las_file\n",
    "    })\n",
    "\n",
    "    # Append to the list of DataFrames\n",
    "    data_las.append(las_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>r</th>\n",
       "      <th>g</th>\n",
       "      <th>b</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.860079</td>\n",
       "      <td>0.086190</td>\n",
       "      <td>0.72822</td>\n",
       "      <td>113.556420</td>\n",
       "      <td>135.470817</td>\n",
       "      <td>93.634241</td>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.860008</td>\n",
       "      <td>0.086186</td>\n",
       "      <td>0.72834</td>\n",
       "      <td>51.797665</td>\n",
       "      <td>51.797665</td>\n",
       "      <td>41.836576</td>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.860058</td>\n",
       "      <td>0.086156</td>\n",
       "      <td>0.72818</td>\n",
       "      <td>101.603113</td>\n",
       "      <td>110.568093</td>\n",
       "      <td>83.673152</td>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.860076</td>\n",
       "      <td>0.086193</td>\n",
       "      <td>0.75631</td>\n",
       "      <td>105.587549</td>\n",
       "      <td>105.587549</td>\n",
       "      <td>85.665370</td>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.860079</td>\n",
       "      <td>0.086156</td>\n",
       "      <td>0.72807</td>\n",
       "      <td>85.665370</td>\n",
       "      <td>68.731518</td>\n",
       "      <td>71.719844</td>\n",
       "      <td>397689_2023_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9818758</th>\n",
       "      <td>0.729969</td>\n",
       "      <td>0.850155</td>\n",
       "      <td>0.29744</td>\n",
       "      <td>67.735409</td>\n",
       "      <td>80.684825</td>\n",
       "      <td>63.750973</td>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9818759</th>\n",
       "      <td>0.729992</td>\n",
       "      <td>0.850108</td>\n",
       "      <td>0.32090</td>\n",
       "      <td>152.404669</td>\n",
       "      <td>149.416342</td>\n",
       "      <td>118.536965</td>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9818760</th>\n",
       "      <td>0.729972</td>\n",
       "      <td>0.850107</td>\n",
       "      <td>0.31263</td>\n",
       "      <td>95.626459</td>\n",
       "      <td>93.634241</td>\n",
       "      <td>72.715953</td>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9818761</th>\n",
       "      <td>0.729904</td>\n",
       "      <td>0.850165</td>\n",
       "      <td>0.29798</td>\n",
       "      <td>87.657588</td>\n",
       "      <td>87.657588</td>\n",
       "      <td>75.704280</td>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9818762</th>\n",
       "      <td>0.729913</td>\n",
       "      <td>0.850152</td>\n",
       "      <td>0.29780</td>\n",
       "      <td>77.696498</td>\n",
       "      <td>78.692607</td>\n",
       "      <td>62.754864</td>\n",
       "      <td>600639_2022_tava.laz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9818763 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                x         y        z           r           g           b  \\\n",
       "0        0.860079  0.086190  0.72822  113.556420  135.470817   93.634241   \n",
       "1        0.860008  0.086186  0.72834   51.797665   51.797665   41.836576   \n",
       "2        0.860058  0.086156  0.72818  101.603113  110.568093   83.673152   \n",
       "3        0.860076  0.086193  0.75631  105.587549  105.587549   85.665370   \n",
       "4        0.860079  0.086156  0.72807   85.665370   68.731518   71.719844   \n",
       "...           ...       ...      ...         ...         ...         ...   \n",
       "9818758  0.729969  0.850155  0.29744   67.735409   80.684825   63.750973   \n",
       "9818759  0.729992  0.850108  0.32090  152.404669  149.416342  118.536965   \n",
       "9818760  0.729972  0.850107  0.31263   95.626459   93.634241   72.715953   \n",
       "9818761  0.729904  0.850165  0.29798   87.657588   87.657588   75.704280   \n",
       "9818762  0.729913  0.850152  0.29780   77.696498   78.692607   62.754864   \n",
       "\n",
       "                         file  \n",
       "0        397689_2023_tava.laz  \n",
       "1        397689_2023_tava.laz  \n",
       "2        397689_2023_tava.laz  \n",
       "3        397689_2023_tava.laz  \n",
       "4        397689_2023_tava.laz  \n",
       "...                       ...  \n",
       "9818758  600639_2022_tava.laz  \n",
       "9818759  600639_2022_tava.laz  \n",
       "9818760  600639_2022_tava.laz  \n",
       "9818761  600639_2022_tava.laz  \n",
       "9818762  600639_2022_tava.laz  \n",
       "\n",
       "[9818763 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Concatenate all DataFrames into one (if processing multiple files)\n",
    "final_df = pd.concat(data_las, ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(final_df)\n",
    "\n",
    "final_df.to_parquet('../data/downsampled_points.parquet', engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpulocal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
